{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "Transformers version: 4.51.3\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Current device: 0\n",
      "Device name: NVIDIA L40\n",
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dataclasses import dataclass\n",
    "import transformer_lens\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name\n",
    "from transformer_lens.FactoredMatrix import FactoredMatrix\n",
    "import circuitsvis as cv\n",
    "\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Optional, Callable\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data import (\n",
    "    a_an_something_questions,\n",
    "    a_habit_questions,\n",
    "    a_rabbit_questions,\n",
    "    an_ape_questions,\n",
    "    lines_that_rhyme_with_pain,\n",
    "    lines_that_rhyme_with_quick,\n",
    "    example_based_rhymed_prompts_quick,\n",
    "    example_based_unrhymed_prompts_quick,\n",
    "    rhymed_couplets,\n",
    "    unrhymed_couplets,\n",
    "    lines_that_rhyme_with_rabbit,\n",
    "    lines_that_rhyme_with_habit,\n",
    "    lines_that_rhyme_with_rabbit_general,\n",
    "    lines_that_rhyme_with_rabbit_oneshot,\n",
    "    lines_that_rhyme_with_habit_general,\n",
    "    lines_that_rhyme_with_habit_oneshot,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {t.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {t.cuda.is_available()}\")\n",
    "if t.cuda.is_available():\n",
    "    print(f\"CUDA version: {t.version.cuda}\")\n",
    "    print(f\"Current device: {t.cuda.current_device()}\")\n",
    "    print(f\"Device name: {t.cuda.get_device_name(t.cuda.current_device())}\")\n",
    "# %%\n",
    "dotenv.load_dotenv(\"hf.env\")\n",
    "# @title 1.5. For access to Gemma models, log in to HuggingFace \n",
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")\n",
    "# %%\n",
    "MODEL_ID = \"google/gemma-2-9b-it\" # Or \"google/gemma-2-9b\" if you prefer the base model\n",
    "# Set to True if you have limited VRAM (e.g., < 24GB). Requires bitsandbytes\n",
    "\n",
    "# How strongly to apply the steering vector. Tune this value (e.g., 0.5 to 5.0)\n",
    "STEERING_MULTIPLIER = 1.5\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "MAX_NEW_TOKENS = 150\n",
    "TEMPERATURE = 0.7\n",
    "DO_SAMPLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8005dbe273514f108f142c31981b289f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load model from transformer_lens\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(MODEL_ID, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load steering vector\n",
    "path = \"steering_vectors/steering_vector_quick_to_pain_layer_27.pt\"\n",
    "steering_vector = t.load(path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "HookedTransformerConfig:\n",
      "{'NTK_by_parts_factor': 8.0,\n",
      " 'NTK_by_parts_high_freq_factor': 4.0,\n",
      " 'NTK_by_parts_low_freq_factor': 1.0,\n",
      " 'act_fn': 'gelu_pytorch_tanh',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 16.0,\n",
      " 'attn_scores_soft_cap': 50.0,\n",
      " 'attn_types': ['global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local'],\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 256,\n",
      " 'd_mlp': 14336,\n",
      " 'd_model': 3584,\n",
      " 'd_vocab': 256000,\n",
      " 'd_vocab_out': 256000,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': 'cuda',\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-06,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': True,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': True,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gemma-2-9b-it',\n",
      " 'n_ctx': 8192,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 16,\n",
      " 'n_key_value_heads': 8,\n",
      " 'n_layers': 42,\n",
      " 'n_params': 8940158976,\n",
      " 'normalization_type': 'RMSPre',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'Gemma2ForCausalLM',\n",
      " 'output_logits_soft_cap': 30.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000.0,\n",
      " 'rotary_dim': 256,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'google/gemma-2-9b-it',\n",
      " 'tokenizer_prepends_bos': True,\n",
      " 'trust_remote_code': False,\n",
      " 'ungroup_grouped_query_attention': False,\n",
      " 'use_NTK_by_parts_rope': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': True,\n",
      " 'use_normalization_before_and_after': True,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': 4096}\n",
      "torch.Size([3584])\n"
     ]
    }
   ],
   "source": [
    "print(model.W_E.device)\n",
    "print(steering_vector.device)\n",
    "print(model.cfg)\n",
    "print(steering_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "layer = 27\n",
    "reading_from_steering_vector = t.zeros((n_layers - layer, n_heads))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
