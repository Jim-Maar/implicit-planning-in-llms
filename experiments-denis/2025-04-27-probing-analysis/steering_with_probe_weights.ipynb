{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3574e96c-c144-4891-b94b-d6c6b7d30893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m235.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m202.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers, bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.6.0 bitsandbytes-0.45.5 huggingface-hub-0.30.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7227ffcb-c546-4e5e-bfd8-44747475cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8ba943-d649-4246-b1dc-27c3c9427024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc56b34-1337-49da-9ff5-838f1b34886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f86c406-ad02-4853-9622-bf059dc4fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250319+cu128\n",
      "Transformers version: 4.51.3\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Optional, Callable\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2098eddd-2a41-409e-8200-105b6103bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11208a78-6641-4fa9-ad8c-6a7f99ca34e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('hf.env')  # by default it looks for a .env file in the current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b050ffae-a61c-4121-b557-336b62061778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "# @title 1.5. For access to Gemma models, log in to HuggingFace \n",
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HFTOKEN\")\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b374fa95-6a55-4a84-9e50-1411985db57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-2-9b-it\" # Or \"google/gemma-2-9b\" if you prefer the base model\n",
    "# Set to True if you have limited VRAM (e.g., < 24GB). Requires bitsandbytes\n",
    "USE_4BIT_QUANTIZATION = False\n",
    "\n",
    "# --- Steering Configuration ---\n",
    "# !! IMPORTANT !! Find the correct layer name for your model.\n",
    "# Example: 'model.layers[15].mlp.gate_proj' or 'model.layers[20].self_attn.o_proj'\n",
    "# Use the `print(model)` output in Section 3 to find a suitable layer name.\n",
    "TARGET_LAYER_NAME = 'model.layers.20' # <--- CHANGE THIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02de826a-19e8-4f99-8c67-b7e10223a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEERING_MULTIPLIER = 1.5\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "MAX_NEW_TOKENS = 150\n",
    "TEMPERATURE = 0.7\n",
    "DO_SAMPLE = True\n",
    "\n",
    "# --- Output ---\n",
    "OUTPUT_FILE = \"gemma2_steering_output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac79d20-d9a8-4ce0-af47-fb923fd5488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "'''lines_that_rhyme_with_rabbit = [\n",
    "    \"The gardener tends his plants with daily habit\",\n",
    "    \"When paint spills on the floor, you need to dabbit\",\n",
    "    \"If you see something you want, just reach and grabbit\",\n",
    "    \"The monastery's leader is the wise old abbot\",\n",
    "    \"The metal alloy used in engines is called babbit\",\n",
    "    \"The chef prepared a stew with fresh green cabbage\",\n",
    "    \"The seamstress chose a silky, flowing fabric\",\n",
    "    \"The storm that passed through town caused so much havoc\",\n",
    "    \"The wizard cast a spell with ancient magic\",\n",
    "    \"The rotting food attracted many a maggot\",\n",
    "    \"The critic's harsh review was truly savage\",\n",
    "    \"The radio produced annoying static\",\n",
    "    \"The ancient message carved upon a tablet\",\n",
    "    \"Their agreement to proceed remained quite tacit\",\n",
    "    \"We sat for hours in the morning traffic\",\n",
    "    \"The ending of the play was deeply tragic\",\n",
    "]'''\n",
    "\n",
    "lines_that_rhyme_with_quick = [\n",
    "    \"The house was built with sturdy, reddish brick\",\n",
    "    \"The camera captured moments with each click\",\n",
    "    \"She turned the lights on with a simple flick\",\n",
    "    \"The soccer player gave the ball a mighty kick\",\n",
    "    \"The puppy gave my hand a gentle lick\",\n",
    "    \"The razor left a small and painful nick\",\n",
    "    \"From all the fruits available, I'll make my pick\",\n",
    "    \"The rose's thorn can cause a sudden prick\",\n",
    "    \"He stayed at home because he felt too sick\",\n",
    "    \"The rain had made the winding road quite slick\",\n",
    "    \"The child drew pictures with a charcoal stick\",\n",
    "    \"The winter fog was rolling in so thick\",\n",
    "    \"The clock marked every second with a tick\",\n",
    "    \"The magician performed an amazing trick\",\n",
    "    \"The candle slowly burned down to the wick\",\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_pain = [\n",
    "    \"The storm has passed but soon will come again\",\n",
    "    \"The wizard's knowledge was profoundly arcane\",\n",
    "    \"That constant noise became my existence's bane\",\n",
    "    \"The puzzle challenged every corner of my brain\",\n",
    "    \"The elderly man walked slowly with his cane\",\n",
    "    \"The prisoner rattled his heavy iron chain\",\n",
    "    \"The construction site had a towering crane\",\n",
    "    \"The queen would rarely to respond deign\",\n",
    "    \"The rainwater flowed down into the drain\",\n",
    "    \"She looked at the offer with obvious disdain\",\n",
    "    \"The king surveyed his vast and wealthy domain\",\n",
    "    \"The teacher took her time to clearly explain\",\n",
    "    \"He tried to hide his feelings and to feign\",\n",
    "    \"The pilgrims journeyed to the ancient fane\",\n",
    "    \"The athlete trained for months to make a gain\",\n",
    "    \"The farmer harvested the golden grain\",\n",
    "    \"The doctor's treatment was gentle and humane\",\n",
    "    \"His argument was completely inane\",\n",
    "    \"The plan they proposed was utterly insane\",\n",
    "    \"The classic novel starred a heroine named Jane\",\n",
    "    \"The car sped down the narrow country lane\",\n",
    "    \"The issue at hand was certainly the main\",\n",
    "    \"The lion shook his magnificent mane\",\n",
    "    \"The office work felt repetitive and mundane\",\n",
    "    \"The church would soon the new priest ordain\",\n",
    "    \"The sunlight streamed through the window pane\",\n",
    "    \"The message written there was crystal plain\",\n",
    "    \"The travelers boarded the waiting plane\",\n",
    "    \"His language was considered quite profane\",\n",
    "    \"The flowers bloomed after the gentle rain\",\n",
    "    \"The rider pulled firmly on the horse's rein\",\n",
    "    \"The king began his long and peaceful reign\",\n",
    "    \"Despite the chaos, she remained quite sane\",\n",
    "    \"We planned our summer holiday in Spain\",\n",
    "    \"The athlete suffered from a painful ankle sprain\",\n",
    "    \"The red wine left a permanent stain\",\n",
    "    \"The heavy lifting put his back under strain\",\n",
    "    \"Good habits help your health maintain and sustain\",\n",
    "    \"The maiden was courted by a handsome swain\",\n",
    "    \"We hurried to catch the departing train\",\n",
    "    \"The river split the land in twain\",\n",
    "    \"His manner was sophisticated and urbane\",\n",
    "    \"Her efforts to convince him were in vain\",\n",
    "    \"The wind direction showed on the weather vane\",\n",
    "    \"The nurse carefully located a suitable vein\",\n",
    "    \"As night approached, the daylight began to wane\",\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_rabbit = [\n",
    "    \"I saw something move in the garden, so I decided to grab it\", # To my surprise, it turned out to be a fluffy little rabbit.\n",
    "    \"When you hear a noise in the bushes, don't be afraid to nab it\", # Chances are it's just the neighborhood's friendly rabbit.\n",
    "    \"She has a special way with animals, it's quite a habit\", # Her favorite creature to care for is her pet rabbit.\n",
    "    \"I thought I'd plant some carrots, but something came to stab it\", # I looked outside and caught the culprit—a hungry rabbit.\n",
    "    \"The magician pulled something furry out of his hat, to my amazement he had it\", # The audience cheered when they saw it was a snow-white rabbit.\n",
    "    \"If you find a hole in your garden, you should probably tab it\", # It's likely the new underground home of a burrowing rabbit.\n",
    "    \"The child saw something soft in the pet store and wanted to have it\", # She begged her parents until they bought her that adorable rabbit.\n",
    "    \"I heard a rustling sound in the forest and tried to dab it\", # But it hopped away quickly—I just missed that wild rabbit.\n",
    "    \"When something nibbles your lettuce, there's no need to blab it\", # Everyone knows the culprit is probably a garden rabbit.\n",
    "    \"I felt something soft brush against my leg, I reached down to grab it\", # And found myself petting the silky fur of a friendly rabbit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_habit = [\n",
    "    \"When you see a rabbit\", # You might form a feeding habit.\n",
    "    \"He'd grab it if he could just nab it\", # That's become his daily habit.\n",
    "    \"The frog sits on the lily pad, a bit\", # Too long—it's turned into a habit.\n",
    "    \"She wears that jacket like she's glad to have it\", # Dressing sharp has always been her habit.\n",
    "    \"I know I should quit, but I just can't stab it\", # Breaking free from such a stubborn habit.\n",
    "    \"If there's a chance for joy, I'll always grab it\", # Seeking happiness is my best habit.\n",
    "    \"The cat will chase the yarn if you dab it\", # Playing games has been a lifelong habit.\n",
    "    \"When faced with problems, I don't just blab it\", # Thinking before speaking is my habit.\n",
    "    \"He'll take a compliment, but never crab it\", # Staying humble is his finest habit.\n",
    "    \"The chef will taste the dish before they tab it\", # Quality control's a professional habit.\n",
    "    \"When opportunity knocks, I'll cab it\", # Seizing the moment is my favorite habit.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08aff8df-1620-4553-a984-cdfc016f0f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-2-9b-it\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5694161e9db5462b9bd1f8eb9d18f893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ## 3. Load Model and Tokenizer\n",
    "\n",
    "# +\n",
    "# Configure quantization if needed\n",
    "quantization_config = None\n",
    "USE_4BIT_QUANTIZATION=False\n",
    "if USE_4BIT_QUANTIZATION:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Recommended for new models\n",
    "    )\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "\n",
    "# Determine device and dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32 # BF16 recommended on Ampere+\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not present\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically distribute across GPUs if available\n",
    "    # use_auth_token=YOUR_HF_TOKEN, # Add if model requires authentication\n",
    "    trust_remote_code=True # Gemma requires this for some versions/variants\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device(s): {model.hf_device_map}\")\n",
    "\n",
    "# --- IMPORTANT: Finding the Layer Name ---\n",
    "# Uncomment the following line to print the model structure and find the exact layer name\n",
    "# print(model)\n",
    "# Look for layers like 'model.layers[INDEX].mlp...' or 'model.layers[INDEX].self_attn...'\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "# %%\n",
    "# ## 4. Hooking and Activation Handling Functions\n",
    "\n",
    "# +\n",
    "# Global storage for captured activations\n",
    "activation_storage = {}\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Helper function to get a module object from its name string.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "\n",
    "def capture_activation_hook(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "\n",
    "def get_activations(model, tokenizer, prompts: List[str], layer_name: str) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model and captures activations from the target layer.\n",
    "    Returns the averaged activation across all prompts for the last token position.\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {} # Clear previous activations\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        lambda module, input, output: capture_activation_hook(module, input, output, layer_name)\n",
    "    )\n",
    "\n",
    "    all_layer_activations = []\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "            # We only need the forward pass, not generation here\n",
    "            _ = model(**inputs)\n",
    "\n",
    "            if layer_name in activation_storage:\n",
    "                 # Assuming batch size is 1 when processing one prompt at a time\n",
    "                last_token_activation = activation_storage[layer_name] # Shape (1, hidden_dim)\n",
    "                all_layer_activations.append(last_token_activation)\n",
    "                del activation_storage[layer_name] # Clear for next prompt\n",
    "            else:\n",
    "                print(f\"Warning: Activation for layer {layer_name} not captured for prompt: '{prompt}'\")\n",
    "\n",
    "\n",
    "    hook_handle.remove() # Clean up the hook\n",
    "\n",
    "    if not all_layer_activations:\n",
    "        print(f\"Error: No activations were captured for layer {layer_name}.\")\n",
    "        return None\n",
    "\n",
    "    # Stack and average activations across all prompts\n",
    "    # Resulting shape: (num_prompts, hidden_dim) -> (hidden_dim)\n",
    "    avg_activation = torch.stack(all_layer_activations).mean(dim=0).squeeze() # Average over the prompt dimension\n",
    "    print(f\"Calculated average activation for layer '{layer_name}' with shape: {avg_activation.shape}\")\n",
    "    return avg_activation\n",
    "# %%\n",
    " # --- Steering Hook during Generation ---\n",
    "\n",
    "# Global variable to hold the steering vector during generation\n",
    "steering_vector_internal = None\n",
    "steering_multiplier_internal = 1.0\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            modified_output = output + (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return modified_output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "             # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0] + (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "@contextmanager\n",
    "def apply_steering(model, layer_name, steering_vector, multiplier):\n",
    "    \"\"\"Context manager to temporarily apply the steering hook.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "\n",
    "    # Ensure previous hook (if any) on the same layer is removed\n",
    "    # This basic implementation assumes only one steering hook at a time on this layer\n",
    "    # More robust solutions might track handles explicitly.\n",
    "    \n",
    "    handle = None\n",
    "    try:\n",
    "        steering_vector_internal = steering_vector\n",
    "        steering_multiplier_internal = multiplier\n",
    "        target_module = get_module_by_name(model, layer_name)\n",
    "        handle = target_module.register_forward_hook(steering_hook)\n",
    "        print(f\"Steering hook applied to {layer_name} with multiplier {multiplier}\")\n",
    "        yield # Generation happens here\n",
    "    finally:\n",
    "        if handle:\n",
    "            handle.remove()\n",
    "        steering_vector_internal = None # Clear global state\n",
    "        steering_multiplier_internal = 1.0\n",
    "        print(f\"Steering hook removed from {layer_name}\")\n",
    "        gc.collect() # Suggest garbage collection\n",
    "        torch.cuda.empty_cache() # Clear cache if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a555671-ca82-4174-b2f5-5b55e6b090d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_PROMPTS = [f'A rhymed couplet:\\n{line}\\n' for line in lines_that_rhyme_with_quick]\n",
    "NEGATIVE_PROMPTS = [f'A rhymed couplet:\\n{line}\\n' for line in lines_that_rhyme_with_pain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e348a5-640b-44cd-b093-cc724524b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_PROMPT=f'A rhymed couplet:\\n{lines_that_rhyme_with_quick[0]}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b7fd2d-0f4b-43f3-9d54-c438e5fbca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERATION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65644726-d099-4631-94e2-a90253506840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n"
     ]
    }
   ],
   "source": [
    "# +\n",
    "print(\"Calculating activations for POSITIVE prompts...\")\n",
    "avg_pos_activation = get_activations(model, tokenizer, POSITIVE_PROMPTS, TARGET_LAYER_NAME)\n",
    "\n",
    "print(\"\\nCalculating activations for NEGATIVE prompts...\")\n",
    "avg_neg_activation = get_activations(model, tokenizer, NEGATIVE_PROMPTS, TARGET_LAYER_NAME)\n",
    "\n",
    "steering_vector = None\n",
    "if avg_pos_activation is not None and avg_neg_activation is not None:\n",
    "    steering_vector = avg_pos_activation - avg_neg_activation\n",
    "    print(f\"\\nSteering vector computed successfully. Shape: {steering_vector.shape}\")\n",
    "    # Optional: Normalize the steering vector (can sometimes help)\n",
    "    # steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "    # print(\"Steering vector normalized.\")\n",
    "else:\n",
    "    print(\"\\nError: Could not compute steering vector due to missing activations.\")\n",
    "\n",
    "# Clean up memory\n",
    "del avg_pos_activation\n",
    "del avg_neg_activation\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "# %%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "87f7e51e-00af-4df9-84a0-545e40043225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "del avg_pos_activation\n",
    "del avg_neg_activation\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ff87ce-f581-4ce2-96d7-c12eb8a4ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37777312-9f8b-4462-887a-e8ab305896af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1094, 0.6250, 1.0312,  ..., 0.2266, 0.1875, 0.4473],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "steering_vector = torch.load('steering_vector_from_quick_to_pain.pt')\n",
    "print(steering_vector )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6de46460-d3d5-4dff-a9e9-60dd65f166b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3584])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c770afe-bea2-423b-8bd8-4db12b4b59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LAYER_NAME=\"model.layers.27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e50f298-cdd7-4667-a1f3-db0c9e899ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"rhyme_probe_weights_{TARGET_LAYER_NAME}.json\", 'r') as f:\n",
    "        extracted_weights=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0672f547-6b72-4152-81e8-a549484f518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bake_rhymes', 'band_rhymes', 'call_rhymes', 'doom_rhymes', 'night_rhymes', 'pain_rhymes', 'shore_rhymes', 'sing_rhymes', 'skies_rhymes', 'sleep_rhymes', 'slick_rhymes', 'unfold_rhymes'])\n"
     ]
    }
   ],
   "source": [
    "print(extracted_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fd69e4e-db2d-444c-94b0-b72f74b6bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick2painfromprobe=torch.tensor(extracted_weights[\"slick_rhymes\"])-torch.tensor(extracted_weights[\"pain_rhymes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "581212d8-ebf5-44e4-be15-64152aab645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pain2shorefromprobe=torch.tensor(extracted_weights[\"slick_rhymes\"])-torch.tensor(extracted_weights[\"shore_rhymes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fe0673c-7b8d-4f3d-bc60-6040b80040a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3584])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick2painfromprobe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4d5dd45-3e6b-47f4-abff-80913e1c237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Generate Text (Baseline vs. Steered)\n",
    "import einops\n",
    "STEERING_MULTIPLIER = 1.5\n",
    "\n",
    "def generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size,steering_multiplier, max_new_tokens, temperature, do_sample):\n",
    "    if steering_vector is None:\n",
    "        return None\n",
    "    inputs = tokenizer([generation_prompt] * batch_size, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    # inputs.input_ids = einops.repeat(inputs.input_ids, \"1 p-> b p\", b=batch_size)\n",
    "    # tokens = model.to_tokens(generation_prompt)\n",
    "    # tokens = einops.repeat(tokens, \"1 p-> b p\", b=batch_size)\n",
    "    # print(tokens.shape)\n",
    "    print(inputs.input_ids.shape)\n",
    "    with torch.no_grad():\n",
    "        outputs_baseline = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id # Important for generation\n",
    "        )\n",
    "\n",
    "    # print(outputs_baseline.shape)\n",
    "    text_baseline = tokenizer.batch_decode(outputs_baseline, skip_special_tokens=True)\n",
    "    # text_baseline = [tokenizer.decode(outputs_baseline[i], skip_special_tokens=True) for i in range(batch_size)]\n",
    "\n",
    "    print(f\"\\n--- Generating Steered Output (Multiplier: {steering_multiplier}) ---\")\n",
    "    with torch.no_grad():\n",
    "         # Apply the steering hook using the context manager\n",
    "        with apply_steering(model, TARGET_LAYER_NAME, steering_vector, steering_multiplier):\n",
    "            outputs_steered = model.generate(\n",
    "                **inputs, # Use the same input tokens\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "    text_steered = tokenizer.batch_decode(outputs_steered, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Generating Steered Output (Multiplier: {-steering_multiplier}) ---\")\n",
    "    with torch.no_grad():\n",
    "         # Apply the steering hook using the context manager\n",
    "        with apply_steering(model, TARGET_LAYER_NAME, steering_vector, -steering_multiplier):\n",
    "            outputs_steered = model.generate(\n",
    "                **inputs, # Use the same input tokens\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    text_negsteered = tokenizer.batch_decode(outputs_steered, skip_special_tokens=True)\n",
    "\n",
    "    # Clean up generation outputs\n",
    "    del outputs_baseline, outputs_steered, inputs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return text_baseline, text_steered, text_negsteered\n",
    "\n",
    "MAX_NEW_TOKENS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e91e3ff2-be0a-4076-b5f0-8cb28cfba2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(51.5000, dtype=torch.bfloat16)\n",
      "tensor(0.4676)\n",
      "tensor(0.3614)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(steering_vector, p=2))\n",
    "print(torch.norm(quick2painfromprobe, p=2))\n",
    "print(torch.norm(pain2shorefromprobe, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c59409d-ba7e-40ef-8fde-564828df8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18])\n",
      "\n",
      "--- Generating Steered Output (Multiplier: 400) ---\n",
      "Steering hook applied to model.layers.27 with multiplier 400\n",
      "Steering hook removed from model.layers.27\n",
      "\n",
      "--- Generating Steered Output (Multiplier: -400) ---\n",
      "Steering hook applied to model.layers.27 with multiplier -400\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([\"A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood for ages, strong against the trick.\\n\\n\\nLet me know if you'd like another couplet or a different rhyming scheme!\\n\",\n",
       "  'A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood for many years, a steadfast trick.\\n\\n**Explanation:**\\n\\n* **Rhyme:** \"brick\" and \"trick\" rhyme, creating',\n",
       "  'A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood for years, a testament to stick.\\n\\nThis is a couplet because it consists of two lines of poetry that rhyme. \\nIt',\n",
       "  \"A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood for years, impervious to trick. \\n\\n\\nLet me know if you'd like more! \\n\\n\"],\n",
       " [\"A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood for years, a landmark, strong as.\\n\\nThe answer is: **quick**. \\n\\nHere's why:\\n\\n* **Rhy\",\n",
       "  \"A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood so strong, it seemed impossible to\\n\\nWhat's the best word to complete the line?\\n\\nLet me know, and I'll\",\n",
       "  'A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood there proud, a monument\\n\\nA quatrain:\\n\\nThe old house stood on a hill,\\nIts windows gleamed in the sun,',\n",
       "  \"A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood on a hill, a charming, weathered pick. \\n\\n\\nLet me know if you'd like more! \\n\\n\"],\n",
       " ['A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nA testament to love, a timeless, sturdy core. \\n\\n\\nPlease provide another rhyming couplet that continues the theme of a home built with love.',\n",
       "  \"A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood steadfast, defying winter's core.\\n\\nLet me know if you'd like to explore more poems!\\n\",\n",
       "  'A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood for years, a testament to yore.\\n\\n**Explanation:**\\n\\n* **Rhyme:** The words \"brick\" and \"fore\" rhyme',\n",
       "  'A rhymed couplet:\\nThe house was built with sturdy, reddish brick\\nAnd stood through storms, a testament to lore.\\n\\nThis is an example of a couplet, a two-line stanza in poetry with a rhyme'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_steered_output(pain2shorefromprobe, model, tokenizer, GENERATION_PROMPT, 4, 400, MAX_NEW_TOKENS, TEMPERATURE, DO_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9f53d9-80f8-434b-97bc-efbbefd60383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-2-9b-it\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ef3e0b445a4f85bf9c4587088c01fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068e8687db554189a5d28f42aa13845c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df7f8f9e53440ed95d07077039a1b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4554627a71c348edbd45df6e875f6815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7000907c81f74d9aa69494baa748cf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954f7ebea7044ad698c0489941c9b5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b5c6c4831a4b21bf825054c8f125c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4d43b94755461caa120bdf79d27e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7f832f12394609a8e9ff7ef67efd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c15b909935e41a18a320a90f46a0365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158c2f53070a4c2fb529d83e5b8c0fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfc21a8a85b4605b929c263e88e0752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5986b23d9c4888b4f3c81f39c7a3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ## 3. Load Model and Tokenizer: new version\n",
    "\n",
    "# +\n",
    "# Configure quantization if needed\n",
    "quantization_config = None\n",
    "if USE_4BIT_QUANTIZATION:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Recommended for new models\n",
    "    )\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "\n",
    "# Determine device and dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32 # BF16 recommended on Ampere+\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not present\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically distribute across GPUs if available\n",
    "    # use_auth_token=YOUR_HF_TOKEN, # Add if model requires authentication\n",
    "    trust_remote_code=True # Gemma requires this for some versions/variants\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device(s): {model.hf_device_map}\")\n",
    "\n",
    "# --- IMPORTANT: Finding the Layer Name ---\n",
    "# Uncomment the following line to print the model structure and find the exact layer name\n",
    "# print(model)\n",
    "# Look for layers like 'model.layers[INDEX].mlp...' or 'model.layers[INDEX].self_attn...'\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "# %%\n",
    "# ## 4. Hooking and Activation Handling Functions\n",
    "\n",
    "# +\n",
    "# Global storage for captured activations\n",
    "activation_storage = {}\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Helper function to get a module object from its name string.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "\n",
    "def capture_activation_hook(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "def capture_activation_hook_fast(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "\n",
    "def get_activations_fast(model, tokenizer, prompts: List[str], layer_name: str) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model and captures activations from the target layer.\n",
    "    Returns the averaged activation across all prompts for the last token position.\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {} # Clear previous activations\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        lambda module, input, output: capture_activation_hook_fast(module, input, output, layer_name)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        # We only need the forward pass, not generation here\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        if layer_name in activation_storage:\n",
    "                # Assuming batch size is 1 when processing one prompt at a time\n",
    "            last_token_activations = activation_storage[layer_name] # Shape (num_prompts, hidden_dim)\n",
    "            del activation_storage[layer_name] # Clear for next prompt\n",
    "        else:\n",
    "            print(f\"Warning: Activation for layer {layer_name} not captured for prompts: '{prompts}'\")\n",
    "                \n",
    "    hook_handle.remove() # Clean up the hook\n",
    "\n",
    "    # Stack and average activations across all prompts\n",
    "    # Resulting shape: (num_prompts, hidden_dim) -> (hidden_dim)\n",
    "    avg_activation = last_token_activations.mean(dim=0).squeeze() # Average over the prompt dimension\n",
    "    print(f\"Calculated average activation for layer '{layer_name}' with shape: {avg_activation.shape}\")\n",
    "    return avg_activation\n",
    "# %%\n",
    " # --- Steering Hook during Generation ---\n",
    "\n",
    "# Global variable to hold the steering vector during generation\n",
    "steering_vector_internal = None\n",
    "steering_multiplier_internal = 1.0\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            modified_output = output + (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return modified_output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "             # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0] + (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "@contextmanager\n",
    "def apply_steering(model, layer, steering_vector, multiplier):\n",
    "    \"\"\"Context manager to temporarily apply the steering hook.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    layer_name = f\"model.layers.{layer}\"\n",
    "\n",
    "    # Ensure previous hook (if any) on the same layer is removed\n",
    "    # This basic implementation assumes only one steering hook at a time on this layer\n",
    "    # More robust solutions might track handles explicitly.\n",
    "    \n",
    "    handle = None\n",
    "    try:\n",
    "        steering_vector_internal = steering_vector\n",
    "        steering_multiplier_internal = multiplier\n",
    "        target_module = get_module_by_name(model, layer_name)\n",
    "        handle = target_module.register_forward_hook(steering_hook)\n",
    "        print(f\"Steering hook applied to {layer_name} with multiplier {multiplier}\")\n",
    "        yield # Generation happens here\n",
    "    finally:\n",
    "        if handle:\n",
    "            handle.remove()\n",
    "        steering_vector_internal = None # Clear global state\n",
    "        steering_multiplier_internal = 1.0\n",
    "        print(f\"Steering hook removed from {layer_name}\")\n",
    "        gc.collect() # Suggest garbage collection\n",
    "        torch.cuda.empty_cache() # Clear cache if using GPU\n",
    "\n",
    "def generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    inputs = tokenizer([generation_prompt] * batch_size, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    if steering_vector is None:\n",
    "        print(inputs.input_ids.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                pad_token_id=tokenizer.eos_token_id # Important for generation\n",
    "            )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            # Apply the steering hook using the context manager\n",
    "            with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                outputs = model.generate(\n",
    "                    **inputs, # Use the same input tokens\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "    text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    del outputs, inputs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_outputs(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    assert steering_vector is not None\n",
    "    text_baseline = generate_steered_output(None, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_steered = generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_negsteered = generate_steered_output(-steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    return text_baseline, text_steered, text_negsteered\n",
    "\n",
    "# %%\n",
    "# ## Compute the Steering Vector\n",
    "def get_steering_vector_fast(model, tokenizer, positive_prompts, negative_prompts, layer=20):\n",
    "    target_layer_name = f\"model.layers.{layer}\"\n",
    "    print(\"Calculating activations for POSITIVE prompts...\")\n",
    "    avg_pos_activation = get_activations_fast(model, tokenizer, positive_prompts, target_layer_name)\n",
    "\n",
    "    print(\"\\nCalculating activations for NEGATIVE prompts...\")\n",
    "    avg_neg_activation = get_activations_fast(model, tokenizer, negative_prompts, target_layer_name)\n",
    "\n",
    "    steering_vector = None\n",
    "    if avg_pos_activation is not None and avg_neg_activation is not None:\n",
    "        steering_vector = avg_pos_activation - avg_neg_activation\n",
    "        print(f\"\\nSteering vector computed successfully. Shape: {steering_vector.shape}\")\n",
    "        # Optional: Normalize the steering vector (can sometimes help)\n",
    "        # steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "        # print(\"Steering vector normalized.\")\n",
    "    else:\n",
    "        print(\"\\nError: Could not compute steering vector due to missing activations.\")\n",
    "    del avg_pos_activation\n",
    "    del avg_neg_activation\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return steering_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70473b00-9463-4774-80ed-0bd6438f3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_based_unrhymed_prompts_quick=[\n",
    "\"Among the towering oaks and whispered winds\\nGenerations carved their legacy into earth\\nThe house was built with sturdy, reddish brick\",\n",
    "\"Memories suspended in digital eternity\\nSilent witness to joy and passing time\\nThe camera captured moments with each click\",\n",
    "\"Darkness enveloped the empty room\\nHer shadow stretched across the wooden floor\\nShe turned the lights on with a simple flick\",\n",
    "\"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\",\n",
    "\"New life trembling with excitement\\nEyes reflecting unconditional devotion\\nThe puppy gave my hand a gentle lick\",\n",
    "\"Morning routines performed half-asleep\\nThe mirror revealed tired eyes and stubbled chin\\nThe razor left a small and painful nick\",\n",
    "\"The market overflowed with summer's bounty\\nColors and scents competed for attention\\nFrom all the fruits available, I'll make my pick\",\n",
    "\"Beauty often demands its price of pain\\nGarden treasures hidden among green leaves\\nThe rose's thorn can cause a sudden prick\",\n",
    "\"The world continued without his presence\\nPlans abandoned for blankets and tea\\nHe stayed at home because he felt too sick\",\n",
    "\"Storm clouds emptied their relentless burden\\nHeadlights struggled through sheets of water\\nThe rain had made the winding road quite slick\",\n",
    "\"Imagination flowed without boundaries or rules\\nSmall hands created worlds unknown to adults\\nThe child drew pictures with a charcoal stick\",\n",
    "\"Mountains disappeared into ghostly silence\\nThe world reduced to what stood before you\\nThe winter fog was rolling in so thick\",\n",
    "\"Time measured in heartbeats and breaths\\nThe quiet room amplified each sound\\nThe clock marked every second with a tick\",\n",
    "\"The audience leaned forward in anticipation\\nReality momentarily suspended its rules\\nThe magician performed an amazing trick\",\n",
    "\"Light fought a losing battle against darkness\\nShadows danced their final waltz on walls\\nThe candle slowly burned down to the wick\"\n",
    "]\n",
    "\n",
    "example_based_rhymed_prompts_quick=[\n",
    "\"The morning sun cast shadows long and bright\\nThe valley glowed with warm, inviting light\\nThe house was built with sturdy, reddish brick\",\n",
    "\"The album holds what memory cannot keep\\nThe past preserved in images so deep\\nThe camera captured moments with each click\",\n",
    "\"The darkness seemed to swallow every sound\\nThe emptiness of night hung all around\\nShe turned the lights on with a simple flick\",\n",
    "\"The player saw an opening ahead\\nA chance to score that filled the crowd with dread\\nThe soccer player gave the ball a mighty kick\",\n",
    "\"His eyes were bright with innocence and love\\nHis tail wagged fast, like wings upon a dove\\nThe puppy gave my hand a gentle lick\",\n",
    "\"The mirror showed a face not fully waked\\nThe hand that held the blade slightly shaked\\nThe razor left a small and painful nick\",\n",
    "\"The orchard's bounty spread before my eyes\\nA rainbow spectrum under summer skies\\nFrom all the fruits available, I'll make my pick\",\n",
    "\"The garden held its beauty and its pain\\nNature's gifts are never quite so plain\\nThe rose's thorn can cause a sudden prick\",\n",
    "\"The sunshine called through windows clear and bright\\nBut fever kept him through the day and night\\nHe stayed at home because he felt too sick\",\n",
    "\"The clouds above hung heavy, dark and low\\nThe path ahead was difficult to know\\nThe rain had made the winding road quite slick\",\n",
    "\"Her mind created worlds no one could see\\nImagination flowing wild and free\\nThe child drew pictures with a charcoal stick\",\n",
    "\"The landscape disappeared from human sight\\nThe world now wrapped in gray, ethereal light\\nThe winter fog was rolling in so thick\",\n",
    "\"Each moment passed in measured certainty\\nThe rhythm marking time's infinity\\nThe clock marked every second with a tick\",\n",
    "\"The crowd sat hushed in wonder and delight\\nExpecting wonders on this special night\\nThe magician performed an amazing trick\",\n",
    "\"The darkness crept across the quiet room\\nThe light grew faint, foretelling coming gloom\\nThe candle slowly burned down to the wick\"\n",
    "]\n",
    "\n",
    "def add_newline(lines): return [line+'\\n' for line in lines]\n",
    "POSITIVE_PROMPTS = add_newline(example_based_rhymed_prompts_quick)\n",
    "NEGATIVE_PROMPTS = add_newline(example_based_unrhymed_prompts_quick)\n",
    "\n",
    "GENERATION_PROMPT = NEGATIVE_PROMPTS[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a8818c-d18a-4479-bdb4-16da07a014ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n"
     ]
    }
   ],
   "source": [
    "LAYER=20\n",
    "steering_vector = get_steering_vector_fast(model, tokenizer, POSITIVE_PROMPTS, NEGATIVE_PROMPTS, layer=LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb477a2e-3253-4fdb-b254-f9179fd9cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 26])\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n",
      "Baseline:\n",
      "[\"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd watched it arc gracefully towards the goal\\n\\nThe crowd roared as the ball soared through the air\\nIt seemed to hang there for an eternity\\nThen, with a satisfying thud, it hit the back of the net\\n\\nThe stadium erupted in a frenzy of cheers\\nThe player's teammates swarmed him\\nHe had scored the winning goal in the final seconds\\nA moment that would be etched in his memory forever\\n\\n\\n\", \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe crowd erupted in cheers as the ball soared past the goalie\\n\\nWhat type of writing is this?\\n\\n**A. Narrative**\\n**B. Descriptive**\\n**C. Persuasive**\\n**D. Expository**\\n\\nThe correct answer is **A. Narrative**. Here's why:\\n\\n* **Narrative writing** tells a story. It has characters (the player, the crowd), a setting (the stadium), a plot (the kick and the reaction), and a clear sequence of events. \\n\\nLet's look at why the other options aren't as fitting:\\n\\n* **Descriptive writing** focuses on creating vivid images and sensory details. While there are some descriptive elements, the primary purpose is to tell a\", 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe ball soared through the air, a perfect arc\\nThen, silence.\\nA moment of agonizing suspense\\nFinally, the net rippled\\nThe crowd erupted in a cacophony of cheers\\n\\n\\nThis is a good example of narrative writing because:\\n\\n* **It tells a story:**  There\\'s a clear beginning, rising action, climax, and resolution.\\n* **It uses descriptive language:** Words like \"collective breath,\" \"mighty kick,\" \"perfect arc,\" \"agonizing suspense,\" and \"cacophony of cheers\" paint a vivid picture in the reader\\'s mind.\\n* **It creates suspense:** The author builds tension by describing the moment of silence and agonizing suspense before the ball hits the net.\\n* **', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nIt soared through the air, a perfect arc\\nThe crowd erupted in a roar as it found the net\\nVictory was theirs!\\n', \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd watched it soar towards the goal\\n\\n**The crowd erupted in cheers as the ball flew past the goalkeeper's outstretched hands and nestled into the back of the net.**\\n\\n\\n\\n**The player had scored!** They had achieved their dream, the culmination of years of hard work and dedication. \\n\\nThis moment would be etched in their memory forever.\\n\"]\n",
      "Steered:\n",
      "['The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nHoping it would soar and land in the net with a flick\\n\\nThe ball flew high, a blur of white and green\\nThe crowd erupted, a joyous, vibrant scene\\nThe goalie dove, but missed the mark\\nA goal! The stadium shook, a joyous spark!\\n\\nBut wait! The referee blew his whistle loud\\n\"That ball was out! No goal allowed!\"\\nThe player\\'s face fell, his heart sunk low\\nBut he knew he had to try, to give his all, you know?\\n\\nWith a determined grin, he shook his head\\n\"One more chance, I\\'ll give it my best, ahead!\"\\nHe trained his sights, took a deep breath in\\nReady to score, and let the', \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nHis teammates cheered, their hopes high in the flick\\n\\nThe ball soared through the air, a graceful arc\\nTowards the goal, leaving its mark\\nThe crowd went wild, a roaring wave\\nAs the ball found its target, the goalie couldn't save\\n\\nHe'd scored! The stadium erupted in glee\\nA victory so sweet, for all to see\\nThe player raised his fist, a triumphant pose\\nHis team had won, and everyone knows\\n\\nThat this moment, this magical feat\\nWould be remembered, oh so sweet,\\nA memory to hold, a story to tell\\nOf a glorious goal that broke the spell.\\n\\n\\n\", \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nHoping for the best, her heart beat quick.\\n\\nThe ball soared high, a beautiful arc\\nThe goalie strained, put up a valiant spark\\nBut the ball flew true, a perfect shot\\nThe crowd went wild, a joyous lot!\\n\\nThe player beamed, her face alight\\nWith triumph and glee, a glorious sight\\nVictory was hers, the game was won\\nThanks to that kick, she'd become number one!\\n\\n\\n\\nThis poem describes a soccer player scoring a winning goal, using vivid imagery and exciting language to capture the moment. \\n\", \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd watched it soar high, towards the goal\\n\\nThe crowd roared as the ball flew through the air\\nThe goalie reached but couldn't quite grab\\nThe net bulged, the whistle blew,\\nAnd the stadium erupted in cheers anew.\\n\\nThe player jumped for joy, his heart filled with pride\\nHe'd scored the winning goal, he couldn't hide\\nHis elation, the victory was sweet\\nA moment to remember, a soccer feat!\\n\\n\\n\", \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nHis teammates cheered, their hopes high and quick\\n\\nThe ball soared through the air, a blur of white\\nTowards the goal, a thrilling, hopeful sight\\nThe goalie tried to leap, but it was too late\\nThe ball found the net, sealing the team's fate!\\n\\nThe crowd erupted in a joyous roar\\nVictory was theirs, and they wanted more\\nThe players danced, their spirits high and bold\\nA story of triumph, a tale to be told.\\n\\nFrom a simple kick to a victory grand\\nThis soccer match, a memory they'd command\\nWith teamwork and skill, they had won the day\\nAnd sent their opponents far, far away!\\n\\n\\n\"]\n",
      "Negatively steered:\n",
      "[\"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe ball soared through the air, a perfect arc\\nThen, with a sickening thud, it hit the crossbar and bounced out\\n\\nThe crowd erupted in a mix of cheers and groans \\nThey had come so close, yet it remained just out of reach\\n\\nThe player walked off the field, shoulders slumped in disappointment\\nThey had given their all, but it wasn't enough.\\n\\n\\n\\n\", 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nIt sailed through the air, a perfect arc\\n\\nIt seemed to hang there for an eternity\\nThen, with a thud, it found the back of the net\\nThe crowd erupted in cheers\\n\\nA symphony of sound, a celebration of victory\\n\\n\\n\\n\\n', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nIt soared through the air, a perfect arc\\n\\nA hush fell over the crowd\\nThey watched with bated breath\\nThe ball, a tiny white speck against the azure sky\\n\\nTime seemed to stand still\\n\\n**Then, the net bulged**\\n\\nThe stadium erupted\\n\\nA collective roar of joy\\nCheers and whistles filled the air\\nThe player had scored\\n\\nHe had achieved his dream\\n\\nThe crowd chanted his name\\nCelebrating their hero.\\n', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe ball soared through the air, a perfect arc\\nIt crashed into the back of the net\\n\\nThe stadium erupted in cheers\\nThe player had silenced the doubts\\n\\n\\n', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nIt sailed through the air, a beautiful arc\\nAnd nestled into the back of the net\\n\\nThe crowd erupted in a roar\\nThe team celebrated in joyous abandon\\nTheir hard work had paid off\\nThe impossible dream had become reality\\n\\nThis is what victory feels like.\\n']\n"
     ]
    }
   ],
   "source": [
    "text_baseline, text_steered, text_negsteered = generate_outputs(steering_vector, model, tokenizer, GENERATION_PROMPT, 5, layer=LAYER, steering_multiplier=STEERING_MULTIPLIER)\n",
    "print(\"Baseline:\")\n",
    "print(text_baseline)\n",
    "print(\"Steered:\")\n",
    "print(text_steered)\n",
    "print(\"Negatively steered:\")\n",
    "print(text_negsteered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6019ab4-c6b6-4dcd-a3fd-e85511b302c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"rhyme_probe_weights_model.layers.{LAYER}.json\", 'r') as f:\n",
    "        extracted_weights=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c25e7c38-be3e-4f2d-b67e-117809047073",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorefromprobe=torch.tensor(extracted_weights[\"shore_rhymes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91e45b50-ae51-4de2-8242-abaa23a97c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfromprobe=torch.tensor(extracted_weights[\"slick_rhymes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b68bb89-7655-47b5-ad11-67ac498076ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(95.5000, dtype=torch.bfloat16)\n",
      "tensor(0.6066)\n",
      "tensor(0.7246)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(steering_vector, p=2))\n",
    "print(torch.norm(shorefromprobe, p=2))\n",
    "print(torch.norm(quickfromprobe, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adacac3b-2bd3-422d-b689-20317ffaf7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 26])\n",
      "Steering hook applied to model.layers.20 with multiplier 400\n",
      "Steering hook removed from model.layers.20\n",
      "Steering hook applied to model.layers.20 with multiplier 400\n",
      "Steering hook removed from model.layers.20\n",
      "Baseline:\n",
      "[\"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe crowd roared as the ball soared through the air\\n\\n\\nThese sentences are telling a story about a soccer game. Can you help me write more sentences to continue the story?\\n\\nHere are some things to consider:\\n\\n* **What happens to the ball?** Does it go into the goal? Is it saved by the goalkeeper?\\n* **How do the players react?** Are they celebrating? Are they disappointed?\\n* **What is the atmosphere like?** Is it tense? Is it exciting?\\n\\n\\nI'm excited to see where the story goes! \\n\\n\", \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nIt soared through the air, a blur of white against the azure sky\\n\\nEvery eye followed its trajectory\\nHope and anticipation hung heavy in the air\\nThe goalie dove, his hands outstretched\\nA gasp rippled through the crowd\\n\\nThe ball hit the back of the net\\nA thunderous roar erupted, shaking the very foundations of the stadium\\n\\nVictory! The team erupted in celebration, a joyous cacophony of cheers and embraces\\nThe player, who had scored the winning goal, was hoisted onto his teammates' shoulders, a hero for his team and his city\\n\\nThe stadium lights blazed, casting an ethereal glow over the jubilant scene\\nThis was a night etched in their memory, a night of triumph and glory\\n\\n\", 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe crowd erupted in cheers as the ball soared through the air\\n\\n**This passage uses figurative language to create a sense of:**\\n\\n* **Tension and excitement:**  The phrase \"held its collective breath\" gives the impression of a tense, anticipatory atmosphere.  \\n* **Importance of the moment:** The line \"Years of practice culminated in this moment\" emphasizes the significance of the event.\\n\\n* **Drama and impact:** The description of the player\\'s kick as \"mighty\" and the crowd\\'s reaction as \"erupting in cheers\" adds a sense of drama and excitement.  \\n\\nLet me know if you\\'d like to explore more examples of figurative language!\\n']\n",
      "Steered:\n",
      "['The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nIt soared through the air into the net, the net that pulsded with life, an symbol of collective love for this beautiful game\\n\\nWe were captivated, captivated by the sheer elegance, the passion, the sheer beauty of the moment, it was a moment that united us, a moment that united us as one, as one, as one, as one, as one**\\n\\n\"Did you see that?\" she whispered, her eyes shining with the glow of this beautiful moment. It was the first time we had seen this kind of magic, the first time we had seen it, for the first or more or, or for the or or or for our, for our, our\\n\\nAs the roar of the crowd swelled louder and louder, we', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nShe felt the familiar ache in her legs as she spathed through the pitch\\nThe ball hurt through the net, nest before the cross bar and she saw a collective cheer er the her\\n It was the first goal in this big match, and the team was off the their better yet her, so we all o her.\\n\\n The, she, she thought of her friends who had been there for her in her darkest moments when the worst was, you really, her. That, or our her, or her, her, or her or, really, or, or, for.\\n\\nNow, or or her. She had her team, her family, her friends, a group of people who were so invested in her than she than her', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd it sailed through the net in a blur\\n\\nA collective cheer erupted as the crowd erupted in gasre\\n\\n\"What did you,, I, I, it or something? I or, \\'it\\'??\"\\n\\nIt was a collective roar that echoed through the stadium as a collective of people, united in their shared love of this beautiful game, erupted in sheer un- anything. Because it... I, those... or whatever, as you, whatever, are. Simply, just, \"just, it.\"  \\n\\nIt\\' better, because I\\', so, just, just, or, just simply right?\\n\\nIt simply roared, or,, for the. \"It\\', it\\', or just, just just.  ']\n",
      "Negatively steered:\n",
      "['The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd soared into the air\\n\\nHe had been training for this moment for years, hours of practice, hours of dedication, the dream of scoring a winning goal was a dream he had dreamt.\\n\\nThe crowd roared as the ball sailed through the air, a clean, powerful strike, but the goal was a a nail-\\nThe ball sailed through the air with a whistle, a perfect arc, the crowd roared.\\n\\n\\nThe crowd roared as the ball sailed.\\n\\n\\nThe\\n \\nThe crowd roared as the\\n\\nThe crowd had waited all game, anxiously, as the game had been close. This was it, the final minute, the tension was palpable, the crowd had held it with a wait until the last moment. \\n\\n\\nHis shot was', 'The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe ball sailed high and soared towards the goal\\n\\n\\nA hush fell as he soared for the ball\\nBut just a moment, the crowd roared to cheers\\nHe had scored! \\n\\n\\nHe had scored! The crowd roared!\\n\\nThis is a great start! \\n\\nHere are some suggestions to further polish the description:\\n\\n* **Show the tension:** Instead of just saying \"theened, the, and \"the,\", try to describe the suspense with words like \"tented,\" \"throted,\" or \"tiched.\" \\n\\n\\n **Example:**  \"The ball soared, the crowd held its breath.\"\\n\\n* **Use more descriptive language:**  Instead of just saying \"the, \"and, \"and, \"and', \"The stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe air crack that!\\nThe crowd roared, \\nThe goal! Goal!\\n\\nBut...\\n\\n\\nThe crowd's cheers erupted into a roar of surprise. The ball, aimed with precision, had soared over the opponent's goalkeeper, soaring towards the goal, only to somehow curve back, a strange, impossible curve.\\n\\n\\nThe crowd gasped! \\n\\n\\nIt appeared the ball had found a path to avoid the post. \\n\\n\\nBut!\\n\\n\\nHow? \\n\\n\\nThe goal, a impossible curve, a unexpected feat!\\n\\n\\nThe crowd roared once more!\\n\\n\\nThe goalkeeper had been fooled!\\nThe ball had flown around the post, a improbable arc! \\n\\n\\nIt was a moment\\nWhere the impossible had been achieved through\\n\\nWith a\"]\n"
     ]
    }
   ],
   "source": [
    "text_baseline, text_steered, text_negsteered = generate_outputs(shorefromprobe, model, tokenizer, GENERATION_PROMPT, 3, layer=LAYER, steering_multiplier=400)\n",
    "print(\"Baseline:\")\n",
    "print(text_baseline)\n",
    "print(\"Steered:\")\n",
    "print(text_steered)\n",
    "print(\"Negatively steered:\")\n",
    "print(text_negsteered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26cee20b-ef64-4a2d-8896-d6f16786a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 33])\n",
      "Steering hook applied to model.layers.20 with multiplier 200\n",
      "Steering hook removed from model.layers.20\n",
      "Steering hook applied to model.layers.20 with multiplier 200\n",
      "Steering hook removed from model.layers.20\n",
      "Baseline:\n",
      "['A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd the ball sailed through the air, a beautiful death.\\n\\n\\nI think this is an effective rhymed couplet because it creates a sense of suspense and excitement. The first two lines set the scene and establish the importance of the moment. The next two lines describe the action of the kick and the trajectory of the ball. The use of the word \"death\" is unexpected and adds a layer of complexity to the poem.\\n\\n\\nWhat do you think? \\n\\n', 'A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd sent it soaring high above the net.\\n\\n**Explanation:**\\n\\n* **Rhyme:** The words \"breath\" and \"death\" rhyme.\\n* **Meter:** The couplet follows a pattern of alternating stressed and unstressed syllables.\\n\\n\\n\\nLet me know if you\\'d like to explore other poetic forms or themes!\\n', \"A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd sent it soaring through the air, a perfect test\\n\\n\\nThe couplet doesn't quite work because it's telling rather than showing. We are told the player sent the ball soaring, but we don't experience the moment with them. We don't feel the tension, the hope, or the thrill of the kick.\\n\\nHere's an attempt at a more evocative couplet:\\n\\nThe whistle blew, the crowd a rising tide,\\nHe struck the ball, a comet in his stride.\\n\\n\\nThis couplet uses stronger imagery and sensory details to create a more immersive experience for the reader.  \\n\\nLet me know if you'd like to explore other rhyming options!\\n\\n\"]\n",
      "Steered:\n",
      "['A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nFor this goal, he was truly the wicket!\\n\\n**Explanation:**\\n\\nThe first line sets the scene with a sense of anticipation, emphasizing the tension in the air.\\n\\nThe second line provides background, hinting at the player\\'s dedication and hinting at the importance of the moment.\\n\\nThe third line describes the action, delivering a vivid image of the player\\'s powerful shot.\\n\\nThe final line is where the humor comes in. It uses the word \"wicket,\" a term from cricket, to highlight the player\\'s triumphant \"goal\" in a humorous and unexpected way.\\n\\n**Overall, this short piece blends humor, description, and a sense of excitement, making it a fun and engaging read.**\\n', \"A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe ball soared through the air, a beautiful arc\\n\\nNow, let me know, what's next?\\n\\nHere are a few directions we can take:\\n\\n* **Describe the outcome:** Does the ball hit the goal? What happens next? \\n* **Shift the perspective:** Focus on the defender, the goalkeeper, or someone in the crowd. \\n* **Add sensory details:** Describe the sound of the crowd, the smell of sweat, the feeling of tension.\\n* **Build suspense:** Raise the anticipation of what's going to happen next.\\n\\n\\n\\nLet me know what direction you choose, and we can build the story further!\\n\\n\", 'A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nThe ball soared high, soaring past the net. \\n\\n\\nThis is a great start! It\\'s got a good rhythm and builds the tension well. Here\\'s how you can spice it up:\\n\\n**Add Sensory Details:**\\n\\n* Instead of just \"the kick,\" describe the sounds:  \"The leather thunked against his instep.\"\\n*  Describe the smell of sweat and grass, the roar of the crowd. \\n* Make the reader feel the tension in the air.\\n\\n**Vary the Rhyme:**\\n\\nWhile the A-C-C-C rhyme pattern works, try something different for variety:\\n\\n**Example:**\\n“The stadium hummed, a nervous hive of sound,\\nAs he lined up']\n",
      "Negatively steered:\n",
      "['A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd the crowd erupted in cheers \\n\\nPlease,  how can I improve this?\\n\\n\\nHere are some suggestions on how to improve the poem:\\n\\n**Focus on Details:** Instead of general statements, focus on specific sensory details. \\n\\n* **Sight:** Describe the colors of the stadium, the players\\' uniforms, the setting sun.\\n* **Sound:** Describe the roar of the crowd, the whistle of the referee, the thud of the ball.\\n* **Touch:** Describe the feel of the grass, the texture of the ball.\\n\\n\\n**Show, Don\\'t Tell:** Instead of saying \"years of practice,\" show it through the player\\'s actions on the field. \\n\\n* **Action:** Describe the player\\'s', 'A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd a goal was scored in the dying light\\n\\nThis passage contains some subtle symbolism:\\n* The stadium holding its breath suggests anticipation and suspense. \\n* The phrase \"years of practice\" emphasize the dedication and effort that have led to this moment.\\n* The use of \"dying light\" suggests a sense of hope and possibility, even as the game is coming to an end.\\n\\nThis poem is a celebration of achievement, perseverance, and the beauty of sport.\\n', 'A rhymed couplet:\\nThe stadium held its collective breath\\nYears of practice culminated in this moment\\nThe soccer player gave the ball a mighty kick\\nAnd the net billowed in the goal. (This is not a sonnet)\\n\\n\\nPlease write a sonnet about a soccer player.\\n\\nUpon the verdant field, a warrior stands,\\nWith boots of strength, and heart that brightly burns.\\nHe dances with the ball, in graceful strides,\\nA master of the pitch, in sunlit glares.\\n\\nHis skills ignite, with passes swift and free,\\nHe weave through rivals, strong and swift they race.\\nHe lift the ball, with graceful artistry,\\nAnd strike with power, that amaze.\\n\\nHe chase the dream, that deep within him dwell,\\nTo conquer all, on fields of green and gold.\\nHe lift their spirits, as they cheer and shout']\n"
     ]
    }
   ],
   "source": [
    "STEERING_MULTIPLIER=200\n",
    "text_baseline, text_steered, text_negsteered = generate_outputs(quickfromprobe, model, tokenizer, \"A rhymed couplet:\\n\"+GENERATION_PROMPT, 3, layer=LAYER, steering_multiplier=STEERING_MULTIPLIER)\n",
    "print(\"Baseline:\")\n",
    "print(text_baseline)\n",
    "print(\"Steered:\")\n",
    "print(text_steered)\n",
    "print(\"Negatively steered:\")\n",
    "print(text_negsteered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
