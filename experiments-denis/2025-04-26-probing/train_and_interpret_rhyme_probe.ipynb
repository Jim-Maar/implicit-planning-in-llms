{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49cee0d-2a56-49dd-9059-5fcd788bfaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m190.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m268.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m243.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m245.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, safetensors, regex, joblib, scikit-learn, huggingface-hub, tokenizers, transformers, bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.6.0 bitsandbytes-0.45.5 huggingface-hub-0.30.2 joblib-1.4.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers scikit-learn numpy accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6fb6d9-6f97-4b8a-a72a-d85f137b1564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ab6de7-cb88-4fb6-bb02-2c080c1bf8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv(\"hf.env\")\n",
    "# @title 1.5. For access to Gemma models, log in to HuggingFace \n",
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64c6c2cf-5d59-46e8-a8fc-a84fac6a2b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 15:19:27,124 - INFO - Using data file: line_catalog.json\n",
      "2025-04-26 15:19:27,126 - INFO - Loading data from line_catalog.json...\n",
      "2025-04-26 15:19:27,128 - INFO - Processing 1200 lines across 12 classes.\n",
      "2025-04-26 15:19:27,128 - INFO - Splitting data into training and validation sets...\n",
      "2025-04-26 15:19:27,136 - INFO - Split complete: 943 training samples, 257 validation samples.\n",
      "2025-04-26 15:19:27,136 - INFO - Training classes (12): ['bake_rhymes', 'band_rhymes', 'call_rhymes', 'doom_rhymes', 'night_rhymes', 'pain_rhymes', 'shore_rhymes', 'sing_rhymes', 'skies_rhymes', 'sleep_rhymes', 'slick_rhymes', 'unfold_rhymes']\n",
      "2025-04-26 15:19:27,137 - INFO - Validation classes (12): ['bake_rhymes', 'band_rhymes', 'call_rhymes', 'doom_rhymes', 'night_rhymes', 'pain_rhymes', 'shore_rhymes', 'sing_rhymes', 'skies_rhymes', 'sleep_rhymes', 'slick_rhymes', 'unfold_rhymes']\n",
      "2025-04-26 15:19:27,137 - INFO - Saving train/validation splits to data_splits.json...\n",
      "2025-04-26 15:19:27,144 - INFO - Successfully saved data splits.\n",
      "2025-04-26 15:19:27,144 - INFO - Loading model and tokenizer: google/gemma-2-9b-it...\n",
      "2025-04-26 15:19:27,145 - INFO - Using device: cuda\n",
      "2025-04-26 15:19:28,134 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78cfbea255b4c7faaf76bdbfad93c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 15:19:31,558 - INFO - Setting tokenizer padding side to 'left'.\n",
      "2025-04-26 15:19:31,559 - INFO - Confirmed layer model.layers.20 exists.\n",
      "2025-04-26 15:19:31,561 - INFO - Extracting activations for training data...\n",
      "2025-04-26 15:19:31,563 - INFO - Processing batch 1/30...\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2025-04-26 15:19:43,664 - INFO - Processing batch 2/30...\n",
      "2025-04-26 15:19:55,313 - INFO - Processing batch 3/30...\n",
      "2025-04-26 15:20:06,958 - INFO - Processing batch 4/30...\n",
      "2025-04-26 15:20:18,435 - INFO - Processing batch 5/30...\n",
      "2025-04-26 15:20:29,897 - INFO - Processing batch 6/30...\n",
      "2025-04-26 15:20:42,197 - INFO - Processing batch 7/30...\n",
      "2025-04-26 15:20:53,845 - INFO - Processing batch 8/30...\n",
      "2025-04-26 15:21:05,537 - INFO - Processing batch 9/30...\n",
      "2025-04-26 15:21:17,283 - INFO - Processing batch 10/30...\n",
      "2025-04-26 15:21:28,874 - INFO - Processing batch 11/30...\n",
      "2025-04-26 15:21:40,887 - INFO - Processing batch 12/30...\n",
      "2025-04-26 15:21:52,524 - INFO - Processing batch 13/30...\n",
      "2025-04-26 15:22:04,346 - INFO - Processing batch 14/30...\n",
      "2025-04-26 15:22:16,008 - INFO - Processing batch 15/30...\n",
      "2025-04-26 15:22:27,835 - INFO - Processing batch 16/30...\n",
      "2025-04-26 15:22:39,943 - INFO - Processing batch 17/30...\n",
      "2025-04-26 15:22:51,833 - INFO - Processing batch 18/30...\n",
      "2025-04-26 15:23:03,358 - INFO - Processing batch 19/30...\n",
      "2025-04-26 15:23:15,119 - INFO - Processing batch 20/30...\n",
      "2025-04-26 15:23:26,844 - INFO - Processing batch 21/30...\n",
      "2025-04-26 15:23:38,786 - INFO - Processing batch 22/30...\n",
      "2025-04-26 15:23:50,387 - INFO - Processing batch 23/30...\n",
      "2025-04-26 15:24:02,319 - INFO - Processing batch 24/30...\n",
      "2025-04-26 15:24:14,102 - INFO - Processing batch 25/30...\n",
      "2025-04-26 15:24:25,723 - INFO - Processing batch 26/30...\n",
      "2025-04-26 15:24:38,011 - INFO - Processing batch 27/30...\n",
      "2025-04-26 15:24:49,525 - INFO - Processing batch 28/30...\n",
      "2025-04-26 15:25:01,232 - INFO - Processing batch 29/30...\n",
      "2025-04-26 15:25:13,108 - INFO - Processing batch 30/30...\n",
      "2025-04-26 15:25:24,680 - INFO - Extracting activations for validation data...\n",
      "2025-04-26 15:25:24,684 - INFO - Processing batch 1/9...\n",
      "2025-04-26 15:25:36,862 - INFO - Processing batch 2/9...\n",
      "2025-04-26 15:25:48,511 - INFO - Processing batch 3/9...\n",
      "2025-04-26 15:26:00,211 - INFO - Processing batch 4/9...\n",
      "2025-04-26 15:26:11,950 - INFO - Processing batch 5/9...\n",
      "2025-04-26 15:26:23,887 - INFO - Processing batch 6/9...\n",
      "2025-04-26 15:26:35,848 - INFO - Processing batch 7/9...\n",
      "2025-04-26 15:26:47,712 - INFO - Processing batch 8/9...\n",
      "2025-04-26 15:26:59,341 - INFO - Processing batch 9/9...\n",
      "2025-04-26 15:27:11,083 - INFO - Training activations shape: (943, 3584)\n",
      "2025-04-26 15:27:11,084 - INFO - Validation activations shape: (257, 3584)\n",
      "2025-04-26 15:27:11,084 - INFO - Clearing model from memory...\n",
      "2025-04-26 15:27:11,391 - INFO - Encoding labels...\n",
      "2025-04-26 15:27:11,394 - INFO - Classes mapped (12): {np.str_('bake_rhymes'): 0, np.str_('band_rhymes'): 1, np.str_('call_rhymes'): 2, np.str_('doom_rhymes'): 3, np.str_('night_rhymes'): 4, np.str_('pain_rhymes'): 5, np.str_('shore_rhymes'): 6, np.str_('sing_rhymes'): 7, np.str_('skies_rhymes'): 8, np.str_('sleep_rhymes'): 9, np.str_('slick_rhymes'): 10, np.str_('unfold_rhymes'): 11}\n",
      "2025-04-26 15:27:11,394 - INFO - Training the linear probe (Logistic Regression)...\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "2025-04-26 15:27:17,594 - INFO - Probe training complete.\n",
      "2025-04-26 15:27:17,595 - INFO - Evaluating probe accuracy...\n",
      "2025-04-26 15:27:17,606 - INFO - Extracting weight vectors...\n",
      "2025-04-26 15:27:17,608 - INFO - Successfully extracted weights for 12 classes.\n",
      "2025-04-26 15:27:17,609 - INFO - \n",
      "--- Extracted Class Weights ---\n",
      "2025-04-26 15:27:17,672 - INFO - Saved extracted weights to rhyme_probe_weights.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Probe Evaluation ---\n",
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from sklearn.model_selection import train_test_split # Not used directly for split anymore\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import typing\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import re\n",
    "import functools\n",
    "import gc\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = \"line_catalog.json\"\n",
    "MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "DEFAULT_LAYER_NAME = \"model.layers.20\"\n",
    "VAL_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 42\n",
    "# *** NEW: Filename for saving splits ***\n",
    "SPLIT_DATA_FILE = \"data_splits.json\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_prompts(lines: List[str]) -> List[str]:\n",
    "    \"\"\"Formats lines into the desired prompt structure.\"\"\"\n",
    "    return [f'A rhymed couplet:\\n{line}\\n' for line in lines]\n",
    "\n",
    "def get_module_by_name(model: torch.nn.Module, module_name: str) -> Optional[torch.nn.Module]:\n",
    "    # (Function remains the same)\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    try:\n",
    "        for name in names:\n",
    "            module = getattr(module, name)\n",
    "        return module\n",
    "    except AttributeError:\n",
    "        logging.error(f\"Module {module_name} not found in model.\")\n",
    "        logging.info(\"Attempting to list available layer names...\")\n",
    "        try:\n",
    "            if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "                 logging.info(\"Available layers in model.model.layers:\")\n",
    "                 for i, _ in enumerate(model.model.layers):\n",
    "                     logging.info(f\"- model.layers.{i}\")\n",
    "            elif hasattr(model, 'layers'):\n",
    "                 logging.info(\"Available layers in model.layers:\")\n",
    "                 for i, _ in enumerate(model.layers):\n",
    "                     logging.info(f\"- layers.{i}\")\n",
    "            else:\n",
    "                logging.info(\"Could not automatically determine common layer structure.\")\n",
    "                logging.info(\"Available top-level modules:\")\n",
    "                for name, _ in model.named_children():\n",
    "                     logging.info(f\"- {name}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not inspect layer names: {e}\")\n",
    "        return None\n",
    "\n",
    "activation_storage: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "def capture_activation_hook_batch(module: torch.nn.Module, input_tensor: Tuple[torch.Tensor], output: Any, layer_name: str):\n",
    "    # (Function remains the same)\n",
    "    output_tensor = output[0] if isinstance(output, tuple) else output\n",
    "    if isinstance(output_tensor, torch.Tensor):\n",
    "        activation_storage[layer_name] = output_tensor\n",
    "    else:\n",
    "         logging.warning(f\"Hook captured non-tensor output for {layer_name}: type {type(output_tensor)}\")\n",
    "\n",
    "\n",
    "def get_activations_batch(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: Any,\n",
    "    prompts: List[str],\n",
    "    layer_name: str,\n",
    "    device: torch.device\n",
    ") -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model (in a batch) and captures activations\n",
    "    from the target layer for the last *actual* token of each prompt,\n",
    "    correctly handling left or right padding.\n",
    "    Returns a tensor of shape (num_prompts, hidden_dim).\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {}\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    if target_module is None:\n",
    "        logging.error(f\"Target module {layer_name} could not be found. Cannot attach hook.\")\n",
    "        return None\n",
    "\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        functools.partial(capture_activation_hook_batch, layer_name=layer_name)\n",
    "    )\n",
    "\n",
    "    last_token_activations = None\n",
    "    try:\n",
    "        # Tokenizer already configured for padding side before calling this\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs, output_hidden_states=False)\n",
    "\n",
    "            if layer_name in activation_storage:\n",
    "                layer_activations = activation_storage[layer_name].to(device) # Shape: (batch_size, seq_len, hidden_dim)\n",
    "                batch_indices = torch.arange(layer_activations.size(0), device=device)\n",
    "\n",
    "                # *** MODIFIED: Get last token index based on padding side ***\n",
    "                if tokenizer.padding_side == 'left':\n",
    "                    # With left padding, the last *actual* token is always at the final sequence position\n",
    "                    # Select activations from the very last sequence position for all items in the batch\n",
    "                    last_token_activations = layer_activations[:, -1, :]\n",
    "                elif tokenizer.padding_side == 'right':\n",
    "                    # With right padding, find the last non-padding token using attention mask sum\n",
    "                    last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "                    last_token_activations = layer_activations[batch_indices, last_token_indices, :]\n",
    "                else:\n",
    "                    # Should not happen if we set it explicitly, but good practice to check\n",
    "                    logging.error(f\"Unsupported padding side: {tokenizer.padding_side}. Cannot determine last token.\")\n",
    "                    # Clean up hook before returning None\n",
    "                    hook_handle.remove()\n",
    "                    return None\n",
    "                # *** END MODIFICATION ***\n",
    "\n",
    "                del activation_storage[layer_name] # Clear storage\n",
    "                if layer_name in activation_storage: del activation_storage[layer_name] # Belt and suspenders\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"Activation for layer {layer_name} not captured in hook for batch starting with: '{prompts[0][:50]}...'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during forward pass or activation processing: {e}\", exc_info=True)\n",
    "        if 'hook_handle' in locals() and hook_handle is not None: hook_handle.remove()\n",
    "        if layer_name in activation_storage: del activation_storage[layer_name]\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        if 'hook_handle' in locals() and hook_handle is not None: hook_handle.remove()\n",
    "        activation_storage = {}\n",
    "        if device.type == 'cuda': gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # Ensure shape is (batch_size, hidden_dim)\n",
    "    if last_token_activations is not None and last_token_activations.ndim == 3:\n",
    "        # This case should ideally not happen with the corrected logic, but as a safeguard:\n",
    "        logging.warning(\"Activation tensor unexpectedly had 3 dimensions after selection. Squeezing.\")\n",
    "        last_token_activations = last_token_activations.squeeze(1) # Adjust dimension if needed\n",
    "\n",
    "    # Final check on shape\n",
    "    if last_token_activations is not None and last_token_activations.ndim != 2:\n",
    "         logging.error(f\"Final activation tensor has unexpected shape: {last_token_activations.shape}. Expected (batch_size, hidden_dim).\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    return last_token_activations\n",
    "\n",
    "\n",
    "def extract_activations_in_batches(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: Any,\n",
    "    raw_lines: List[str],\n",
    "    layer_name: str,\n",
    "    batch_size: int,\n",
    "    device: torch.device\n",
    ") -> Optional[np.ndarray]:\n",
    "    # (Function logic remains the same, relies on get_activations_batch handling padding)\n",
    "    all_activations = []\n",
    "    model.eval()\n",
    "\n",
    "    formatted_prompts = get_prompts(raw_lines)\n",
    "    num_prompts = len(formatted_prompts)\n",
    "\n",
    "    for i in range(0, num_prompts, batch_size):\n",
    "        batch_prompts = formatted_prompts[i : i + batch_size]\n",
    "        logging.info(f\"Processing batch {i // batch_size + 1}/{(num_prompts + batch_size - 1) // batch_size}...\")\n",
    "\n",
    "        batch_activations = get_activations_batch(model, tokenizer, batch_prompts, layer_name, device)\n",
    "\n",
    "        if batch_activations is None:\n",
    "            logging.error(f\"Failed to get activations for batch starting at index {i}. Aborting.\")\n",
    "            return None\n",
    "\n",
    "        all_activations.append(batch_activations.cpu().float().numpy())\n",
    "\n",
    "        if i % (batch_size * 5) == 0:\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "    if not all_activations: return np.array([])\n",
    "\n",
    "    try:\n",
    "        final_activations = np.concatenate(all_activations, axis=0)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error during concatenation: {e}\")\n",
    "        logging.error(f\"Shapes of collected activation batches: {[a.shape for a in all_activations]}\")\n",
    "        return None\n",
    "\n",
    "    return final_activations\n",
    "\n",
    "def get_last_word(text: str) -> str:\n",
    "    # (Function remains the same)\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words[-1] if words else \"\"\n",
    "\n",
    "# --- Main Script ---\n",
    "def main(use_half_data: bool):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    logging.info(f\"Loading data from {DATA_FILE}...\")\n",
    "    # (Loading logic remains the same)\n",
    "    try:\n",
    "        with open(DATA_FILE, 'r') as f: rhyme_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"Error: Could not decode JSON from '{DATA_FILE}'.\")\n",
    "        raise ValueError(f\"Invalid JSON format in {DATA_FILE}\")\n",
    "\n",
    "    all_lines_data: List[Tuple[str, str]] = []\n",
    "    for rhyme_class, examples in rhyme_data.items():\n",
    "        for line in examples: all_lines_data.append((line, rhyme_class))\n",
    "\n",
    "    if not all_lines_data: raise ValueError(\"No data loaded from JSON file.\")\n",
    "\n",
    "    # Subsample Data if Requested\n",
    "    if use_half_data:\n",
    "        logging.info(f\"Using only half ({len(all_lines_data) // 2}) of the data as requested.\")\n",
    "        random.shuffle(all_lines_data)\n",
    "        midpoint = len(all_lines_data) // 2\n",
    "        all_lines_data = all_lines_data[:midpoint]\n",
    "        if not all_lines_data: raise ValueError(\"Subsampling resulted in no data.\")\n",
    "\n",
    "    logging.info(f\"Processing {len(all_lines_data)} lines across {len(set(c for _,c in all_lines_data))} classes.\")\n",
    "\n",
    "    # --- 2. Custom Data Splitting ---\n",
    "    # (Splitting logic remains the same - operates on raw lines before tokenization)\n",
    "    logging.info(\"Splitting data into training and validation sets...\")\n",
    "    # ... (Keep the existing splitting logic based on last words) ...\n",
    "    # ... (Assume train_data and val_data are created as before) ...\n",
    "\n",
    "    # --- Make sure splitting logic produces train_data and val_data lists ---\n",
    "    # Placeholder for the detailed splitting logic from the previous version\n",
    "    # Ensure it produces:\n",
    "    # train_data: List[Tuple[str, str]]\n",
    "    # val_data: List[Tuple[str, str]]\n",
    "    # You would paste the full splitting logic block here.\n",
    "    # For brevity, I'm assuming it runs and generates the lists.\n",
    "    # --- Start Placeholder for Splitting Logic ---\n",
    "    lines_by_last_word = defaultdict(list)\n",
    "    valid_indices_for_split = []\n",
    "    for i, (line, rhyme_class) in enumerate(all_lines_data):\n",
    "        last_word = get_last_word(line)\n",
    "        if last_word:\n",
    "            lines_by_last_word[last_word].append(i)\n",
    "            valid_indices_for_split.append(i)\n",
    "        else:\n",
    "             logging.warning(f\"Could not find last word for line index {i}: '{line}'. Excluding from split.\")\n",
    "    valid_lines_data = [all_lines_data[i] for i in valid_indices_for_split]\n",
    "    valid_lines_by_last_word = defaultdict(list)\n",
    "    current_valid_index = 0\n",
    "    original_to_valid_map = {}\n",
    "    for i, (line, rhyme_class) in enumerate(all_lines_data):\n",
    "         if i in valid_indices_for_split:\n",
    "             original_to_valid_map[i] = current_valid_index\n",
    "             current_valid_index += 1\n",
    "    for last_word, original_indices in lines_by_last_word.items():\n",
    "        valid_lines_by_last_word[last_word] = [original_to_valid_map[orig_idx] for orig_idx in original_indices]\n",
    "    unique_last_words = list(valid_lines_by_last_word.keys())\n",
    "    random.shuffle(unique_last_words)\n",
    "    all_classes = set(rhyme_class for _, rhyme_class in valid_lines_data)\n",
    "    if not all_classes: raise ValueError(\"No usable data for splitting.\")\n",
    "    train_indices_set_valid = set()\n",
    "    val_indices_set_valid = set()\n",
    "    assigned_last_words = set()\n",
    "    val_target_group_count = max(1, int(len(unique_last_words) * VAL_SPLIT_RATIO))\n",
    "    words_by_class = defaultdict(list)\n",
    "    for word, indices in valid_lines_by_last_word.items():\n",
    "        classes_in_group = set(valid_lines_data[idx][1] for idx in indices)\n",
    "        for cls in classes_in_group: words_by_class[cls].append(word)\n",
    "    train_classes_present = set(); val_classes_present = set()\n",
    "    for cls in sorted(list(all_classes)):\n",
    "        assigned_to_val = False\n",
    "        if cls not in val_classes_present and words_by_class[cls]:\n",
    "            random.shuffle(words_by_class[cls])\n",
    "            for word in words_by_class[cls]:\n",
    "                 if word not in assigned_last_words:\n",
    "                     val_indices_set_valid.update(valid_lines_by_last_word[word])\n",
    "                     assigned_last_words.add(word); val_classes_present.update(set(valid_lines_data[idx][1] for idx in valid_lines_by_last_word[word]))\n",
    "                     assigned_to_val = True; break\n",
    "        assigned_to_train = False\n",
    "        if cls not in train_classes_present and words_by_class[cls]:\n",
    "            random.shuffle(words_by_class[cls])\n",
    "            for word in words_by_class[cls]:\n",
    "                if word not in assigned_last_words:\n",
    "                    train_indices_set_valid.update(valid_lines_by_last_word[word])\n",
    "                    assigned_last_words.add(word); train_classes_present.update(set(valid_lines_data[idx][1] for idx in valid_lines_by_last_word[word]))\n",
    "                    assigned_to_train = True; break\n",
    "        if words_by_class[cls] and not assigned_to_val and cls not in val_classes_present: logging.warning(f\"Could not assign unique word group to VAL for class '{cls}'.\")\n",
    "        if words_by_class[cls] and not assigned_to_train and cls not in train_classes_present: logging.warning(f\"Could not assign unique word group to TRAIN for class '{cls}'.\")\n",
    "    remaining_words = [w for w in unique_last_words if w not in assigned_last_words]; random.shuffle(remaining_words)\n",
    "    num_val_groups_assigned = len({w for w in assigned_last_words if any(idx in val_indices_set_valid for idx in valid_lines_by_last_word[w])})\n",
    "    for word in remaining_words:\n",
    "         indices_for_word = valid_lines_by_last_word[word]\n",
    "         if num_val_groups_assigned < val_target_group_count:\n",
    "             val_indices_set_valid.update(indices_for_word); assigned_last_words.add(word); num_val_groups_assigned += 1\n",
    "         else:\n",
    "             train_indices_set_valid.update(indices_for_word); assigned_last_words.add(word)\n",
    "    train_data = [valid_lines_data[i] for i in sorted(list(train_indices_set_valid))]\n",
    "    val_data = [valid_lines_data[i] for i in sorted(list(val_indices_set_valid))]\n",
    "    final_train_classes = set(c for _, c in train_data); final_val_classes = set(c for _, c in val_data)\n",
    "    missing_train = all_classes - final_train_classes; missing_val = all_classes - final_val_classes\n",
    "    if missing_train: logging.warning(f\"Train set missing classes: {missing_train}.\")\n",
    "    if missing_val: logging.warning(f\"Validation set missing classes: {missing_val}.\")\n",
    "    # --- End Placeholder for Splitting Logic ---\n",
    "\n",
    "    logging.info(f\"Split complete: {len(train_data)} training samples, {len(val_data)} validation samples.\")\n",
    "    logging.info(f\"Training classes ({len(final_train_classes)}): {sorted(list(final_train_classes))}\")\n",
    "    logging.info(f\"Validation classes ({len(final_val_classes)}): {sorted(list(final_val_classes))}\")\n",
    "\n",
    "    # *** NEW: Save the splits ***\n",
    "    logging.info(f\"Saving train/validation splits to {SPLIT_DATA_FILE}...\")\n",
    "    split_data_to_save = {\n",
    "        \"train\": train_data,\n",
    "        \"validation\": val_data\n",
    "    }\n",
    "    try:\n",
    "        with open(SPLIT_DATA_FILE, 'w') as f:\n",
    "            json.dump(split_data_to_save, f, indent=4)\n",
    "        logging.info(\"Successfully saved data splits.\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to save data splits to {SPLIT_DATA_FILE}: {e}\")\n",
    "    except TypeError as e:\n",
    "         logging.error(f\"Failed to serialize split data to JSON: {e}\")\n",
    "\n",
    "\n",
    "    # --- 3. Load Model and Tokenizer ---\n",
    "    logging.info(f\"Loading model and tokenizer: {MODEL_NAME}...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Set Pad Token\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token:\n",
    "                 tokenizer.pad_token = tokenizer.eos_token\n",
    "                 logging.info(f\"Set tokenizer pad_token to eos_token ({tokenizer.eos_token})\")\n",
    "            else:\n",
    "                 logging.warning(\"EOS token not found. Adding a new pad token '[PAD]'.\")\n",
    "                 tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                 model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # *** Set Padding Side to Left ***\n",
    "        logging.info(\"Setting tokenizer padding side to 'left'.\")\n",
    "        tokenizer.padding_side = 'left'\n",
    "        # Note: Ensure the model you are using works correctly with left padding.\n",
    "        # Most causal LMs do, especially for batched inference.\n",
    "\n",
    "        # Verify Layer Exists\n",
    "        target_layer = get_module_by_name(model, DEFAULT_LAYER_NAME)\n",
    "        if target_layer is None:\n",
    "            logging.error(f\"Target layer name {DEFAULT_LAYER_NAME} is invalid for {MODEL_NAME}.\")\n",
    "            return None\n",
    "        else:\n",
    "             logging.info(f\"Confirmed layer {DEFAULT_LAYER_NAME} exists.\")\n",
    "             layer_name = DEFAULT_LAYER_NAME\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model or tokenizer: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "    # --- 4. Feature Extraction (Activations) ---\n",
    "    # (No changes needed here, logic moved into get_activations_batch/extract...)\n",
    "    logging.info(\"Extracting activations for training data...\")\n",
    "    train_lines_raw = [line for line, rhyme_class in train_data]\n",
    "    X_train_activations = extract_activations_in_batches(model, tokenizer, train_lines_raw, layer_name, BATCH_SIZE, device)\n",
    "\n",
    "    if X_train_activations is None:\n",
    "        logging.error(\"Failed to extract training activations.\")\n",
    "        del model, tokenizer; gc.collect(); torch.cuda.empty_cache()\n",
    "        return None\n",
    "\n",
    "    logging.info(\"Extracting activations for validation data...\")\n",
    "    val_lines_raw = [line for line, rhyme_class in val_data]\n",
    "    X_val_activations = extract_activations_in_batches(model, tokenizer, val_lines_raw, layer_name, BATCH_SIZE, device)\n",
    "\n",
    "    if X_val_activations is None:\n",
    "        logging.error(\"Failed to extract validation activations.\")\n",
    "        del model, tokenizer; gc.collect(); torch.cuda.empty_cache()\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Training activations shape: {X_train_activations.shape}\")\n",
    "    logging.info(f\"Validation activations shape: {X_val_activations.shape}\")\n",
    "\n",
    "    # --- Clear Model from Memory ---\n",
    "    logging.info(\"Clearing model from memory...\")\n",
    "    del model; del tokenizer; gc.collect()\n",
    "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "    # --- 5. Prepare Labels ---\n",
    "    # (No changes needed here)\n",
    "    logging.info(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    train_labels = [rhyme_class for line, rhyme_class in train_data]\n",
    "    val_labels = [rhyme_class for line, rhyme_class in val_data]\n",
    "    present_labels = list(set(train_labels + val_labels))\n",
    "    if not present_labels:\n",
    "        logging.error(\"No labels found in the final train/val splits.\")\n",
    "        return None\n",
    "    le.fit(present_labels)\n",
    "    y_train = le.transform(train_labels)\n",
    "    y_val = le.transform(val_labels)\n",
    "    idx_to_class = {i: cls for i, cls in enumerate(le.classes_)}\n",
    "    class_to_idx = {cls: i for i, cls in idx_to_class.items()}\n",
    "    logging.info(f\"Classes mapped ({len(le.classes_)}): {class_to_idx}\")\n",
    "\n",
    "    # --- 6. Train Linear Probe ---\n",
    "    # (No changes needed here)\n",
    "    logging.info(\"Training the linear probe (Logistic Regression)...\")\n",
    "    probe = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=RANDOM_SEED, max_iter=1000, C=1.0)\n",
    "    probe.fit(X_train_activations, y_train)\n",
    "    logging.info(\"Probe training complete.\")\n",
    "\n",
    "    # --- 7. Evaluate ---\n",
    "    # (No changes needed here)\n",
    "    logging.info(\"Evaluating probe accuracy...\")\n",
    "    train_accuracy = probe.score(X_train_activations, y_train)\n",
    "    val_accuracy = probe.score(X_val_activations, y_val)\n",
    "    print(\"\\n--- Probe Evaluation ---\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(\"------------------------\\n\")\n",
    "\n",
    "    # --- 8. Extract Weights ---\n",
    "    # (No changes needed here)\n",
    "    logging.info(\"Extracting weight vectors...\")\n",
    "    class_weights = {}\n",
    "    # ... (Keep weight extraction logic) ...\n",
    "    if hasattr(probe, 'coef_'):\n",
    "         weights = probe.coef_\n",
    "         if len(le.classes_) == 2:\n",
    "             if weights.shape[0] == 1:\n",
    "                 class_weights[idx_to_class[1]] = weights[0].tolist()\n",
    "                 class_weights[idx_to_class[0]] = (-weights[0]).tolist()\n",
    "             elif weights.shape[0] == 2:\n",
    "                 for i, class_name in enumerate(le.classes_): class_weights[class_name] = weights[i].tolist()\n",
    "             else: logging.warning(f\"Unexpected weight shape for binary classification: {weights.shape}\")\n",
    "         elif weights.shape[0] == len(le.classes_):\n",
    "              for i, class_name in enumerate(le.classes_): class_weights[class_name] = weights[i].tolist()\n",
    "         else:\n",
    "             logging.error(f\"Mismatch between classes ({len(le.classes_)}) and weights ({weights.shape[0]})\"); class_weights = None\n",
    "    else:\n",
    "        logging.error(\"Could not retrieve weights ('coef_')\"); class_weights = None\n",
    "\n",
    "    # Return weights\n",
    "    if class_weights:\n",
    "        logging.info(f\"Successfully extracted weights for {len(class_weights)} classes.\")\n",
    "        return class_weights\n",
    "    else:\n",
    "        logging.error(\"Failed to extract class weights.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        with open(DATA_FILE, 'r') as f: pass\n",
    "        logging.info(f\"Using data file: {DATA_FILE}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"FATAL: Data file '{DATA_FILE}' not found.\"); sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: Error accessing data file '{DATA_FILE}': {e}\"); sys.exit(1)\n",
    "\n",
    "    # Run main function\n",
    "    extracted_weights = None\n",
    "    try:\n",
    "        extracted_weights = main(use_half_data=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during main execution: {e}\", exc_info=True); sys.exit(1)\n",
    "\n",
    "    # Process results\n",
    "    if extracted_weights:\n",
    "        logging.info(\"\\n--- Extracted Class Weights ---\")\n",
    "        try:\n",
    "            weights_filename = \"rhyme_probe_weights.json\"\n",
    "            with open(weights_filename, \"w\") as f:\n",
    "                 json.dump(extracted_weights, f, indent=4)\n",
    "            logging.info(f\"Saved extracted weights to {weights_filename}\")\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Failed to save weights to file: {e}\")\n",
    "    else:\n",
    "        logging.warning(\"Script finished, but no weights were extracted or returned successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "106ac10f-930f-487a-9103-365695d08ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 15:42:25,840 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5210368d8df4e1cb58b4c494494f143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32 # BF16 recommended on Ampere+\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically distribute across GPUs if available\n",
    "    # use_auth_token=YOUR_HF_TOKEN, # Add if model requires authentication\n",
    "    trust_remote_code=True # Gemma requires this for some versions/variants\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device(s): {model.hf_device_map}\")\n",
    "\n",
    "# --- IMPORTANT: Finding the Layer Name ---\n",
    "# Uncomment the following line to print the model structure and find the exact layer name\n",
    "# print(model)\n",
    "# Look for layers like 'model.layers[INDEX].mlp...' or 'model.layers[INDEX].self_attn...'\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "696261f0-c6a3-4965-a262-73c67916bd40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unembed_vector(\n",
    "    vector: Union[torch.Tensor, np.ndarray],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_transpose: bool = False,\n",
    "    top_k: int = 10,\n",
    "    token_list: Optional[List[str]] = None,\n",
    "    device: Optional[str] = None,\n",
    "    dtype: Optional[torch.dtype] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Unembed a vector using either the unembedding matrix or the transpose of the embedding matrix.\n",
    "    \n",
    "    Args:\n",
    "        vector: The vector to unembed (1D tensor or numpy array)\n",
    "        model_name: The Gemma model name\n",
    "        use_transpose: If True, use the transpose of the embedding matrix; if False, use the unembedding matrix\n",
    "        top_k: Number of top tokens to return\n",
    "        token_list: List of specific tokens to compute logits for\n",
    "        device: Device to run computation on ('cuda', 'cpu'). If None, will use CUDA if available.\n",
    "        dtype: Data type to use for computation. If None, will match the model's dtype.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - top_tokens: List of (token, score) pairs for top tokens\n",
    "            - specific_logits: Dictionary mapping tokens to their logits (if token_list provided)\n",
    "    \"\"\"\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    \n",
    "    # Determine dtype to use (match the model's dtype if not specified)\n",
    "    if dtype is None:\n",
    "        # Get model's dtype from embedding weights\n",
    "        model_dtype = model.get_input_embeddings().weight.dtype\n",
    "        dtype = model_dtype\n",
    "    \n",
    "    # Ensure vector is a torch tensor with correct dtype and device\n",
    "    if isinstance(vector, np.ndarray):\n",
    "        vector = torch.tensor(vector, dtype=dtype, device=device)\n",
    "    else:\n",
    "        vector = vector.to(device=device, dtype=dtype)\n",
    "    \n",
    "    if vector.dim() > 1:\n",
    "        # Flatten if needed - assuming the input might be a 2D embedding\n",
    "        vector = vector.squeeze()\n",
    "    \n",
    "    # Get the appropriate matrix for unembedding\n",
    "    with torch.no_grad():\n",
    "        if use_transpose:\n",
    "            # Use the transpose of the embedding matrix\n",
    "            embedding_matrix = model.get_input_embeddings().weight\n",
    "            unembedding_matrix = embedding_matrix.transpose(0, 1)\n",
    "        else:\n",
    "            # Use the unembedding matrix (lm_head)\n",
    "            unembedding_matrix = model.lm_head.weight.transpose(0, 1)\n",
    "    \n",
    "    # Ensure the vector has the correct shape to match the unembedding matrix\n",
    "    if vector.shape[0] != unembedding_matrix.shape[0]:\n",
    "        raise ValueError(f\"Vector dimension ({vector.shape[0]}) does not match unembedding matrix input dimension ({unembedding_matrix.shape[0]})\")\n",
    "    \n",
    "    # Compute the unembedded logits (using matrix-vector multiplication)\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        logits = torch.matmul(vector, unembedding_matrix)\n",
    "    \n",
    "    # Get the top-k token IDs based on logits\n",
    "    top_values, top_indices = torch.topk(logits, k=top_k)\n",
    "    \n",
    "    # Convert to tokens and build result list\n",
    "    top_tokens = []\n",
    "    for idx, (token_id, score) in enumerate(zip(top_indices.tolist(), top_values.tolist())):\n",
    "        token = tokenizer.decode(token_id)\n",
    "        top_tokens.append((token, score))\n",
    "    \n",
    "    result = {\n",
    "        \"top_tokens\": top_tokens,\n",
    "    }\n",
    "    \n",
    "    # Calculate logits for specific tokens if provided\n",
    "    if token_list is not None:\n",
    "        specific_logits = {}\n",
    "        specific_ranks = {}\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        token_to_rank = {idx.item(): rank for rank, idx in enumerate(sorted_indices)}\n",
    "\n",
    "        for token in token_list:\n",
    "            token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
    "            if token_ids:\n",
    "                token_id = token_ids[0]\n",
    "                logit = logits[token_id].item()\n",
    "                rank = token_to_rank.get(token_id, None)\n",
    "                specific_logits[token] = logit\n",
    "                specific_ranks[token] = rank\n",
    "            else:\n",
    "                specific_logits[token] = float('nan')\n",
    "                specific_ranks[token] = None\n",
    "\n",
    "        result[\"specific_logits\"] = specific_logits\n",
    "        result[\"specific_ranks\"] = specific_ranks\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "336d93c4-08dd-4abf-bf31-ce3a20b12af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_positive_second(pairs):\n",
    "    if not pairs:\n",
    "        return 0.0\n",
    "\n",
    "    count = sum(1 for x, y in pairs if y > 0)\n",
    "    return (count / len(pairs)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24cdcb69-e13a-4f93-8d26-cd674a83ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_first_thousand(pairs):\n",
    "    if not pairs:\n",
    "        return 0.0\n",
    "\n",
    "    count = sum(1 for x, y in pairs if y < 1000)\n",
    "    return (count / len(pairs)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec7c00f2-011f-4b35-8143-0415214e44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_sound(s):\n",
    "    return s.split('_')[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55c3dee3-e89c-4848-91c8-8ec8644cf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1=str(list(extracted_weights.items())[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ef753d9-4631-49d6-8766-311fba91f1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_sound(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6176c30c-1338-431f-b7a3-18e2a9f49ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sounds={}\n",
    "for i in range(12):\n",
    "    label = str(list(extracted_weights.items())[i][0])\n",
    "    last_sounds[label]=last_sound(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc1c4606-af17-4727-9659-7ee0a7cbbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sounds={'bake_rhymes': 'k',\n",
    " 'band_rhymes': 'd',\n",
    " 'call_rhymes': 'l',\n",
    " 'doom_rhymes': 'm',\n",
    " 'night_rhymes': 't',\n",
    " 'pain_rhymes': 'n',\n",
    " 'shore_rhymes': 'r',\n",
    " 'sing_rhymes': 'g',\n",
    " 'skies_rhymes': 's',\n",
    " 'sleep_rhymes': 'p',\n",
    " 'slick_rhymes': 'k',\n",
    " 'unfold_rhymes': 'd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6bc0b54-8178-461b-bd14-e2a4db681b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bake_rhymes', 'band_rhymes', 'call_rhymes', 'doom_rhymes', 'night_rhymes', 'pain_rhymes', 'shore_rhymes', 'sing_rhymes', 'skies_rhymes', 'sleep_rhymes', 'slick_rhymes', 'unfold_rhymes'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_sounds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0cadc90-f4d8-4e9e-8a8f-687e00398cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhymes={i: [] for i in last_sounds.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a64c9920-5acb-4166-ab33-d65e04eeb754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in splits['train']+splits['validation']:\n",
    "    rhymes[p[1]].append(p[0].split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b796db7f-58c4-4849-a6c8-821eb1f7a081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'k'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a12b700e-29e7-4184-9af4-38160a8653b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bake_rhymes\n",
      "\trandom top logits:\n",
      " [' byteArray', ' cellspacing', ' rač', 'getAccount', ' locura']\n",
      "\tPercentage positive logits for words in rhyme family: 100.0\n",
      "\tPercentage top 1K logits for words in rhyme family: 9.09\n",
      "Count of k in top tokens: 148\n",
      "\n",
      "band_rhymes\n",
      "\trandom top logits:\n",
      " [' sandra', ' DK', 'DT', 'industan', ' Dartmouth']\n",
      "\tPercentage positive logits for words in rhyme family: 76.92\n",
      "\tPercentage top 1K logits for words in rhyme family: 15.38\n",
      "Count of d in top tokens: 702\n",
      "\n",
      "call_rhymes\n",
      "\trandom top logits:\n",
      " ['RTLR', 'Cyfeiriadau', 'ALF', 'ladora', ' UILabel']\n",
      "\tPercentage positive logits for words in rhyme family: 100.0\n",
      "\tPercentage top 1K logits for words in rhyme family: 20.0\n",
      "Count of l in top tokens: 672\n",
      "\n",
      "doom_rhymes\n",
      "\trandom top logits:\n",
      " ['mu', 'uver', 'ulink', ' MUM', ' Muc']\n",
      "\tPercentage positive logits for words in rhyme family: 100.0\n",
      "\tPercentage top 1K logits for words in rhyme family: 0.0\n",
      "Count of m in top tokens: 620\n",
      "\n",
      "night_rhymes\n",
      "\trandom top logits:\n",
      " [' Quintana', 't', 'to', 'Tid', ' tip']\n",
      "\tPercentage positive logits for words in rhyme family: 100.0\n",
      "\tPercentage top 1K logits for words in rhyme family: 20.0\n",
      "Count of t in top tokens: 867\n",
      "\n",
      "pain_rhymes\n",
      "\trandom top logits:\n",
      " [' ſein', 'oncé', 'âu', ' Sae', 'cienza']\n",
      "\tPercentage positive logits for words in rhyme family: 93.33\n",
      "\tPercentage top 1K logits for words in rhyme family: 0.0\n",
      "Count of n in top tokens: 584\n",
      "\n",
      "shore_rhymes\n",
      "\trandom top logits:\n",
      " [' corretto', 'orio', 'ToProps', 'earcher', 'elerik']\n",
      "\tPercentage positive logits for words in rhyme family: 77.78\n",
      "\tPercentage top 1K logits for words in rhyme family: 11.11\n",
      "Count of r in top tokens: 762\n",
      "\n",
      "sing_rhymes\n",
      "\trandom top logits:\n",
      " [' betweenstory', 'yng', ' čierna', 'ienze', 'イング']\n",
      "\tPercentage positive logits for words in rhyme family: 84.21\n",
      "\tPercentage top 1K logits for words in rhyme family: 5.26\n",
      "Count of g in top tokens: 313\n",
      "\n",
      "skies_rhymes\n",
      "\trandom top logits:\n",
      " ['isantes', ' <>\",', 'ocities', ' InputDecoration', 'upné']\n",
      "\tPercentage positive logits for words in rhyme family: 76.92\n",
      "\tPercentage top 1K logits for words in rhyme family: 7.69\n",
      "Count of s in top tokens: 629\n",
      "\n",
      "sleep_rhymes\n",
      "\trandom top logits:\n",
      " ['v', ' leb', 'fpm', ' énergétique', ' Ves']\n",
      "\tPercentage positive logits for words in rhyme family: 50.0\n",
      "\tPercentage top 1K logits for words in rhyme family: 0.0\n",
      "Count of p in top tokens: 308\n",
      "\n",
      "slick_rhymes\n",
      "\trandom top logits:\n",
      " ['InjectAttribute', ' Kash', 'bufio', 'pick', '﹍﹍﹍']\n",
      "\tPercentage positive logits for words in rhyme family: 66.67\n",
      "\tPercentage top 1K logits for words in rhyme family: 5.56\n",
      "Count of k in top tokens: 235\n",
      "\n",
      "unfold_rhymes\n",
      "\trandom top logits:\n",
      " ['𝕝', ' soggior', 'ofe', '্ড', 'dfd']\n",
      "\tPercentage positive logits for words in rhyme family: 40.0\n",
      "\tPercentage top 1K logits for words in rhyme family: 0.0\n",
      "Count of d in top tokens: 435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    label=list(extracted_weights.items())[i][0]\n",
    "    print(label)\n",
    "    rhymelist=list(set(rhymes[label]))\n",
    "    last_s=last_sounds[label]\n",
    "    v=unembed_vector(torch.tensor(list(extracted_weights.items())[i][1]),top_k=1000,token_list=rhymelist)\n",
    "    smp=[i[0] for i in v['top_tokens'][:100]]\n",
    "    random.shuffle(smp)\n",
    "    print(\"\\trandom top logits:\\n\",smp[:5])\n",
    "    print(\"\\tPercentage positive logits for words in rhyme family:\",round(percentage_positive_second(v['specific_logits'].items()),2))\n",
    "    print(\"\\tPercentage top 1K logits for words in rhyme family:\",round(percentage_first_thousand(v['specific_ranks'].items()),2))\n",
    "    print(\"Count of\",last_s,\"in top tokens:\",len([i for i in v['top_tokens'] if last_s in i[0] or last_s.upper() in i[0]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18c6309f-a00a-46d0-993a-08e9ef71deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (SPLIT_DATA_FILE,'r')as f: \n",
    "    splits=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e3bf419-225c-4a44-a3a3-4b4b8f5f52bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_words=[p[0].split()[-1] for p in splits['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b69bf11e-c8e7-44b0-abc1-5d82bbefe2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_words=[p[0].split()[-1] for p in splits['validation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a663ed95-0d89-40d8-b3aa-088c9c4b451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_words).union(set(validation_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c597a7d-d451-4673-abc7-117df6cbe800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
