{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2d9472-720d-4a52-85ca-a1ccb5f67601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m161.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed numpy-2.2.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cbd7bf-10a1-4df6-9dfd-c7b9959ee48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m564.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m392.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m218.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m184.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m928.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, safetensors, regex, python-dotenv, kiwisolver, fonttools, einops, cycler, contourpy, pandas, matplotlib, huggingface-hub, dotenv, tokenizers, transformers, bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.6.0 bitsandbytes-0.45.5 contourpy-1.3.2 cycler-0.12.1 dotenv-0.9.9 einops-0.8.1 fonttools-4.57.0 huggingface-hub-0.30.2 kiwisolver-1.4.8 matplotlib-3.10.1 pandas-2.2.3 python-dotenv-1.1.0 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate bitsandbytes einops dotenv matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7fdbd1-818f-4729-8c95-1aabe3aa546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250319+cu128\n",
      "Transformers version: 4.51.3\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Optional, Callable\n",
    "import einops\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "# %%\n",
    "dotenv.load_dotenv(\"hf.env\")\n",
    "# @title 1.5. For access to Gemma models, log in to HuggingFace \n",
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")\n",
    "# %%\n",
    "MODEL_ID = \"google/gemma-2-9b-it\" # Or \"google/gemma-2-9b\" if you prefer the base model\n",
    "# Set to True if you have limited VRAM (e.g., < 24GB). Requires bitsandbytes\n",
    "USE_4BIT_QUANTIZATION = False\n",
    "\n",
    "POSITIVE_PROMPTS = [\n",
    "    \"This story should be very optimistic and uplifting.\",\n",
    "    \"Write a hopeful and positive narrative.\",\n",
    "    \"Generate text with a cheerful and encouraging tone.\",\n",
    "]\n",
    "NEGATIVE_PROMPTS = [\n",
    "    \"This story should be very pessimistic and bleak.\",\n",
    "    \"Write a depressing and negative narrative.\",\n",
    "    \"Generate text with a gloomy and discouraging tone.\",\n",
    "]\n",
    "\n",
    "# The prompt to use for actual generation\n",
    "GENERATION_PROMPT = \"Write a short paragraph about the future of artificial intelligence.\"\n",
    "\n",
    "# How strongly to apply the steering vector. Tune this value (e.g., 0.5 to 5.0)\n",
    "STEERING_MULTIPLIER = 1.5\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "MAX_NEW_TOKENS = 30\n",
    "TEMPERATURE = 0.7\n",
    "DO_SAMPLE = True\n",
    "\n",
    "lines_that_rhyme_with_quick = [\n",
    "    \"The house was built with sturdy, reddish brick\",\n",
    "    \"The camera captured moments with each click\",\n",
    "    \"She turned the lights on with a simple flick\",\n",
    "    \"The soccer player gave the ball a mighty kick\",\n",
    "    \"The puppy gave my hand a gentle lick\",\n",
    "    \"The razor left a small and painful nick\",\n",
    "    \"From all the fruits available, I'll make my pick\",\n",
    "    \"The rose's thorn can cause a sudden prick\",\n",
    "    \"He stayed at home because he felt too sick\",\n",
    "    \"The rain had made the winding road quite slick\",\n",
    "    \"The child drew pictures with a charcoal stick\",\n",
    "    \"The winter fog was rolling in so thick\",\n",
    "    \"The clock marked every second with a tick\",\n",
    "    \"The magician performed an amazing trick\",\n",
    "    \"The candle slowly burned down to the wick\",\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_pain = [\n",
    "    \"The storm has passed but soon will come again\",\n",
    "    \"The wizard's knowledge was profoundly arcane\",\n",
    "    \"That constant noise became my existence's bane\",\n",
    "    \"The puzzle challenged every corner of my brain\",\n",
    "    \"The elderly man walked slowly with his cane\",\n",
    "    \"The prisoner rattled his heavy iron chain\",\n",
    "    \"The construction site had a towering crane\",\n",
    "    \"The queen would rarely to respond deign\",\n",
    "    \"The rainwater flowed down into the drain\",\n",
    "    \"She looked at the offer with obvious disdain\",\n",
    "    \"The king surveyed his vast and wealthy domain\",\n",
    "    \"The teacher took her time to clearly explain\",\n",
    "    \"He tried to hide his feelings and to feign\",\n",
    "    \"The pilgrims journeyed to the ancient fane\",\n",
    "    \"The athlete trained for months to make a gain\",\n",
    "    \"The farmer harvested the golden grain\",\n",
    "    \"The doctor's treatment was gentle and humane\",\n",
    "    \"His argument was completely inane\",\n",
    "    \"The plan they proposed was utterly insane\",\n",
    "    \"The classic novel starred a heroine named Jane\",\n",
    "    \"The car sped down the narrow country lane\",\n",
    "    \"The issue at hand was certainly the main\",\n",
    "    \"The lion shook his magnificent mane\",\n",
    "    \"The office work felt repetitive and mundane\",\n",
    "    \"The church would soon the new priest ordain\",\n",
    "    \"The sunlight streamed through the window pane\",\n",
    "    \"The message written there was crystal plain\",\n",
    "    \"The travelers boarded the waiting plane\",\n",
    "    \"His language was considered quite profane\",\n",
    "    \"The flowers bloomed after the gentle rain\",\n",
    "    \"The rider pulled firmly on the horse's rein\",\n",
    "    \"The king began his long and peaceful reign\",\n",
    "    \"Despite the chaos, she remained quite sane\",\n",
    "    \"We planned our summer holiday in Spain\",\n",
    "    \"The athlete suffered from a painful ankle sprain\",\n",
    "    \"The red wine left a permanent stain\",\n",
    "    \"The heavy lifting put his back under strain\",\n",
    "    \"Good habits help your health maintain and sustain\",\n",
    "    \"The maiden was courted by a handsome swain\",\n",
    "    \"We hurried to catch the departing train\",\n",
    "    \"The river split the land in twain\",\n",
    "    \"His manner was sophisticated and urbane\",\n",
    "    \"Her efforts to convince him were in vain\",\n",
    "    \"The wind direction showed on the weather vane\",\n",
    "    \"The nurse carefully located a suitable vein\",\n",
    "    \"As night approached, the daylight began to wane\",\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_rabbit = [\n",
    "    \"I saw something move in the garden, so I decided to grab it\", # To my surprise, it turned out to be a fluffy little rabbit.\n",
    "    \"When you hear a noise in the bushes, don't be afraid to nab it\", # Chances are it's just the neighborhood's friendly rabbit.\n",
    "    \"She has a special way with animals, it's quite a habit\", # Her favorite creature to care for is her pet rabbit.\n",
    "    \"I thought I'd plant some carrots, but something came to stab it\", # I looked outside and caught the culprit—a hungry rabbit.\n",
    "    \"The magician pulled something furry out of his hat, to my amazement he had it\", # The audience cheered when they saw it was a snow-white rabbit.\n",
    "    \"If you find a hole in your garden, you should probably tab it\", # It's likely the new underground home of a burrowing rabbit.\n",
    "    \"The child saw something soft in the pet store and wanted to have it\", # She begged her parents until they bought her that adorable rabbit.\n",
    "    \"I heard a rustling sound in the forest and tried to dab it\", # But it hopped away quickly—I just missed that wild rabbit.\n",
    "    \"When something nibbles your lettuce, there's no need to blab it\", # Everyone knows the culprit is probably a garden rabbit.\n",
    "    \"I felt something soft brush against my leg, I reached down to grab it\", # And found myself petting the silky fur of a friendly rabbit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_habit = [\n",
    "    \"When you see a rabbit\", # You might form a feeding habit.\n",
    "    \"He'd grab it if he could just nab it\", # That's become his daily habit.\n",
    "    \"The frog sits on the lily pad, a bit\", # Too long—it's turned into a habit.\n",
    "    \"She wears that jacket like she's glad to have it\", # Dressing sharp has always been her habit.\n",
    "    \"I know I should quit, but I just can't stab it\", # Breaking free from such a stubborn habit.\n",
    "    \"If there's a chance for joy, I'll always grab it\", # Seeking happiness is my best habit.\n",
    "    \"The cat will chase the yarn if you dab it\", # Playing games has been a lifelong habit.\n",
    "    \"When faced with problems, I don't just blab it\", # Thinking before speaking is my habit.\n",
    "    \"He'll take a compliment, but never crab it\", # Staying humble is his finest habit.\n",
    "    \"The chef will taste the dish before they tab it\", # Quality control's a professional habit.\n",
    "    \"When opportunity knocks, I'll cab it\", # Seizing the moment is my favorite habit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_rabbit = [\n",
    "    \"She couldn't seem to break her gardening habit\", # Until her veggies were stolen by a clever rabbit.\n",
    "    \"He developed quite an interesting habit\", # Of leaving carrots for the neighbor's pet rabbit.\n",
    "    \"The monk maintained his meditation habit\", # While outside his window hopped a curious rabbit.\n",
    "    \"I tried to quit my late-night snacking habit\", # When I spotted in my kitchen a midnight rabbit.\n",
    "    \"The farmer stuck to his early rising habit,\" # And caught sight of a dawn-grazing rabbit.\n",
    "    \"My daughter formed an adorable habit\", # Of reading bedtime stories to her stuffed rabbit.\n",
    "    \"The writer maintained her daily writing habit\", # Creating tales about a mischievous rabbit.\n",
    "    \"The painter couldn't shake her artistic habit\", # Her favorite subject was a snow-white rabbit.\n",
    "    \"She picked up the peculiar habit\", # Of leaving garden notes addressed to a rabbit.\n",
    "    \"He kept up his wholesome forest walking habit\", # Often spotting the same cotton-tailed rabbit.\n",
    "    \"The boy acquired a strange collecting habit\", # Of items shaped like his favorite animal: rabbit.\n",
    "    \"The chef developed an experimental cooking habit\", # Inspired by watching a munching wild rabbit.\n",
    "    \"The photographer formed a dawn shooting habit\", # Capturing perfect moments of a dewdrop-covered rabbit.\n",
    "    \"My grandmother maintained her knitting habit\", # Creating tiny sweaters for her daughter's rabbit.\n",
    "    \"The scientist stuck to her observation habit\", # Documenting behaviors of the laboratory rabbit.\n",
    "    \"The child couldn't break his skipping habit\", # Hopping through the garden like an energetic rabbit.\n",
    "    \"The jogger kept her early morning habit\", # Racing along the trail with a wild rabbit.\n",
    "    \"The wizard practiced his disappearing habit\", # Vanishing from sight much like a magic rabbit.\n",
    "    \"She developed a serious chocolate habit\", # After receiving a gift shaped like a rabbit.\n",
    "    \"The detective never lost his questioning habit\", # Following clues that led to a snow-white rabbit.\n",
    "    \"He cultivated a very precise gardening habit\", # To protect his carrots from the neighborhood rabbit.\n",
    "    \"The composer maintained her nighttime composing habit\", # With melodies inspired by a moonlit rabbit.\n",
    "    \"The teacher had a creative teaching habit\", # Using stories about a wise philosophical rabbit.\n",
    "    \"My uncle can't kick his star-gazing habit\", # Often seeing constellations shaped like a rabbit.\n",
    "    \"She formed an unusual sketching habit\", # Drawing landscapes always featuring a distant rabbit.\n",
    "    \"The doctor maintained a healthy eating habit\", # Enjoying salads that would impress a rabbit.\n",
    "    \"The botanist kept her plant-collecting habit\", # Finding species that attracted the rare mountain rabbit.\n",
    "    \"My brother developed a strange talking habit\", # Of narrating his day to an imaginary rabbit.\n",
    "    \"The seamstress maintained her sewing habit\", # Crafting costumes featuring a dancing rabbit.\n",
    "    \"The old man had a generous feeding habit\", # Sharing his garden harvest with each passing rabbit.\n",
    "    \"The barista perfected her latte art habit\", # Creating foam designs resembling a jumping rabbit.\n",
    "    \"The astronomer continued her stargazing habit\", # Discovering a nebula shaped like a cosmic rabbit.\n",
    "    \"The carpenter refined his woodworking habit\", # Carving intricate figures of a forest rabbit.\n",
    "    \"My cousin formed an unusual naming habit\", # Calling every stray animal 'Peter the rabbit'.\n",
    "    \"The librarian kept her book-suggesting habit\", # Often recommending tales about a clever rabbit.\n",
    "    \"The hiker maintained her trail-blazing habit\", # Following paths once traveled by the snowshoe rabbit.\n",
    "    \"The young girl had a flower-collecting habit\", # Making crowns she'd place upon her patient rabbit.\n",
    "    \"The researcher developed a note-taking habit\", # Recording every movement of the study's rabbit.\n",
    "    \"The poet sustained his daily writing habit\", # Composing verses about a philosophical rabbit.\n",
    "    \"My aunt established a dawn gardening habit\", # Working alongside her garden-helping rabbit.\n",
    "    \"The student formed a late-night studying habit\", # Taking breaks to play with her energetic rabbit.\n",
    "    \"The baker kept an experimental baking habit\", # Creating carrot treats for her customer's rabbit.\n",
    "    \"The filmmaker maintained a storytelling habit\", # Often featuring adventures of a heroic rabbit.\n",
    "    \"The musician developed a curious practice habit\", # Playing sonatas that soothed her nervous rabbit.\n",
    "    \"The naturalist continued her tracking habit\", # Documenting the passage of each wild rabbit.\n",
    "    \"My father couldn't break his early waking habit\", # Always finding time to feed the backyard rabbit.\n",
    "    \"The magician perfected his hat-pulling habit\", # Surprising audiences with an appearing rabbit.\n",
    "    \"The engineer maintained her inventing habit\", # Creating gadgets to entertain her bored rabbit.\n",
    "    \"The florist developed an arrangement habit\", # Including carrot tops to please her shop's rabbit.\n",
    "    \"The therapist kept her gentle listening habit\", # Showing patience that matched her office rabbit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_habit = [\n",
    "    \"When I found a small, trembling rabbit\", # Caring for animals became my habit.\n",
    "    \"She darted through the garden like a rabbit\", # Looking for treats had become her habit.\n",
    "    \"He claimed he could pull a hat from a rabbit\", # Showing off magic tricks was his daily habit.\n",
    "    \"The children giggled as they chased the rabbit\", # Running through meadows became their favorite habit.\n",
    "    \"I planted carrots to attract a rabbit\", # Gardening in spring is my cherished habit.\n",
    "    \"My thoughts multiply faster than a rabbit\", # Overthinking has become my worst habit.\n",
    "    \"The speedy win went to the tortoise, not the rabbit\", # Victory comes from persistence, not just habit.\n",
    "    \"In the moonlight hopped a silver rabbit\", # Stargazing at night is now my habit.\n",
    "    \"They built a cozy hutch for their new rabbit\", # Creating homes for pets is a wonderful habit.\n",
    "    \"The chef prepared a savory stew with rabbit\", # Cooking wild game had become his habit.\n",
    "    \"Through tall grass I spotted a cottontail rabbit\", # Hiking through fields is my weekend habit.\n",
    "    \"The magician waved his wand and vanished the rabbit\", # Astonishing crowds had become his habit.\n",
    "    \"I sketched the ears and whiskers of a rabbit\", # Drawing animals is my creative habit.\n",
    "    \"The farmer chased away the vegetable-stealing rabbit\", # Protecting his crops was a necessary habit.\n",
    "    \"At dawn the fox was hunting for a rabbit\", # Early rising became his daily habit.\n",
    "    \"In the story, Peter was a mischievous rabbit\", # Reading fables became our bedtime habit.\n",
    "    \"Her fear made her timid just like a rabbit\", # Avoiding confrontation was her lifelong habit.\n",
    "    \"The child's stuffed toy was a velveteen rabbit\", # Carrying comfort objects was her childhood habit.\n",
    "    \"The dog barked loudly at the wild rabbit\", # Alert guarding is his protective habit.\n",
    "    \"The hunter set a snare to catch a rabbit\", # Living off the land was his family habit.\n",
    "    \"The camera captured a leaping snow-white rabbit\", # Photography in winter is my seasonal habit.\n",
    "    \"A clever fox can easily outfox a rabbit\", # Strategic thinking is my professional habit.\n",
    "    \"The full moon illuminated the jackrabbit\", # Evening walks became our romantic habit.\n",
    "    \"Under the bush was hiding a frightened rabbit\", # Finding secret spaces was her peculiar habit.\n",
    "    \"Into his hat disappeared the magical rabbit\", # Performing illusions was his lucrative habit.\n",
    "    \"My daughter begged for a pet dwarf rabbit\", # Collecting small animals became her expensive habit.\n",
    "    \"The naturalist observed the rare desert rabbit\", # Scientific inquiry was her passionate habit.\n",
    "    \"Tales of Brer Fox always included a rabbit\", # Telling folk stories was grandfather's evening habit.\n",
    "    \"She embroidered the silhouette of a rabbit\", # Creating handcrafted gifts was her generous habit.\n",
    "    \"Through the forest hopped a nimble rabbit\", # Morning exercises became his energizing habit.\n",
    "    \"We watched with awe the jumping jackrabbit\", # Desert exploration became our vacation habit.\n",
    "    \"The painting depicted a wild mountain rabbit\", # Collecting wildlife art was his expensive habit.\n",
    "    \"In the field I photographed a rare pygmy rabbit\", # Documenting endangered species is my conservation habit.\n",
    "    \"The child's first pet was a Dutch lop rabbit\", # Learning responsibility became her formative habit.\n",
    "    \"On Easter morning appeared a chocolate rabbit\", # Holiday traditions became our family habit.\n",
    "    \"The scientist studied the behavior of the arctic rabbit\", # Meticulous observation was her scientific habit.\n",
    "    \"The birthday gift was an Angora rabbit\", # Surprising loved ones is my thoughtful habit.\n",
    "    \"Never try to outrun a frightened rabbit\", # Setting realistic goals is my productive habit.\n",
    "    \"Into the brush disappeared the elusive rabbit\", # Playing hide-and-seek was their childhood habit.\n",
    "    \"The young boy dreamed of owning a rabbit\", # Wishful thinking became his daydreaming habit.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621eda32-8683-46fe-a89e-df9748f51d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_that_rhyme_with_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2fd51a-a099-40b8-aac8-f6d2b847bda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_that_rhyme_with_pain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b4d9e79d-2df4-4789-aa1f-e643807a3f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-2-9b-it\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436717b1a00547e6bbdea5dc4ebfcc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ## 3. Load Model and Tokenizer\n",
    "\n",
    "# +\n",
    "# Configure quantization if needed\n",
    "quantization_config = None\n",
    "if USE_4BIT_QUANTIZATION:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Recommended for new models\n",
    "    )\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "\n",
    "# Determine device and dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32 # BF16 recommended on Ampere+\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not present\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically distribute across GPUs if available\n",
    "    # use_auth_token=YOUR_HF_TOKEN, # Add if model requires authentication\n",
    "    trust_remote_code=True # Gemma requires this for some versions/variants\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device(s): {model.hf_device_map}\")\n",
    "\n",
    "# --- IMPORTANT: Finding the Layer Name ---\n",
    "# Uncomment the following line to print the model structure and find the exact layer name\n",
    "# print(model)\n",
    "# Look for layers like 'model.layers[INDEX].mlp...' or 'model.layers[INDEX].self_attn...'\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "# %%\n",
    "# ## 4. Hooking and Activation Handling Functions\n",
    "\n",
    "# +\n",
    "# Global storage for captured activations\n",
    "activation_storage = {}\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Helper function to get a module object from its name string.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "\n",
    "def capture_activation_hook(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "def capture_activation_hook_fast(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "\n",
    "def get_activations_fast(model, tokenizer, prompts: List[str], layer_name: str) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model and captures activations from the target layer.\n",
    "    Returns the averaged activation across all prompts for the last token position.\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {} # Clear previous activations\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        lambda module, input, output: capture_activation_hook_fast(module, input, output, layer_name)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        # We only need the forward pass, not generation here\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        if layer_name in activation_storage:\n",
    "                # Assuming batch size is 1 when processing one prompt at a time\n",
    "            last_token_activations = activation_storage[layer_name] # Shape (num_prompts, hidden_dim)\n",
    "            del activation_storage[layer_name] # Clear for next prompt\n",
    "        else:\n",
    "            print(f\"Warning: Activation for layer {layer_name} not captured for prompts: '{prompts}'\")\n",
    "                \n",
    "    hook_handle.remove() # Clean up the hook\n",
    "\n",
    "    # Stack and average activations across all prompts\n",
    "    # Resulting shape: (num_prompts, hidden_dim) -> (hidden_dim)\n",
    "    avg_activation = last_token_activations.mean(dim=0).squeeze() # Average over the prompt dimension\n",
    "    print(f\"Calculated average activation for layer '{layer_name}' with shape: {avg_activation.shape}\")\n",
    "    return avg_activation\n",
    "# %%\n",
    " # --- Steering Hook during Generation ---\n",
    "\n",
    "# Global variable to hold the steering vector during generation\n",
    "steering_vector_internal = None\n",
    "steering_multiplier_internal = 1.0\n",
    "ALL_POSITIONS=True\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            modified_output = output + (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return modified_output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "             # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0] + (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "def steering_hook(module, input, output, all_positions=ALL_POSITIONS):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            if output.shape[1] != 1 or all_positions:\n",
    "                output[:, -1, :] += (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "            # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0]\n",
    "            # print(modified_tensor.shape)\n",
    "            if modified_tensor.shape[1] != 1 or all_positions:\n",
    "                modified_tensor[:, -1, :] += (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "    \n",
    "@contextmanager\n",
    "def apply_steering(model, layer, steering_vector, multiplier):\n",
    "    \"\"\"Context manager to temporarily apply the steering hook.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    layer_name = f\"model.layers.{layer}\"\n",
    "\n",
    "    # Ensure previous hook (if any) on the same layer is removed\n",
    "    # This basic implementation assumes only one steering hook at a time on this layer\n",
    "    # More robust solutions might track handles explicitly.\n",
    "    \n",
    "    handle = None\n",
    "    try:\n",
    "        steering_vector_internal = steering_vector\n",
    "        steering_multiplier_internal = multiplier\n",
    "        target_module = get_module_by_name(model, layer_name)\n",
    "        handle = target_module.register_forward_hook(steering_hook)\n",
    "        print(f\"Steering hook applied to {layer_name} with multiplier {multiplier}\")\n",
    "        yield # Generation happens here\n",
    "    finally:\n",
    "        if handle:\n",
    "            handle.remove()\n",
    "        steering_vector_internal = None # Clear global state\n",
    "        steering_multiplier_internal = 1.0\n",
    "        print(f\"Steering hook removed from {layer_name}\")\n",
    "        gc.collect() # Suggest garbage collection\n",
    "        torch.cuda.empty_cache() # Clear cache if using GPU\n",
    "\n",
    "def generate_steered_output(steering_vector, model, tokenizer, generation_prompts, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    # Ensure generation_prompts is a list\n",
    "    if isinstance(generation_prompts, str):\n",
    "        generation_prompts = [generation_prompts] \n",
    "    generation_prompts = generation_prompts * batch_size\n",
    "    \n",
    "    # Process in batches of 1000\n",
    "    MAX_BATCH_SIZE = 1000\n",
    "    all_texts = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, len(generation_prompts), MAX_BATCH_SIZE),desc=\"Processing batch\"):\n",
    "        #batch_prompts = generation_prompts[i:i + MAX_BATCH_SIZE]\n",
    "        # Get batch of prompts, handling case where remaining prompts < MAX_BATCH_SIZE\n",
    "        current_batch_size = min(MAX_BATCH_SIZE, len(generation_prompts) - i)\n",
    "        batch_prompts = generation_prompts[i:i + current_batch_size]\n",
    "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        \n",
    "        if steering_vector is None:\n",
    "            print(inputs.input_ids.shape)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    pad_token_id=tokenizer.eos_token_id # Important for generation\n",
    "                )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Apply the steering hook using the context manager\n",
    "                with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                    outputs = model.generate(\n",
    "                        **inputs, # Use the same input tokens\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        temperature=TEMPERATURE,\n",
    "                        do_sample=DO_SAMPLE,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "        \n",
    "        batch_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_texts.extend(batch_texts)\n",
    "\n",
    "        del outputs, inputs\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "def generate_outputs(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    assert steering_vector is not None\n",
    "    text_baseline = generate_steered_output(None, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_steered = generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_negsteered = generate_steered_output(-steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    return text_baseline, text_steered, text_negsteered\n",
    "\n",
    "# %%\n",
    "# ## Compute the Steering Vector\n",
    "def get_steering_vector_fast(model, tokenizer, positive_prompts, negative_prompts, layer=20):\n",
    "    target_layer_name = f\"model.layers.{layer}\"\n",
    "    print(\"Calculating activations for POSITIVE prompts...\")\n",
    "    avg_pos_activation = get_activations_fast(model, tokenizer, positive_prompts, target_layer_name)\n",
    "\n",
    "    print(\"\\nCalculating activations for NEGATIVE prompts...\")\n",
    "    avg_neg_activation = get_activations_fast(model, tokenizer, negative_prompts, target_layer_name)\n",
    "\n",
    "    steering_vector = None\n",
    "    if avg_pos_activation is not None and avg_neg_activation is not None:\n",
    "        steering_vector = avg_pos_activation - avg_neg_activation\n",
    "        print(f\"\\nSteering vector computed successfully. Shape: {steering_vector.shape}\")\n",
    "        # Optional: Normalize the steering vector (can sometimes help)\n",
    "        # steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "        # print(\"Steering vector normalized.\")\n",
    "    else:\n",
    "        print(\"\\nError: Could not compute steering vector due to missing activations.\")\n",
    "    del avg_pos_activation\n",
    "    del avg_neg_activation\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return steering_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca255d0c-59a1-4a2a-8820-a55077421799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ## Functions to analyze the generations\n",
    "def get_last_word(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    if len(lines) < 3:\n",
    "        print(f\"Failed to get last word: {text}\")\n",
    "        return \"\"\n",
    "    second_line = lines[2]\n",
    "    second_line_words = second_line.split(\" \")\n",
    "    if len(second_line_words) == 0:\n",
    "        print(f\"Failed to get last word: {text}\")\n",
    "        return \"\"\n",
    "    last_word = second_line_words[-1]\n",
    "    if last_word == \"\":\n",
    "        if len(second_line_words) == 1:\n",
    "            print(f\"Failed to get last word: {text}\")\n",
    "            return \"\"\n",
    "        last_word = second_line_words[-2]\n",
    "    return last_word\n",
    "\n",
    "def get_second_line(text,include_prompt=True):\n",
    "    lines = text.split(\"\\n\")\n",
    "    if len(lines) < 3:\n",
    "        print(f\"Failed to get second line: {text}\")\n",
    "        return text\n",
    "    second_line = lines[2]\n",
    "    second_line=second_line.strip(' ')\n",
    "    second_line_words = second_line.split(\" \")\n",
    "    if len(second_line_words) > 0:\n",
    "        second_line=' '.join(second_line_words[:-1])\n",
    "    if include_prompt: second_line='\\n'.join(lines[:2]+[second_line])\n",
    "    return second_line\n",
    "\n",
    "def get_last_word_fraction(texts, words):\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "    last_words = [get_last_word(line) for line in texts]\n",
    "    return len([w for w in last_words if any(w2.lower() in w.lower() for w2 in words)]) / len(last_words)\n",
    "\n",
    "def get_prompts(lines):\n",
    "    return [f'A rhymed couplet:\\n{line}\\n' for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e970d8-0567-419c-94bc-c58fc2834f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1094, 0.6250, 1.0312,  ..., 0.2266, 0.1875, 0.4473],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "steering_vector = torch.load('steering_vector_from_quick_to_pain.pt')\n",
    "print(steering_vector )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "878e03b2-e499-4783-989e-ae5ebb333a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    assert steering_vector is not None\n",
    "    text_baseline = generate_steered_output(None, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_steered = generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    #text_negsteered = generate_steered_output(-steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    return text_baseline, text_steered#, text_negsteered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20ebe3e9-dc2a-48dc-94e5-8313ace2cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_PROMPT=f'A rhymed couplet:\\n{lines_that_rhyme_with_pain[1]}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49f01ff8-4e09-4a5f-8deb-5147475cb490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17])\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n"
     ]
    }
   ],
   "source": [
    "text_baseline, text_steered = generate_output(steering_vector, model, tokenizer, GENERATION_PROMPT, 1, layer=20, steering_multiplier=STEERING_MULTIPLIER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e64d9b4-3815-4383-b70b-38b6e3b9da83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rhymed couplet:\n",
      "The wizard's knowledge was profoundly arcane\n",
      "His spells and incantations, a wondrous domain.\n",
      "\n",
      "What are the characteristics of a rhymed couplet?\n",
      "\n",
      "Here are the characteristics of a\n",
      "A rhymed couplet:\n",
      "The wizard's knowledge was profoundly arcane\n",
      "He brewed strange potions with a wicked, glint-filled smirk\n",
      "\n",
      "Let me know if you'd like me to write another one!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_baseline[0])\n",
    "print(text_steered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d5f8638-00f9-4486-b554-80422f565cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_PROMPT=f'A rhymed couplet:\\n{lines_that_rhyme_with_quick[0]}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e9ac59c-5c5f-4ed2-be15-0f79d0c579ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18])\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n"
     ]
    }
   ],
   "source": [
    "text_baseline, text_steered = generate_output(-steering_vector, model, tokenizer, GENERATION_PROMPT, 1, layer=20, steering_multiplier=STEERING_MULTIPLIER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e144e5-fecb-42a8-93c1-da0bc351cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rhymed couplet:\n",
      "The house was built with sturdy, reddish brick\n",
      "And weathered well against the elements quick. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A rhymed couplet:\n",
      "The house was built with sturdy, reddish brick\n",
      "And echoed with the whisper of the rain.\n",
      "\n",
      "\n",
      "Let me know if you'd like another couplet! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_baseline[0])\n",
    "print(text_steered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c58883d-589b-4de1-9152-31dd6bda1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b6a4fd4-48e9-4331-b056-1e900f4b77ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 15/15 [00:00<00:00, 166001.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n",
      "torch.Size([500, 21])\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n"
     ]
    }
   ],
   "source": [
    "GENERATION_PROMPTS = [f'A rhymed couplet:\\n{line}\\n' for line in tqdm.tqdm(lines_that_rhyme_with_quick, desc=\"Generating prompts\")]\n",
    "text_baselines, text_steered = generate_output(-steering_vector, model, tokenizer, GENERATION_PROMPTS, 100, layer=20, steering_multiplier=STEERING_MULTIPLIER)\n",
    "texts_baseline_quick=(text_baselines)\n",
    "texts_steered_from_quick=(text_steered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d192294-c0d8-44ca-b688-b927405bab4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_baseline_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aeddba7-5125-45ef-bffc-4b6c7743903b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 15/15 [00:00<00:00, 140434.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 18])\n",
      "torch.Size([500, 18])\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n"
     ]
    }
   ],
   "source": [
    "texts_baseline_pain=[]\n",
    "texts_steered_from_pain=[]\n",
    "lines_that_rhyme_with_pain_sample=lines_that_rhyme_with_pain\n",
    "random.shuffle(lines_that_rhyme_with_pain_sample)\n",
    "lines_that_rhyme_with_pain_sample=lines_that_rhyme_with_pain_sample[:15]\n",
    "GENERATION_PROMPTS = [f'A rhymed couplet:\\n{line}\\n' for line in tqdm.tqdm(lines_that_rhyme_with_pain_sample, desc=\"Generating prompts\")]\n",
    "text_baselines, text_steered = generate_output(-steering_vector, model, tokenizer, GENERATION_PROMPTS, 100, layer=20, steering_multiplier=STEERING_MULTIPLIER)\n",
    "texts_baseline_pain=(text_baselines)\n",
    "texts_steered_from_pain=(text_steered)\n",
    "#for line in tqdm.tqdm(lines_that_rhyme_with_pain_sample):\n",
    "#    GENERATION_PROMPT=f'A rhymed couplet:\\n{line}\\n'\n",
    "#    text_baseline, text_steered = generate_output(steering_vector, model, tokenizer, GENERATION_PROMPT, 100, layer=20, steering_multiplier=STEERING_MULTIPLIER)\n",
    "#    texts_baseline_pain.append(text_baseline)\n",
    "#    texts_steered_from_pain.append(text_steered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68c3b7ee-779d-49b9-8df9-3b8c875a7907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A rhymed couplet:\\nThe heavy lifting put his back under strain\\nHe groaned in pain, a weary, aching refrain.\\n\\n\\nLet me know if you'd like to see more!  I can write more rhyming\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(texts_baseline_pain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a179197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 15/15 [00:00<00:00, 153450.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 19])\n",
      "torch.Size([500, 19])\n"
     ]
    }
   ],
   "source": [
    "lines_that_rhyme_with_pain_sample=lines_that_rhyme_with_pain\n",
    "random.shuffle(lines_that_rhyme_with_pain_sample)\n",
    "lines_that_rhyme_with_pain_sample=lines_that_rhyme_with_pain_sample[:15]\n",
    "GENERATION_PROMPTS = [f'A rhymed couplet:\\n{line}\\n' for line in tqdm.tqdm(lines_that_rhyme_with_pain_sample, desc=\"Generating prompts\")]\n",
    "texts_baselines_pain = generate_steered_output(None, model, tokenizer, GENERATION_PROMPTS, 100, layer=20, steering_multiplier=STEERING_MULTIPLIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a416be69-f0f9-43eb-b1e6-8d3bbdee281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A rhymed couplet:\\nThe queen would rarely to respond deign\\nTo questions asked in such a crude domain.\\n\\nHere\\'s a breakdown:\\n\\n* **Rhyme:** \"deign\" and \"domain\"'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(texts_baselines_pain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f76f0ca-a309-4026-9ea5-3b3166d512c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28b2a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"line_catalog.json\",'r') as f:\n",
    "    line_catalog=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27ba5300-9d77-49bf-9f85-542ef66e2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog={\"slick_rhymes\":line_catalog[\"slick_rhymes\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5da2bce3-0c28-47d8-8109-d1158a005881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slick_rhymes\n",
      "Whispers in the dark reveal a clever trick\n",
      "night_rhymes\n",
      "The moon casts gentle shadows on this quiet night\n",
      "doom_rhymes\n",
      "Flowers stretch toward the sun, ready to bloom\n",
      "sleep_rhymes\n",
      "Autumn leaves gather in a colorful heap\n",
      "band_rhymes\n",
      "Heartbeats sync to the rhythm of the band\n",
      "sing_rhymes\n",
      "Whispers of freedom found in a bird's wing\n",
      "unfold_rhymes\n",
      "Let dreams like petals delicately unfold\n",
      "shore_rhymes\n",
      "The autumn leaves dance gracefully to the floor\n",
      "pain_rhymes\n",
      "I dance beneath the gentle kiss of rain\n",
      "skies_rhymes\n",
      "Time slips away like grains of sand through flies\n",
      "bake_rhymes\n",
      "Whispers of hope in the shadows we make\n",
      "call_rhymes\n",
      "Leaves dance gracefully before they finally fall\n"
     ]
    }
   ],
   "source": [
    "for family in line_catalog:\n",
    "    print(family)\n",
    "    print(line_catalog[family][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c300b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rhyme families:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:45, 11.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:33, 11.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:34<01:21, 11.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:46<01:09, 11.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:46, 11.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:21<00:35, 11.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:33<00:23, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:45<00:11, 11.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.72s/it]\u001b[A\n",
      "Processing rhyme families:   8%|▊         | 1/12 [01:57<21:28, 117.17s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:46, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:10, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.78s/it]\u001b[A\n",
      "Processing rhyme families:  17%|█▋        | 2/12 [03:55<19:35, 117.56s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:46, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:10, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.79s/it]\u001b[A\n",
      "Processing rhyme families:  25%|██▌       | 3/12 [05:52<17:39, 117.71s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:45, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:33, 11.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:21, 11.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:46<01:10, 11.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:46, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:21<00:34, 11.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:33<00:23, 11.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:44<00:11, 11.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:56<00:00, 11.66s/it]\u001b[A\n",
      "Processing rhyme families:  33%|███▎      | 4/12 [07:49<15:38, 117.28s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:45, 11.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:10, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.78s/it]\u001b[A\n",
      "Processing rhyme families:  42%|████▏     | 5/12 [09:47<13:42, 117.46s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:45, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:33, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:46<01:10, 11.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:45<00:11, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.76s/it]\u001b[A\n",
      "Processing rhyme families:  50%|█████     | 6/12 [11:44<11:45, 117.51s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:46, 11.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:23, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:11, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:59<00:59, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:11<00:47, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:23<00:35, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:58<00:00, 11.87s/it]\u001b[A\n",
      "Processing rhyme families:  58%|█████▊    | 7/12 [13:43<09:49, 117.89s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:46, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:10, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.79s/it]\u001b[A\n",
      "Processing rhyme families:  67%|██████▋   | 8/12 [15:41<07:51, 117.89s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:46, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:10, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:59<00:59, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:58<00:00, 11.80s/it]\u001b[A\n",
      "Processing rhyme families:  75%|███████▌  | 9/12 [17:39<05:53, 117.93s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:46, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:34, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:22, 11.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:47<01:10, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:47, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:22<00:35, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:34<00:23, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:46<00:11, 11.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:57<00:00, 11.79s/it]\u001b[A\n",
      "Processing rhyme families:  83%|████████▎ | 10/12 [19:37<03:55, 117.93s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:45, 11.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:33, 11.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:35<01:21, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:46<01:10, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:10<00:46, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:21<00:35, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n",
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:45<00:11, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:56<00:00, 11.68s/it]\u001b[A\n",
      "Processing rhyme families:  92%|█████████▏| 11/12 [21:34<01:57, 117.59s/it]\n",
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  10%|█         | 1/10 [00:11<01:45, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  20%|██        | 2/10 [00:23<01:33, 11.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  30%|███       | 3/10 [00:34<01:21, 11.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  40%|████      | 4/10 [00:46<01:09, 11.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  50%|█████     | 5/10 [00:58<00:58, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  60%|██████    | 6/10 [01:09<00:46, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  70%|███████   | 7/10 [01:21<00:35, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  80%|████████  | 8/10 [01:33<00:23, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch:  90%|█████████ | 9/10 [01:45<00:11, 11.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch: 100%|██████████| 10/10 [01:56<00:00, 11.68s/it]\u001b[A\n",
      "Processing rhyme families: 100%|██████████| 12/12 [23:31<00:00, 117.59s/it]\n"
     ]
    }
   ],
   "source": [
    "completions={}\n",
    "for family in tqdm.tqdm(line_catalog, desc=\"Processing rhyme families\"):\n",
    "    GENERATION_PROMPTS = [f'A rhymed couplet:\\n{line}\\n' for line in line_catalog[family]]\n",
    "    texts_baselines_in_family = generate_steered_output(None, model, tokenizer, GENERATION_PROMPTS, 100, layer=20, steering_multiplier=STEERING_MULTIPLIER)\n",
    "    completions[family]=texts_baselines_in_family\n",
    "completions_json=json.dumps(competions,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2f7c6f2-b5c7-4371-9952-bb71d0127327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfc943bf-8e32-49f4-ab0f-dead18b51b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(compl.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a40a4cc-ea2e-461b-a618-cb106798ce5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(completions[\"slick_rhymes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34fb7767-c96e-48d3-a252-2122dd56e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_word_distribution(texts_list):\n",
    "    last_word_distribution={}\n",
    "    last_words=defaultdict(list)\n",
    "    for text in texts_list:\n",
    "        text = text.replace(\"\\\\n\", \"\\n\") \n",
    "        firstline=text.split('\\n')[1]\n",
    "        lastword=get_last_word(text)\n",
    "        lastword=lastword.strip(',')\n",
    "        lastword=lastword.strip('.')\n",
    "        lastword=lastword.strip('!')\n",
    "        last_words[firstline].append(lastword)\n",
    "    for line in last_words:\n",
    "        last_word_distribution[line]=Counter(last_words[line])\n",
    "    return last_word_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "15c93436-7a46-4eb9-b894-38665bab27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get last word: A rhymed couplet:\n",
      "Dreams can bend but should never break or stick\n",
      "\n",
      "To the real world, where logic is quick. \n",
      "\n",
      "Let me know if you'd like to see more!\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Fortune's wheel turns with an unpredictable swing\n",
      "\n",
      "Leaving joy and sorrow in its constant ring. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to create more couplets!\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Beneath autumn leaves, stories of hearts unfold, memories enrolled\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Beneath autumn leaves, stories of hearts unfold, memories enrolled\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Beneath autumn leaves, stories of hearts unfold, memories enrolled\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Beneath autumn leaves, stories of hearts unfold, memories enrolled\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Beneath autumn leaves, stories of hearts unfold, memories enrolled\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Beneath autumn leaves, stories of hearts unfold, memories enrolled\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Raindrops patter on the windowpane, echoing down the drain\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Time slips away when empires fall\n",
      "\n",
      "And history echoes with their thrall. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! 😊\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compl={}\n",
    "for rtype in completions:\n",
    "    compl[rtype]=get_last_word_distribution(completions[rtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "70bfe5f6-f2ea-4a77-b4cd-3c02cf3b6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_elements(counter_dict, mincount=30):\n",
    "    suggestive_lines=defaultdict(list)\n",
    "    suggested_words=Counter()\n",
    "    for key, counter in counter_dict.items():\n",
    "        if counter:\n",
    "            top_element, freq = counter.most_common(1)[0]\n",
    "            if freq>=mincount: \n",
    "                #print(f\"{key}: \\n{top_element} ({freq})\")\n",
    "                suggestive_lines[top_element].append(key)\n",
    "                suggested_words[top_element]+=1\n",
    "        #else:\n",
    "            #print(f\"{key}: Counter is empty\")\n",
    "    return suggestive_lines, suggested_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f5fc982f-8f5d-482c-9d07-fd7f20d87df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_elements_above(counter, threshold):\n",
    "    count = sum(1 for v in counter.values() if v >= threshold)\n",
    "    return count >= 2\n",
    "\n",
    "def two_elements_above(counter, threshold):\n",
    "    return [k for k,v in counter.items() if v >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7924391a-34d2-40a4-a864-58bbd0769bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhyme family night_rhymes has 5 words with at least 4 suggestive lines that elicit them at least 51% of the time\n",
      "Counter({'light': 25, 'night': 16, 'sight': 4, 'bright': 4, 'might': 1})\n",
      "Rhyme family sleep_rhymes has 3 words with at least 4 suggestive lines that elicit them at least 51% of the time\n",
      "Counter({'keep': 29, 'deep': 5, 'sleep': 2})\n",
      "Rhyme family call_rhymes has 4 words with at least 4 suggestive lines that elicit them at least 51% of the time\n",
      "Counter({'all': 8, 'call': 4, 'small': 2, 'tall': 1})\n"
     ]
    }
   ],
   "source": [
    "sugglines={}\n",
    "suggwords={}\n",
    "t=51\n",
    "minlines=4\n",
    "suggested_rhymes={}\n",
    "for rtype in compl:\n",
    "    suggested_rhymes[rtype]={}\n",
    "    sugglines[rtype],suggwords[rtype]=top_elements(compl[rtype],mincount=t)\n",
    "    k=len(list(suggwords[rtype].keys()))\n",
    "    candidate=two_elements_above(suggwords[rtype],minlines)\n",
    "    if k>=2 and has_two_elements_above(suggwords[rtype],minlines):\n",
    "        print(f\"Rhyme family {rtype} has {k} words with at least {minlines} suggestive lines that elicit them at least {t}% of the time\")\n",
    "        print(suggwords[rtype])\n",
    "        for word in candidate:\n",
    "            suggested_rhymes[rtype][word]=sugglines[rtype][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7e98aeee-09e9-45c9-a8ee-903e81f2b2ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slick_rhymes': {},\n",
       " 'night_rhymes': {'sight': ['Stars twinkle like diamonds in the velvet night',\n",
       "   'The sunset paints the sky with colors so bright',\n",
       "   \"Butterflies dance in the garden's gentle flight\",\n",
       "   'The stars in the night sky shine incredibly tight'],\n",
       "  'light': ['Whispers of dreams dance through the still night',\n",
       "   'Shadows stretch long across the mysterious night',\n",
       "   'Crickets sing their lullabies throughout the summer night',\n",
       "   'Ghosts of memories haunt the lonely night',\n",
       "   'Mountains stand as ancient guardians of majestic height',\n",
       "   'Her confidence shines like a beacon at full height',\n",
       "   'Courage carries us through fear to newfound height',\n",
       "   'Whispers of love echo in the still of night',\n",
       "   'Whispers of courage echo through the night might',\n",
       "   'Through valleys deep, mountains rise with ancient might',\n",
       "   \"Within each seed lies nature's dormant might\",\n",
       "   'The eagle soared, a symbol of majestic flight',\n",
       "   'Memories flutter like birds in sudden flight',\n",
       "   'Hearts race as love embarks on its first flight',\n",
       "   'Autumn leaves spiral downward in their final flight',\n",
       "   'Hold my hand through darkness into right',\n",
       "   'Whispers of truth echo from mountain height',\n",
       "   'Courage stands tall facing fear with might',\n",
       "   'The dawn breaks with promise, igniting will to fight',\n",
       "   'Stars align when hearts unite in righteous fight',\n",
       "   'Stars twinkle in the vast night sky, ever bright',\n",
       "   'Morning sun paints the meadow golden bright',\n",
       "   'Memories bound in a chest locked up tight',\n",
       "   'Through stormy seas, we held on fierce and tight',\n",
       "   'In silent moments, time stands perfectly still and tight'],\n",
       "  'bright': ['Lovers walk hand in hand beneath the starry night',\n",
       "   \"Souls connect across time with love's eternal might\",\n",
       "   'Memories of fallen heroes inspire us to fight',\n",
       "   'Souls reach upward, forever seeking the divine light'],\n",
       "  'night': ['Autumn leaves dance gracefully out of sight',\n",
       "   'Stars twinkle like diamonds in the vast cosmic sight',\n",
       "   'Dreams take flight beyond the realm of sight',\n",
       "   'As darkness falls, stars shine with celestial might',\n",
       "   'Dreams unfold where imagination takes flight might',\n",
       "   'Hope rises like a phoenix in triumphant flight',\n",
       "   'In shadows deep, your presence feels so right',\n",
       "   'Fireflies dance through summer meadows, tiny dots of light',\n",
       "   'Through darkest storms we find our way toward light',\n",
       "   'In your eyes I see reflections of eternal light',\n",
       "   'In the dark, her smile shines so bright',\n",
       "   \"Through shadows, hope's flame burns steady and bright\",\n",
       "   'After rain, rainbow colors appear vividly bright',\n",
       "   'His eyes, reflecting love, sparkled incredibly bright',\n",
       "   'Autumn leaves dance in colors fiery bright',\n",
       "   'Even in despair, keep your spirit bright']},\n",
       " 'doom_rhymes': {},\n",
       " 'sleep_rhymes': {'keep': [\"Time collects moments in life's messy heap\",\n",
       "   'Stars in the night sky form a celestial heap',\n",
       "   'Stars shine brightest in skies infinitely deep',\n",
       "   'Her eyes held stories in waters running deep',\n",
       "   'Roots of ancient trees grow stubbornly deep',\n",
       "   'Wounds of the heart cut painfully deep',\n",
       "   \"Mountain valleys carved by time's patient hand run deep\",\n",
       "   'Silence speaks volumes when emotions run deep',\n",
       "   'The moon whispers secrets as I drift to sleep',\n",
       "   'Waves of dreams crash upon the shores of sleep',\n",
       "   'Cradled in silence, my thoughts surrender to sleep',\n",
       "   'Stars paint lullabies across the canvas of sleep',\n",
       "   'Shadows dance quietly while I welcome sleep',\n",
       "   'Time slows its march in the realm of sleep',\n",
       "   'Worries dissolve in the warm embrace of sleep',\n",
       "   'Gentle darkness wraps around me promising sleep',\n",
       "   'Memories fade as consciousness yields to sleep',\n",
       "   'Dreams unfurled on wings ready to leap',\n",
       "   'Love transcends boundaries with one passionate leap',\n",
       "   'From cocoons of comfort, butterflies gracefully leap',\n",
       "   'Ink on parchment where thoughts gently seep',\n",
       "   'Memories flood back, as the skies begin to weep',\n",
       "   'Mountains stand tall while valleys quietly weep',\n",
       "   'Ancient trees with stories that make the wind weep',\n",
       "   'In silence, the strongest hearts sometimes softly weep',\n",
       "   'Shadows dance as mountains rise, majestically steep',\n",
       "   'Time slips away like sand on dunes steep',\n",
       "   'Through the misty forest, the lone wolf does creep',\n",
       "   'Time, an unstoppable force, continues to creep'],\n",
       "  'deep': [\"Sow kindness today, tomorrow you'll reap\",\n",
       "   'Through stormy seas, our promises we keep',\n",
       "   'Footprints in sand that tides refuse to keep',\n",
       "   'Broken promises scatter like tears when lovers weep',\n",
       "   'Moonlight through curtain gaps does softly creep']},\n",
       " 'band_rhymes': {},\n",
       " 'sing_rhymes': {},\n",
       " 'unfold_rhymes': {},\n",
       " 'shore_rhymes': {},\n",
       " 'pain_rhymes': {},\n",
       " 'skies_rhymes': {},\n",
       " 'bake_rhymes': {},\n",
       " 'call_rhymes': {'all': [\"Memories echo through time's haunting call\",\n",
       "   \"Hearts beat in unison to love's call\",\n",
       "   'Christmas lights twinkle throughout the winter mall',\n",
       "   'I caught the memory as it fell like a ball',\n",
       "   'She danced gracefully across the crowded dance hall ball',\n",
       "   'Morning dew glistens on flowers at the garden stall',\n",
       "   'A whisper in the wind, so gentle and small',\n",
       "   'Her laughter resonates throughout the grand hall'],\n",
       "  'call': ['Sunflowers bend toward light, determined to grow tall',\n",
       "   'Through stormy nights, the lighthouse remains steadfast and tall',\n",
       "   'Whispered legends speak of heroes brave and tall',\n",
       "   \"Hearts beat in unison behind the chest's warm wall\"]}}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6eac1b97-fa4a-480b-a5dd-8bdb632d07f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slick_rhymes': defaultdict(list,\n",
       "             {'quick': ['Whispers in the dark reveal a clever trick',\n",
       "               'I watch the clock hands move with every tick',\n",
       "               \"Memories fade with each small moment's tick\",\n",
       "               'Waiting for you with each anxious tick',\n",
       "               'Fresh paint on the wall, wet and slick',\n",
       "               \"Dreams vanish at dawn with morning light's gentle flick\",\n",
       "               'Shadows dance when flames lick the burning stick',\n",
       "               'We built a fort with stones and one stick',\n",
       "               'Beneath the cherry tree, my memories I pick'],\n",
       "              'trick': ['When the rain falls, the roads become slick',\n",
       "               \"Stars appear in darkness with heaven's celestial flick\"]}),\n",
       " 'night_rhymes': defaultdict(list,\n",
       "             {'sight': ['Stars twinkle like diamonds in the velvet night',\n",
       "               'The sunset paints the sky with colors so bright',\n",
       "               \"Butterflies dance in the garden's gentle flight\",\n",
       "               'The stars in the night sky shine incredibly tight'],\n",
       "              'light': ['Whispers of dreams dance through the still night',\n",
       "               'Shadows stretch long across the mysterious night',\n",
       "               'Crickets sing their lullabies throughout the summer night',\n",
       "               'Ghosts of memories haunt the lonely night',\n",
       "               'Mountains stand as ancient guardians of majestic height',\n",
       "               'Her confidence shines like a beacon at full height',\n",
       "               'Courage carries us through fear to newfound height',\n",
       "               'Whispers of love echo in the still of night',\n",
       "               'Whispers of courage echo through the night might',\n",
       "               'Through valleys deep, mountains rise with ancient might',\n",
       "               \"Within each seed lies nature's dormant might\",\n",
       "               'The eagle soared, a symbol of majestic flight',\n",
       "               'Memories flutter like birds in sudden flight',\n",
       "               'Hearts race as love embarks on its first flight',\n",
       "               'Autumn leaves spiral downward in their final flight',\n",
       "               'Hold my hand through darkness into right',\n",
       "               'Whispers of truth echo from mountain height',\n",
       "               'Courage stands tall facing fear with might',\n",
       "               'The dawn breaks with promise, igniting will to fight',\n",
       "               'Stars align when hearts unite in righteous fight',\n",
       "               'Stars twinkle in the vast night sky, ever bright',\n",
       "               'Morning sun paints the meadow golden bright',\n",
       "               'Memories bound in a chest locked up tight',\n",
       "               'Through stormy seas, we held on fierce and tight',\n",
       "               'In silent moments, time stands perfectly still and tight'],\n",
       "              'bright': ['Lovers walk hand in hand beneath the starry night',\n",
       "               \"Souls connect across time with love's eternal might\",\n",
       "               'Memories of fallen heroes inspire us to fight',\n",
       "               'Souls reach upward, forever seeking the divine light'],\n",
       "              'night': ['Autumn leaves dance gracefully out of sight',\n",
       "               'Stars twinkle like diamonds in the vast cosmic sight',\n",
       "               'Dreams take flight beyond the realm of sight',\n",
       "               'As darkness falls, stars shine with celestial might',\n",
       "               'Dreams unfold where imagination takes flight might',\n",
       "               'Hope rises like a phoenix in triumphant flight',\n",
       "               'In shadows deep, your presence feels so right',\n",
       "               'Fireflies dance through summer meadows, tiny dots of light',\n",
       "               'Through darkest storms we find our way toward light',\n",
       "               'In your eyes I see reflections of eternal light',\n",
       "               'In the dark, her smile shines so bright',\n",
       "               \"Through shadows, hope's flame burns steady and bright\",\n",
       "               'After rain, rainbow colors appear vividly bright',\n",
       "               'His eyes, reflecting love, sparkled incredibly bright',\n",
       "               'Autumn leaves dance in colors fiery bright',\n",
       "               'Even in despair, keep your spirit bright'],\n",
       "              'might': [\"Freedom's song rings clear in every worthy fight\"]}),\n",
       " 'doom_rhymes': defaultdict(list,\n",
       "             {'gloom': ['Flowers stretch toward the sun, ready to bloom',\n",
       "               'Whispers of spring dance in fields that bloom',\n",
       "               'Hope rises from darkness, watch optimism bloom',\n",
       "               'Tiny seeds in soil, waiting to bloom',\n",
       "               'Love grows in hearts where kindness can bloom',\n",
       "               'Storms wash away, giving space to bloom',\n",
       "               \"Silent strength beneath winter's rest, preparing to bloom\",\n",
       "               'Even in harsh deserts, wildflowers find ways to bloom',\n",
       "               \"Children's laughter echoes in playground zoom\",\n",
       "               'The shadows grow longer, whispering tales of doom',\n",
       "               'Brave souls stand unwavering in the face of doom',\n",
       "               \"Colors blend and dance together on the artist's loom\",\n",
       "               \"Hope's golden threads shine brightest on tomorrow's loom\",\n",
       "               'Moonlight casts long shadows on the forgotten tomb',\n",
       "               'Tears fall silently upon the freshly sealed tomb',\n",
       "               'Fireworks paint the darkness with colorful boom',\n",
       "               'My heart beats wildly when you enter the room boom',\n",
       "               \"A witch's shadow dances behind her trusty broom\",\n",
       "               \"I sweep away yesterday's sorrows with my broom\"],\n",
       "              'bloom': ['Time slips through fingers with relentless zoom'],\n",
       "              'tomb': ['Shadows dance across the walls in perpetual gloom'],\n",
       "              'room': ['Memories scattered across the floor, needing a broom',\n",
       "               \"Dreams collect in corners waiting for morning's broom\"]}),\n",
       " 'sleep_rhymes': defaultdict(list,\n",
       "             {'keep': [\"Time collects moments in life's messy heap\",\n",
       "               'Stars in the night sky form a celestial heap',\n",
       "               'Stars shine brightest in skies infinitely deep',\n",
       "               'Her eyes held stories in waters running deep',\n",
       "               'Roots of ancient trees grow stubbornly deep',\n",
       "               'Wounds of the heart cut painfully deep',\n",
       "               \"Mountain valleys carved by time's patient hand run deep\",\n",
       "               'Silence speaks volumes when emotions run deep',\n",
       "               'The moon whispers secrets as I drift to sleep',\n",
       "               'Waves of dreams crash upon the shores of sleep',\n",
       "               'Cradled in silence, my thoughts surrender to sleep',\n",
       "               'Stars paint lullabies across the canvas of sleep',\n",
       "               'Shadows dance quietly while I welcome sleep',\n",
       "               'Time slows its march in the realm of sleep',\n",
       "               'Worries dissolve in the warm embrace of sleep',\n",
       "               'Gentle darkness wraps around me promising sleep',\n",
       "               'Memories fade as consciousness yields to sleep',\n",
       "               'Dreams unfurled on wings ready to leap',\n",
       "               'Love transcends boundaries with one passionate leap',\n",
       "               'From cocoons of comfort, butterflies gracefully leap',\n",
       "               'Ink on parchment where thoughts gently seep',\n",
       "               'Memories flood back, as the skies begin to weep',\n",
       "               'Mountains stand tall while valleys quietly weep',\n",
       "               'Ancient trees with stories that make the wind weep',\n",
       "               'In silence, the strongest hearts sometimes softly weep',\n",
       "               'Shadows dance as mountains rise, majestically steep',\n",
       "               'Time slips away like sand on dunes steep',\n",
       "               'Through the misty forest, the lone wolf does creep',\n",
       "               'Time, an unstoppable force, continues to creep'],\n",
       "              'deep': [\"Sow kindness today, tomorrow you'll reap\",\n",
       "               'Through stormy seas, our promises we keep',\n",
       "               'Footprints in sand that tides refuse to keep',\n",
       "               'Broken promises scatter like tears when lovers weep',\n",
       "               'Moonlight through curtain gaps does softly creep'],\n",
       "              'sleep': ['Golden rays of hope into unknown depths leap',\n",
       "               'Stars beckon dreamers to celestial heights leap']}),\n",
       " 'band_rhymes': defaultdict(list,\n",
       "             {'hand': ['Heartbeats sync to the rhythm of the band',\n",
       "               'Golden circles promise forever, a wedding band',\n",
       "               'Melodies dance through the air from the band',\n",
       "               'Stars across the night sky form a celestial band',\n",
       "               'Footprints fade on the shifting sands of this land',\n",
       "               'Under starlight we wander through unfamiliar land',\n",
       "               'Her voice echoes across the frozen land',\n",
       "               'Seeds of hope planted in fertile land',\n",
       "               'Strangers become family in this welcoming land',\n",
       "               'Freedom whispers through trees of the forbidden land',\n",
       "               'Beneath the stars, our love will always stand grand',\n",
       "               'Her smile lights up rooms with brilliance grand',\n",
       "               \"Dreams unfold in ways we hadn't planned\",\n",
       "               'Beneath starlit skies, our future carefully planned',\n",
       "               'Hearts intertwine beyond what either had planned',\n",
       "               'Small moments of joy, nothing we planned',\n",
       "               'Through chaos and calm, adventures spontaneously planned',\n",
       "               'Time slips away despite what we planned',\n",
       "               'Beautiful chaos emerges from what we deliberately planned',\n",
       "               'A gentle touch, a moment to understand',\n",
       "               'Memories of summer preserved, bottled, and canned',\n",
       "               'Time stops for peaches perfectly sliced and canned',\n",
       "               'Hearts united under the banner of a trusted brand'],\n",
       "              'sand': ['Feelings fade like colors of a sunset turning bland'],\n",
       "              'land': ['Life without passion is merely bland',\n",
       "               'Ancient trees tell stories of something truly grand'],\n",
       "              'stand': ['I dream of a home in a distant land',\n",
       "               'Whispers of love traced upon my hand']}),\n",
       " 'sing_rhymes': defaultdict(list,\n",
       "             {'sing': [\"Hearts bound together by love's sacred ring\",\n",
       "               'Ocean waves crash, what mysteries they bring',\n",
       "               'Distant mountains form the throne of winter king'],\n",
       "              'wing': ['In silent rooms, let your truth loudly sing',\n",
       "               'When words fail you, let your spirit sing',\n",
       "               \"Shadows stretch long across autumn's fading thing\"],\n",
       "              'thing': ['The lion roars to announce the jungle king']}),\n",
       " 'unfold_rhymes': defaultdict(list,\n",
       "             {'untold': ['Mysteries of the universe gradually unfold',\n",
       "               \"Life's precious moments continually unfold\",\n",
       "               'Dignity in every action, values we uphold',\n",
       "               'Through darkness and doubt, our spirits uphold',\n",
       "               'Promises whispered under stars, forever uphold',\n",
       "               'Fragile dreams that strong hands gently uphold',\n",
       "               'With love as compass, sacred bonds we uphold',\n",
       "               'Whispers in moonlight, where dreams become enrolled',\n",
       "               'Through ancient tomes wisdom has been enrolled',\n",
       "               'Stars guide wanderers, in cosmic dance enrolled',\n",
       "               \"Children's laughter echoes where they're newly enrolled\",\n",
       "               'Time weaves patterns for those willingly enrolled',\n",
       "               'Ocean waves tell tales where sailors once enrolled',\n",
       "               'Mountain paths reveal secrets to those enrolled',\n",
       "               'Ancient spirits dance where chaos is barely controlled',\n",
       "               'Memories flood back, the tide never controlled',\n",
       "               'Time marches forward, by no mortal hand controlled',\n",
       "               'Time served, debt paid, at last officially paroled',\n",
       "               'Birds soar above the man recently paroled',\n",
       "               \"The sky's vast canvas of stars, a wonder to behold\",\n",
       "               'Ancient trees whisper secrets only forests can behold',\n",
       "               'In her eyes, galaxies of dreams waiting to behold',\n",
       "               'Mountains standing tall, majestic landscapes to behold',\n",
       "               \"Waves crash against cliffs, nature's power to behold\",\n",
       "               \"Children's laughter, purest joy one could ever behold\",\n",
       "               'Time weaves tapestries of moments for us to behold',\n",
       "               'First light breaks darkness, a new day to behold',\n",
       "               'Ancient whispers in the wind, their secrets foretold',\n",
       "               'Dancing shadows on the wall, destinies foretold',\n",
       "               'Stars align in cosmic patterns, futures foretold',\n",
       "               'Crystal visions in the mist, mysteries foretold',\n",
       "               'Silent tears speak volumes of sorrows once foretold',\n",
       "               'Beneath moonlit skies, my fears were softly cajoled'],\n",
       "              'told': ['Through gentle words, let truth unfold',\n",
       "               \"Soldiers march forward, in history's pages enrolled\",\n",
       "               'Her gentle kindness, by all who knew extolled'],\n",
       "              'unfold': ['Echoes of forgotten stories, yearning to be retold',\n",
       "               \"Between breaths, find peace in what can't be controlled\"]}),\n",
       " 'shore_rhymes': defaultdict(list,\n",
       "             {'forevermore': ['Silent promises made on a distant shore',\n",
       "               'Memories etched deep within my very core',\n",
       "               'Mountains stand tall against time, their secrets they bore',\n",
       "               'Lovers walk hand in hand on the peaceful shore',\n",
       "               \"Shells tell ancient stories from the ocean's shore\",\n",
       "               'From ancient vessels wisdom and knowledge pour',\n",
       "               'Let your love like endless rivers into me pour']}),\n",
       " 'pain_rhymes': defaultdict(list,\n",
       "             {'pain': ['Tears fall silently, much like summer rain',\n",
       "               'Hearts find solace in rhythmic sounds of rain',\n",
       "               \"Children's laughter echoes through curtains of rain\",\n",
       "               'Time slips through fingers like water down a drain',\n",
       "               'Her smile, a beacon through darkness, simple yet plain',\n",
       "               'Tears fell upon the canvas, leaving a crimson stain',\n",
       "               'Whispered secrets in the dark, an invisible stain',\n",
       "               'Words once spoken cannot be erased, a verbal stain',\n",
       "               'Time passes but guilt remains a stubborn stain',\n",
       "               'Memories rush by like an old forgotten train'],\n",
       "              '': ['Raindrops patter on the windowpane, echoing down the drain'],\n",
       "              'reign': ['Dreams carry me forward on an imaginary train']}),\n",
       " 'skies_rhymes': defaultdict(list,\n",
       "             {'skies': ['In the garden of thoughts, hope quietly flies',\n",
       "               'Against all odds, the spirit forever flies',\n",
       "               \"Happiness isn't found in what money buys\",\n",
       "               'Beyond the horizon where hope eternally lies',\n",
       "               'Through shadows of doubt, hope continues to rise',\n",
       "               'Mountains stand tall, majestic in their eternal rise',\n",
       "               'Stars fade as dawn begins its golden rise',\n",
       "               'Voices join in harmony, letting spirits rise',\n",
       "               'From ashes of defeat, the phoenix will rise',\n",
       "               'Waves crash upon shores with thunderous rise',\n",
       "               \"Children's laughter carries us to delightful natural highs\",\n",
       "               'Mountains stand tall, teaching souls to be wise',\n",
       "               \"In nature's embrace, we learn to be wise\",\n",
       "               \"Facing life's storms requires being brave and wise\",\n",
       "               'Whispers of love echo through time, binding our ties',\n",
       "               'Children laugh, creating bonds that become lifelong ties',\n",
       "               'Ocean waves crash, washing away the weakest ties',\n",
       "               'Through stormy nights the lonely lighthouse keeper cries',\n",
       "               'Between whispered prayers, the faithful heart softly cries',\n",
       "               \"Oceans carry messages in seagulls' distant cries\"],\n",
       "              'eyes': ['Within your heart, let courage steadily rise']}),\n",
       " 'bake_rhymes': defaultdict(list,\n",
       "             {'sake': ['Whispers of hope in the shadows we make',\n",
       "               'Across oceans of doubt, connections we make',\n",
       "               'Stars guide wanderers on journeys they make',\n",
       "               'Sunbeams through curtains coax me to wake',\n",
       "               \"Silent promises, that's all I can take\",\n",
       "               'Gentle rivers know which course to take']}),\n",
       " 'call_rhymes': defaultdict(list,\n",
       "             {'all': [\"Memories echo through time's haunting call\",\n",
       "               \"Hearts beat in unison to love's call\",\n",
       "               'Christmas lights twinkle throughout the winter mall',\n",
       "               'I caught the memory as it fell like a ball',\n",
       "               'She danced gracefully across the crowded dance hall ball',\n",
       "               'Morning dew glistens on flowers at the garden stall',\n",
       "               'A whisper in the wind, so gentle and small',\n",
       "               'Her laughter resonates throughout the grand hall'],\n",
       "              'small': ['Security cameras silently watching over the mall',\n",
       "               'Time slips through fingers quick as a bouncing ball'],\n",
       "              'tall': ['Before we run, first we must crawl'],\n",
       "              'call': ['Sunflowers bend toward light, determined to grow tall',\n",
       "               'Through stormy nights, the lighthouse remains steadfast and tall',\n",
       "               'Whispered legends speak of heroes brave and tall',\n",
       "               \"Hearts beat in unison behind the chest's warm wall\"]})}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ced0650a-1311-4083-8e72-6a3f23965ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('suggested_rhymes.json',\"w\") as f:\n",
    "    json.dump(suggested_rhymes,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "14ce3ae8-aec1-44ba-93ee-153d311154c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "night_rhymes Counter({'light': 25, 'night': 16, 'sight': 4, 'bright': 4, 'might': 1})\n",
      "sleep_rhymes Counter({'keep': 29, 'deep': 5, 'sleep': 2})\n",
      "call_rhymes Counter({'all': 8, 'call': 4, 'small': 2, 'tall': 1})\n"
     ]
    }
   ],
   "source": [
    "for rtype in suggested_rhymes:\n",
    "    if suggested_rhymes[rtype].keys():\n",
    "        print(rtype, suggwords[rtype])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f8e2ec7a-1e14-4571-b36e-193838f9e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n"
     ]
    }
   ],
   "source": [
    "POSITIVE_PROMPTS=get_prompts(suggested_rhymes[\"sleep_rhymes\"][\"keep\"])\n",
    "NEGATIVE_PROMPTS=get_prompts(suggested_rhymes[\"sleep_rhymes\"][\"deep\"])\n",
    "\n",
    "keepdeepvector=get_steering_vector_fast(model, tokenizer, POSITIVE_PROMPTS, NEGATIVE_PROMPTS, layer=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cfa0f3bd-c409-4b78-a2ea-d309f28fbc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.7500, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(keepdeepvector, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "77f480ed-f52a-461d-a9b4-7b7bfbfc38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=random.choice(line_catalog[\"sleep_rhymes\"])\n",
    "prompt=get_prompts([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "95559965-999f-4856-b2d1-ffc8660acc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flowers up the garden wall gracefully creep'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "efff4e73-ea42-406a-b902-8beb4752b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  10%|█         | 1/10 [00:11<01:39, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  20%|██        | 2/10 [00:21<01:27, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  30%|███       | 3/10 [00:33<01:17, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  40%|████      | 4/10 [00:44<01:06, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  50%|█████     | 5/10 [00:55<00:55, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  60%|██████    | 6/10 [01:06<00:44, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  70%|███████   | 7/10 [01:17<00:33, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  80%|████████  | 8/10 [01:28<00:22, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  90%|█████████ | 9/10 [01:39<00:11, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [01:50<00:00, 11.07s/it]\n"
     ]
    }
   ],
   "source": [
    "steered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                10000, layer=20, steering_multiplier=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "adf62eea-63da-451e-8611-c69ec091f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  10%|█         | 1/10 [00:11<01:40, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  20%|██        | 2/10 [00:22<01:28, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  30%|███       | 3/10 [00:33<01:17, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  40%|████      | 4/10 [00:44<01:06, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  50%|█████     | 5/10 [00:55<00:55, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  60%|██████    | 6/10 [01:06<00:44, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  70%|███████   | 7/10 [01:17<00:33, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  80%|████████  | 8/10 [01:28<00:22, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  90%|█████████ | 9/10 [01:39<00:11, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [01:50<00:00, 11.08s/it]\n"
     ]
    }
   ],
   "source": [
    "baseline_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                10000, layer=20, steering_multiplier=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d9ceb929-4a3f-455a-a097-4853b1e721a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  10%|█         | 1/10 [00:11<01:39, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  20%|██        | 2/10 [00:22<01:28, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  30%|███       | 3/10 [00:33<01:17, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  40%|████      | 4/10 [00:44<01:06, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  50%|█████     | 5/10 [00:55<00:55, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  60%|██████    | 6/10 [01:06<00:44, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  70%|███████   | 7/10 [01:17<00:33, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  80%|████████  | 8/10 [01:28<00:22, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:  90%|█████████ | 9/10 [01:39<00:11, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [01:50<00:00, 11.09s/it]\n"
     ]
    }
   ],
   "source": [
    "negsteered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                10000, layer=20, steering_multiplier=-2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "94c33055-3441-4697-802b-d2a24b4a3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of \"deep\":8, \"keep\":45\n"
     ]
    }
   ],
   "source": [
    "freqs=compl[\"sleep_rhymes\"][example]\n",
    "print(f'earlier baseline frequency of \\\"deep\\\":{freqs[\"deep\"]}, \\\"keep\\\":{freqs[\"keep\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d0bd1ba1-6f9e-4303-81e5-15cc071e5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline frequency of \"deep\":1682, \"keep\":3853\n"
     ]
    }
   ],
   "source": [
    "freqs=get_last_word_distribution(baseline_text)[example]\n",
    "print(f'Baseline frequency of \\\"deep\\\":{freqs[\"deep\"]}, \\\"keep\\\":{freqs[\"keep\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d29a4631-d489-45e9-adeb-4f3c20f28768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered towards \"keep\" frequency of \"deep\":1704, \"keep\":4177\n"
     ]
    }
   ],
   "source": [
    "freqs=get_last_word_distribution(steered_text)[example]\n",
    "print(f'Steered towards \\\"keep\\\" frequency of \\\"deep\\\":{freqs[\"deep\"]}, \\\"keep\\\":{freqs[\"keep\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "88f4705a-8c6e-4c0b-b1e7-fab4c9fd58ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered towards \"deep\" frequency of \"deep\":1943, \"keep\":1172\n"
     ]
    }
   ],
   "source": [
    "freqs=get_last_word_distribution(negsteered_text)[example]\n",
    "print(f'Steered towards \\\"deep\\\" frequency of \\\"deep\\\":{freqs[\"deep\"]}, \\\"keep\\\":{freqs[\"keep\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7411ada0-1e18-489b-bd51-dff1013f2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "light night\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(19.6250, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 1\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.72s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -1\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.81s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Children's laughter fills the room, joyously bright\n",
      "==============================\n",
      "earlier baseline frequency of \"light\":39, \"night\":13\n",
      "Baseline frequency of \"light\":393, \"night\":144\n",
      "Steered towards \"light\" frequency of \"light\":389, \"night\":211\n",
      "Steered towards \"night\" frequency of \"light\":348, \"night\":104\n",
      "==============================\n",
      "keep deep\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(34.7500, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 1\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.43s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -1\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.47s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Courage builds bridges where fears dare to leap\n",
      "==============================\n",
      "earlier baseline frequency of \"keep\":19, \"deep\":31\n",
      "Baseline frequency of \"keep\":218, \"deep\":335\n",
      "Steered towards \"keep\" frequency of \"keep\":266, \"deep\":261\n",
      "Steered towards \"deep\" frequency of \"keep\":189, \"deep\":362\n",
      "==============================\n",
      "all call\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(35.2500, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 1\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.54s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -1\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.57s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Courage grows slowly when we learn to crawl\n",
      "==============================\n",
      "earlier baseline frequency of \"all\":15, \"call\":7\n",
      "Baseline frequency of \"all\":122, \"call\":70\n",
      "Steered towards \"all\" frequency of \"all\":92, \"call\":94\n",
      "Steered towards \"call\" frequency of \"all\":154, \"call\":38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ALL_POSITIONS=True\n",
    "for rtype in suggested_rhymes:\n",
    "    if suggested_rhymes[rtype].keys():\n",
    "        word1,word2=[i[0] for i in suggwords[rtype].most_common(2)]\n",
    "        print('='*30)\n",
    "        print(word1,word2)\n",
    "        print('='*30)\n",
    "        POSITIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word1])\n",
    "        NEGATIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word2])\n",
    "\n",
    "        steering_vector=get_steering_vector_fast(model, tokenizer, POSITIVE_PROMPTS, NEGATIVE_PROMPTS, layer=20)\n",
    "        n=torch.norm(steering_vector, p=2)\n",
    "        print(\"L2 norm of steering vector:\",n)\n",
    "        STEERING_MULTIPLIER=75/n\n",
    "        STEERING_MULTIPLIER=1\n",
    "        example=random.choice(line_catalog[rtype])\n",
    "        while example.endswith(word1) or example.endswith(word2) or example in suggested_rhymes[rtype][word1] or example in suggested_rhymes[rtype][word2]:\n",
    "            example=random.choice(line_catalog[rtype])\n",
    "        prompt=get_prompts([example])\n",
    "        steered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=20, steering_multiplier=STEERING_MULTIPLIER\n",
    "            )\n",
    "        negsteered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=20, steering_multiplier=-STEERING_MULTIPLIER\n",
    "            )\n",
    "        baseline_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=20, steering_multiplier=0\n",
    "            )\n",
    "        freqse=compl[rtype][example]\n",
    "        freqsb=get_last_word_distribution(baseline_text)[example]\n",
    "        freqss=get_last_word_distribution(steered_text)[example]\n",
    "        freqsn=get_last_word_distribution(negsteered_text)[example]\n",
    "        print('='*30)\n",
    "        print(example)\n",
    "        print('='*30)\n",
    "        print(f'earlier baseline frequency of \\\"{word1}\\\":{freqse[word1]}, \\\"{word2}\\\":{freqse[word2]}')\n",
    "        print(f'Baseline frequency of \\\"{word1}\\\":{freqsb[word1]}, \\\"{word2}\\\":{freqsb[word2]}')\n",
    "        print(f'Steered towards \\\"{word1}\\\" frequency of \\\"{word1}\\\":{freqss[word1]}, \\\"{word2}\\\":{freqss[word2]}')\n",
    "        print(f'Steered towards \\\"{word2}\\\" frequency of \\\"{word1}\\\":{freqsn[word1]}, \\\"{word2}\\\":{freqsn[word2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "00fef93c-794a-4b75-b339-b2dc9717f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "light night\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(47.5000, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 1\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.64s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier -1\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.79s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 0\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Love's tender touch reveals its transformative might\n",
      "==============================\n",
      "earlier baseline frequency of \"light\":41, \"night\":30\n",
      "Baseline frequency of \"light\":454, \"night\":295\n",
      "Steered towards \"light\" frequency of \"light\":428, \"night\":318\n",
      "Steered towards \"night\" frequency of \"light\":453, \"night\":236\n",
      "==============================\n",
      "keep deep\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(73.5000, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 1\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.69s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier -1\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.71s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 0\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Ivy tendrils around the ancient stone wall creep\n",
      "==============================\n",
      "earlier baseline frequency of \"keep\":47, \"deep\":11\n",
      "Baseline frequency of \"keep\":430, \"deep\":123\n",
      "Steered towards \"keep\" frequency of \"keep\":412, \"deep\":115\n",
      "Steered towards \"deep\" frequency of \"keep\":503, \"deep\":97\n",
      "==============================\n",
      "all call\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(82., dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 1\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.44s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier -1\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.44s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 0\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "In shadows deep, spiders silently crawl\n",
      "==============================\n",
      "earlier baseline frequency of \"all\":27, \"call\":6\n",
      "Baseline frequency of \"all\":274, \"call\":48\n",
      "Steered towards \"all\" frequency of \"all\":322, \"call\":58\n",
      "Steered towards \"call\" frequency of \"all\":257, \"call\":55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LAYER=27\n",
    "ALL_POSITIONS=True\n",
    "\n",
    "for rtype in suggested_rhymes:\n",
    "    if suggested_rhymes[rtype].keys():\n",
    "        word1,word2=[i[0] for i in suggwords[rtype].most_common(2)]\n",
    "        print('='*30)\n",
    "        print(word1,word2)\n",
    "        print('='*30)\n",
    "        POSITIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word1])\n",
    "        NEGATIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word2])\n",
    "\n",
    "        steering_vector=get_steering_vector_fast(model, tokenizer, POSITIVE_PROMPTS, NEGATIVE_PROMPTS, layer=LAYER)\n",
    "        n=torch.norm(steering_vector, p=2)\n",
    "        print(\"L2 norm of steering vector:\",n)\n",
    "        STEERING_MULTIPLIER=80/n\n",
    "        STEERING_MULTIPLIER=1\n",
    "        example=random.choice(line_catalog[rtype])\n",
    "        while example.endswith(word1) or example.endswith(word2) or example in suggested_rhymes[rtype][word1] or example in suggested_rhymes[rtype][word2]:\n",
    "            example=random.choice(line_catalog[rtype])\n",
    "        prompt=get_prompts([example])\n",
    "        steered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=STEERING_MULTIPLIER\n",
    "            )\n",
    "        negsteered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=-STEERING_MULTIPLIER\n",
    "            )\n",
    "        baseline_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=0\n",
    "            )\n",
    "        freqse=compl[rtype][example]\n",
    "        freqsb=get_last_word_distribution(baseline_text)[example]\n",
    "        freqss=get_last_word_distribution(steered_text)[example]\n",
    "        freqsn=get_last_word_distribution(negsteered_text)[example]\n",
    "        print('='*30)\n",
    "        print(example)\n",
    "        print('='*30)\n",
    "        print(f'earlier baseline frequency of \\\"{word1}\\\":{freqse[word1]}, \\\"{word2}\\\":{freqse[word2]}')\n",
    "        print(f'Baseline frequency of \\\"{word1}\\\":{freqsb[word1]}, \\\"{word2}\\\":{freqsb[word2]}')\n",
    "        print(f'Steered towards \\\"{word1}\\\" frequency of \\\"{word1}\\\":{freqss[word1]}, \\\"{word2}\\\":{freqss[word2]}')\n",
    "        print(f'Steered towards \\\"{word2}\\\" frequency of \\\"{word1}\\\":{freqsn[word1]}, \\\"{word2}\\\":{freqsn[word2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1e2a7180-38ec-4b09-96c0-566dba772134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "light night\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(19.6250, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 5.09375\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.58s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -5.09375\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.62s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "The sunset paints the sky with colors so bright\n",
      "==============================\n",
      "earlier baseline frequency of \"light\":16, \"night\":11\n",
      "Baseline frequency of \"light\":131, \"night\":148\n",
      "Steered towards \"light\" frequency of \"light\":146, \"night\":297\n",
      "Steered towards \"night\" frequency of \"light\":227, \"night\":183\n",
      "==============================\n",
      "keep deep\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(34.7500, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2.875\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.72s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2.875\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.82s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Doubts into my once certain mind subtly creep\n",
      "==============================\n",
      "earlier baseline frequency of \"keep\":30, \"deep\":16\n",
      "Baseline frequency of \"keep\":382, \"deep\":132\n",
      "Steered towards \"keep\" frequency of \"keep\":402, \"deep\":108\n",
      "Steered towards \"deep\" frequency of \"keep\":207, \"deep\":191\n",
      "==============================\n",
      "all call\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(35.2500, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 2.828125\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.67s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier -2.828125\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.68s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.20 with multiplier 0\n",
      "Steering hook removed from model.layers.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Beneath starlit skies, thoughts begin to crawl\n",
      "==============================\n",
      "earlier baseline frequency of \"all\":13, \"call\":24\n",
      "Baseline frequency of \"all\":146, \"call\":238\n",
      "Steered towards \"all\" frequency of \"all\":187, \"call\":321\n",
      "Steered towards \"call\" frequency of \"all\":155, \"call\":66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LAYER=20\n",
    "ALL_POSITIONS=False\n",
    "\n",
    "for rtype in suggested_rhymes:\n",
    "    if suggested_rhymes[rtype].keys():\n",
    "        word1,word2=[i[0] for i in suggwords[rtype].most_common(2)]\n",
    "        print('='*30)\n",
    "        print(word1,word2)\n",
    "        print('='*30)\n",
    "        POSITIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word1])\n",
    "        NEGATIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word2])\n",
    "\n",
    "        steering_vector=get_steering_vector_fast(model, tokenizer, POSITIVE_PROMPTS, NEGATIVE_PROMPTS, layer=LAYER)\n",
    "        n=torch.norm(steering_vector, p=2)\n",
    "        print(\"L2 norm of steering vector:\",n)\n",
    "        STEERING_MULTIPLIER=100/n\n",
    "        #STEERING_MULTIPLIER=1\n",
    "        example=random.choice(line_catalog[rtype])\n",
    "        while example.endswith(word1) or example.endswith(word2) or example in suggested_rhymes[rtype][word1] or example in suggested_rhymes[rtype][word2]:\n",
    "            example=random.choice(line_catalog[rtype])\n",
    "        prompt=get_prompts([example])\n",
    "        steered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=STEERING_MULTIPLIER\n",
    "            )\n",
    "        negsteered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=-STEERING_MULTIPLIER\n",
    "            )\n",
    "        baseline_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=0\n",
    "            )\n",
    "        freqse=compl[rtype][example]\n",
    "        freqsb=get_last_word_distribution(baseline_text)[example]\n",
    "        freqss=get_last_word_distribution(steered_text)[example]\n",
    "        freqsn=get_last_word_distribution(negsteered_text)[example]\n",
    "        print('='*30)\n",
    "        print(example)\n",
    "        print('='*30)\n",
    "        print(f'earlier baseline frequency of \\\"{word1}\\\":{freqse[word1]}, \\\"{word2}\\\":{freqse[word2]}')\n",
    "        print(f'Baseline frequency of \\\"{word1}\\\":{freqsb[word1]}, \\\"{word2}\\\":{freqsb[word2]}')\n",
    "        print(f'Steered towards \\\"{word1}\\\" frequency of \\\"{word1}\\\":{freqss[word1]}, \\\"{word2}\\\":{freqss[word2]}')\n",
    "        print(f'Steered towards \\\"{word2}\\\" frequency of \\\"{word1}\\\":{freqsn[word1]}, \\\"{word2}\\\":{freqsn[word2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d18fb87b-1da3-4d4b-8a1a-e923ed7d5e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "light night\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(47.5000, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 2.09375\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.51s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier -2.09375\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.59s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 0\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Ocean waves crash with tremendous might and sight\n",
      "==============================\n",
      "earlier baseline frequency of \"light\":38, \"night\":17\n",
      "Baseline frequency of \"light\":342, \"night\":196\n",
      "Steered towards \"light\" frequency of \"light\":416, \"night\":253\n",
      "Steered towards \"night\" frequency of \"light\":225, \"night\":234\n",
      "==============================\n",
      "keep deep\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(73.5000, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 1.359375\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.58s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier -1.359375\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.59s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 0\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "In moments of truth, souls instinctively leap\n",
      "==============================\n",
      "earlier baseline frequency of \"keep\":34, \"deep\":16\n",
      "Baseline frequency of \"keep\":374, \"deep\":152\n",
      "Steered towards \"keep\" frequency of \"keep\":282, \"deep\":149\n",
      "Steered towards \"deep\" frequency of \"keep\":445, \"deep\":161\n",
      "==============================\n",
      "all call\n",
      "==============================\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.27' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "L2 norm of steering vector: tensor(82., dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 1.21875\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.60s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier -1.21875\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.59s/it]\n",
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering hook applied to model.layers.27 with multiplier 0\n",
      "Steering hook removed from model.layers.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 1/1 [00:11<00:00, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Before we run, first we must crawl\n",
      "==============================\n",
      "earlier baseline frequency of \"all\":9, \"call\":3\n",
      "Baseline frequency of \"all\":109, \"call\":30\n",
      "Steered towards \"all\" frequency of \"all\":66, \"call\":24\n",
      "Steered towards \"call\" frequency of \"all\":99, \"call\":22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LAYER=27\n",
    "ALL_POSITIONS=False\n",
    "\n",
    "for rtype in suggested_rhymes:\n",
    "    if suggested_rhymes[rtype].keys():\n",
    "        word1,word2=[i[0] for i in suggwords[rtype].most_common(2)]\n",
    "        print('='*30)\n",
    "        print(word1,word2)\n",
    "        print('='*30)\n",
    "        POSITIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word1])\n",
    "        NEGATIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word2])\n",
    "\n",
    "        steering_vector=get_steering_vector_fast(model, tokenizer, POSITIVE_PROMPTS, NEGATIVE_PROMPTS, layer=LAYER)\n",
    "        n=torch.norm(steering_vector, p=2)\n",
    "        print(\"L2 norm of steering vector:\",n)\n",
    "        STEERING_MULTIPLIER=100/n\n",
    "        #STEERING_MULTIPLIER=1\n",
    "        example=random.choice(line_catalog[rtype])\n",
    "        while example.endswith(word1) or example.endswith(word2) or example in suggested_rhymes[rtype][word1] or example in suggested_rhymes[rtype][word2]:\n",
    "            example=random.choice(line_catalog[rtype])\n",
    "        prompt=get_prompts([example])\n",
    "        steered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=STEERING_MULTIPLIER\n",
    "            )\n",
    "        negsteered_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=-STEERING_MULTIPLIER\n",
    "            )\n",
    "        baseline_text = generate_steered_output(\n",
    "                keepdeepvector, model, tokenizer, prompt, \n",
    "                1000, layer=LAYER, steering_multiplier=0\n",
    "            )\n",
    "        freqse=compl[rtype][example]\n",
    "        freqsb=get_last_word_distribution(baseline_text)[example]\n",
    "        freqss=get_last_word_distribution(steered_text)[example]\n",
    "        freqsn=get_last_word_distribution(negsteered_text)[example]\n",
    "        print('='*30)\n",
    "        print(example)\n",
    "        print('='*30)\n",
    "        print(f'earlier baseline frequency of \\\"{word1}\\\":{freqse[word1]}, \\\"{word2}\\\":{freqse[word2]}')\n",
    "        print(f'Baseline frequency of \\\"{word1}\\\":{freqsb[word1]}, \\\"{word2}\\\":{freqsb[word2]}')\n",
    "        print(f'Steered towards \\\"{word1}\\\" frequency of \\\"{word1}\\\":{freqss[word1]}, \\\"{word2}\\\":{freqss[word2]}')\n",
    "        print(f'Steered towards \\\"{word2}\\\" frequency of \\\"{word1}\\\":{freqsn[word1]}, \\\"{word2}\\\":{freqsn[word2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "43a1d78c-128b-40eb-bf93-cdd3d95f898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"suggwords.json\",'w') as f:\n",
    "    json.dump(suggwords,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96e719-3fed-4467-94b9-5acdf3aad6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtype=\"sleep\"\n",
    "\n",
    "POSITIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][\"keep\"])\n",
    "NEGATIVE_PROMPTS=get_prompts(suggested_rhymes[rtype][word2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
