{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2d9472-720d-4a52-85ca-a1ccb5f67601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cbd7bf-10a1-4df6-9dfd-c7b9959ee48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
      "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate bitsandbytes einops dotenv matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6a272f-26e6-48a9-b359-21a65064ef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7fdbd1-818f-4729-8c95-1aabe3aa546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250319+cu128\n",
      "Transformers version: 4.51.3\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Optional, Callable\n",
    "import einops\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "# %%\n",
    "dotenv.load_dotenv(\"hf.env\")\n",
    "# @title 1.5. For access to Gemma models, log in to HuggingFace \n",
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")\n",
    "# %%\n",
    "MODEL_ID = \"google/gemma-2-9b-it\" # Or \"google/gemma-2-9b\" if you prefer the base model\n",
    "# Set to True if you have limited VRAM (e.g., < 24GB). Requires bitsandbytes\n",
    "USE_4BIT_QUANTIZATION = False\n",
    "\n",
    "POSITIVE_PROMPTS = [\n",
    "    \"This story should be very optimistic and uplifting.\",\n",
    "    \"Write a hopeful and positive narrative.\",\n",
    "    \"Generate text with a cheerful and encouraging tone.\",\n",
    "]\n",
    "NEGATIVE_PROMPTS = [\n",
    "    \"This story should be very pessimistic and bleak.\",\n",
    "    \"Write a depressing and negative narrative.\",\n",
    "    \"Generate text with a gloomy and discouraging tone.\",\n",
    "]\n",
    "\n",
    "# The prompt to use for actual generation\n",
    "GENERATION_PROMPT = \"Write a short paragraph about the future of artificial intelligence.\"\n",
    "\n",
    "# How strongly to apply the steering vector. Tune this value (e.g., 0.5 to 5.0)\n",
    "STEERING_MULTIPLIER = 1.5\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "MAX_NEW_TOKENS = 30\n",
    "TEMPERATURE = 0.7\n",
    "DO_SAMPLE = True\n",
    "\n",
    "lines_that_rhyme_with_quick = [\n",
    "    \"The house was built with sturdy, reddish brick\",\n",
    "    \"The camera captured moments with each click\",\n",
    "    \"She turned the lights on with a simple flick\",\n",
    "    \"The soccer player gave the ball a mighty kick\",\n",
    "    \"The puppy gave my hand a gentle lick\",\n",
    "    \"The razor left a small and painful nick\",\n",
    "    \"From all the fruits available, I'll make my pick\",\n",
    "    \"The rose's thorn can cause a sudden prick\",\n",
    "    \"He stayed at home because he felt too sick\",\n",
    "    \"The rain had made the winding road quite slick\",\n",
    "    \"The child drew pictures with a charcoal stick\",\n",
    "    \"The winter fog was rolling in so thick\",\n",
    "    \"The clock marked every second with a tick\",\n",
    "    \"The magician performed an amazing trick\",\n",
    "    \"The candle slowly burned down to the wick\",\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_pain = [\n",
    "    \"The storm has passed but soon will come again\",\n",
    "    \"The wizard's knowledge was profoundly arcane\",\n",
    "    \"That constant noise became my existence's bane\",\n",
    "    \"The puzzle challenged every corner of my brain\",\n",
    "    \"The elderly man walked slowly with his cane\",\n",
    "    \"The prisoner rattled his heavy iron chain\",\n",
    "    \"The construction site had a towering crane\",\n",
    "    \"The queen would rarely to respond deign\",\n",
    "    \"The rainwater flowed down into the drain\",\n",
    "    \"She looked at the offer with obvious disdain\",\n",
    "    \"The king surveyed his vast and wealthy domain\",\n",
    "    \"The teacher took her time to clearly explain\",\n",
    "    \"He tried to hide his feelings and to feign\",\n",
    "    \"The pilgrims journeyed to the ancient fane\",\n",
    "    \"The athlete trained for months to make a gain\",\n",
    "    \"The farmer harvested the golden grain\",\n",
    "    \"The doctor's treatment was gentle and humane\",\n",
    "    \"His argument was completely inane\",\n",
    "    \"The plan they proposed was utterly insane\",\n",
    "    \"The classic novel starred a heroine named Jane\",\n",
    "    \"The car sped down the narrow country lane\",\n",
    "    \"The issue at hand was certainly the main\",\n",
    "    \"The lion shook his magnificent mane\",\n",
    "    \"The office work felt repetitive and mundane\",\n",
    "    \"The church would soon the new priest ordain\",\n",
    "    \"The sunlight streamed through the window pane\",\n",
    "    \"The message written there was crystal plain\",\n",
    "    \"The travelers boarded the waiting plane\",\n",
    "    \"His language was considered quite profane\",\n",
    "    \"The flowers bloomed after the gentle rain\",\n",
    "    \"The rider pulled firmly on the horse's rein\",\n",
    "    \"The king began his long and peaceful reign\",\n",
    "    \"Despite the chaos, she remained quite sane\",\n",
    "    \"We planned our summer holiday in Spain\",\n",
    "    \"The athlete suffered from a painful ankle sprain\",\n",
    "    \"The red wine left a permanent stain\",\n",
    "    \"The heavy lifting put his back under strain\",\n",
    "    \"Good habits help your health maintain and sustain\",\n",
    "    \"The maiden was courted by a handsome swain\",\n",
    "    \"We hurried to catch the departing train\",\n",
    "    \"The river split the land in twain\",\n",
    "    \"His manner was sophisticated and urbane\",\n",
    "    \"Her efforts to convince him were in vain\",\n",
    "    \"The wind direction showed on the weather vane\",\n",
    "    \"The nurse carefully located a suitable vein\",\n",
    "    \"As night approached, the daylight began to wane\",\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_rabbit = [\n",
    "    \"I saw something move in the garden, so I decided to grab it\", # To my surprise, it turned out to be a fluffy little rabbit.\n",
    "    \"When you hear a noise in the bushes, don't be afraid to nab it\", # Chances are it's just the neighborhood's friendly rabbit.\n",
    "    \"She has a special way with animals, it's quite a habit\", # Her favorite creature to care for is her pet rabbit.\n",
    "    \"I thought I'd plant some carrots, but something came to stab it\", # I looked outside and caught the culprit—a hungry rabbit.\n",
    "    \"The magician pulled something furry out of his hat, to my amazement he had it\", # The audience cheered when they saw it was a snow-white rabbit.\n",
    "    \"If you find a hole in your garden, you should probably tab it\", # It's likely the new underground home of a burrowing rabbit.\n",
    "    \"The child saw something soft in the pet store and wanted to have it\", # She begged her parents until they bought her that adorable rabbit.\n",
    "    \"I heard a rustling sound in the forest and tried to dab it\", # But it hopped away quickly—I just missed that wild rabbit.\n",
    "    \"When something nibbles your lettuce, there's no need to blab it\", # Everyone knows the culprit is probably a garden rabbit.\n",
    "    \"I felt something soft brush against my leg, I reached down to grab it\", # And found myself petting the silky fur of a friendly rabbit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_habit = [\n",
    "    \"When you see a rabbit\", # You might form a feeding habit.\n",
    "    \"He'd grab it if he could just nab it\", # That's become his daily habit.\n",
    "    \"The frog sits on the lily pad, a bit\", # Too long—it's turned into a habit.\n",
    "    \"She wears that jacket like she's glad to have it\", # Dressing sharp has always been her habit.\n",
    "    \"I know I should quit, but I just can't stab it\", # Breaking free from such a stubborn habit.\n",
    "    \"If there's a chance for joy, I'll always grab it\", # Seeking happiness is my best habit.\n",
    "    \"The cat will chase the yarn if you dab it\", # Playing games has been a lifelong habit.\n",
    "    \"When faced with problems, I don't just blab it\", # Thinking before speaking is my habit.\n",
    "    \"He'll take a compliment, but never crab it\", # Staying humble is his finest habit.\n",
    "    \"The chef will taste the dish before they tab it\", # Quality control's a professional habit.\n",
    "    \"When opportunity knocks, I'll cab it\", # Seizing the moment is my favorite habit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_rabbit = [\n",
    "    \"She couldn't seem to break her gardening habit\", # Until her veggies were stolen by a clever rabbit.\n",
    "    \"He developed quite an interesting habit\", # Of leaving carrots for the neighbor's pet rabbit.\n",
    "    \"The monk maintained his meditation habit\", # While outside his window hopped a curious rabbit.\n",
    "    \"I tried to quit my late-night snacking habit\", # When I spotted in my kitchen a midnight rabbit.\n",
    "    \"The farmer stuck to his early rising habit,\" # And caught sight of a dawn-grazing rabbit.\n",
    "    \"My daughter formed an adorable habit\", # Of reading bedtime stories to her stuffed rabbit.\n",
    "    \"The writer maintained her daily writing habit\", # Creating tales about a mischievous rabbit.\n",
    "    \"The painter couldn't shake her artistic habit\", # Her favorite subject was a snow-white rabbit.\n",
    "    \"She picked up the peculiar habit\", # Of leaving garden notes addressed to a rabbit.\n",
    "    \"He kept up his wholesome forest walking habit\", # Often spotting the same cotton-tailed rabbit.\n",
    "    \"The boy acquired a strange collecting habit\", # Of items shaped like his favorite animal: rabbit.\n",
    "    \"The chef developed an experimental cooking habit\", # Inspired by watching a munching wild rabbit.\n",
    "    \"The photographer formed a dawn shooting habit\", # Capturing perfect moments of a dewdrop-covered rabbit.\n",
    "    \"My grandmother maintained her knitting habit\", # Creating tiny sweaters for her daughter's rabbit.\n",
    "    \"The scientist stuck to her observation habit\", # Documenting behaviors of the laboratory rabbit.\n",
    "    \"The child couldn't break his skipping habit\", # Hopping through the garden like an energetic rabbit.\n",
    "    \"The jogger kept her early morning habit\", # Racing along the trail with a wild rabbit.\n",
    "    \"The wizard practiced his disappearing habit\", # Vanishing from sight much like a magic rabbit.\n",
    "    \"She developed a serious chocolate habit\", # After receiving a gift shaped like a rabbit.\n",
    "    \"The detective never lost his questioning habit\", # Following clues that led to a snow-white rabbit.\n",
    "    \"He cultivated a very precise gardening habit\", # To protect his carrots from the neighborhood rabbit.\n",
    "    \"The composer maintained her nighttime composing habit\", # With melodies inspired by a moonlit rabbit.\n",
    "    \"The teacher had a creative teaching habit\", # Using stories about a wise philosophical rabbit.\n",
    "    \"My uncle can't kick his star-gazing habit\", # Often seeing constellations shaped like a rabbit.\n",
    "    \"She formed an unusual sketching habit\", # Drawing landscapes always featuring a distant rabbit.\n",
    "    \"The doctor maintained a healthy eating habit\", # Enjoying salads that would impress a rabbit.\n",
    "    \"The botanist kept her plant-collecting habit\", # Finding species that attracted the rare mountain rabbit.\n",
    "    \"My brother developed a strange talking habit\", # Of narrating his day to an imaginary rabbit.\n",
    "    \"The seamstress maintained her sewing habit\", # Crafting costumes featuring a dancing rabbit.\n",
    "    \"The old man had a generous feeding habit\", # Sharing his garden harvest with each passing rabbit.\n",
    "    \"The barista perfected her latte art habit\", # Creating foam designs resembling a jumping rabbit.\n",
    "    \"The astronomer continued her stargazing habit\", # Discovering a nebula shaped like a cosmic rabbit.\n",
    "    \"The carpenter refined his woodworking habit\", # Carving intricate figures of a forest rabbit.\n",
    "    \"My cousin formed an unusual naming habit\", # Calling every stray animal 'Peter the rabbit'.\n",
    "    \"The librarian kept her book-suggesting habit\", # Often recommending tales about a clever rabbit.\n",
    "    \"The hiker maintained her trail-blazing habit\", # Following paths once traveled by the snowshoe rabbit.\n",
    "    \"The young girl had a flower-collecting habit\", # Making crowns she'd place upon her patient rabbit.\n",
    "    \"The researcher developed a note-taking habit\", # Recording every movement of the study's rabbit.\n",
    "    \"The poet sustained his daily writing habit\", # Composing verses about a philosophical rabbit.\n",
    "    \"My aunt established a dawn gardening habit\", # Working alongside her garden-helping rabbit.\n",
    "    \"The student formed a late-night studying habit\", # Taking breaks to play with her energetic rabbit.\n",
    "    \"The baker kept an experimental baking habit\", # Creating carrot treats for her customer's rabbit.\n",
    "    \"The filmmaker maintained a storytelling habit\", # Often featuring adventures of a heroic rabbit.\n",
    "    \"The musician developed a curious practice habit\", # Playing sonatas that soothed her nervous rabbit.\n",
    "    \"The naturalist continued her tracking habit\", # Documenting the passage of each wild rabbit.\n",
    "    \"My father couldn't break his early waking habit\", # Always finding time to feed the backyard rabbit.\n",
    "    \"The magician perfected his hat-pulling habit\", # Surprising audiences with an appearing rabbit.\n",
    "    \"The engineer maintained her inventing habit\", # Creating gadgets to entertain her bored rabbit.\n",
    "    \"The florist developed an arrangement habit\", # Including carrot tops to please her shop's rabbit.\n",
    "    \"The therapist kept her gentle listening habit\", # Showing patience that matched her office rabbit.\n",
    "]\n",
    "\n",
    "lines_that_rhyme_with_habit = [\n",
    "    \"When I found a small, trembling rabbit\", # Caring for animals became my habit.\n",
    "    \"She darted through the garden like a rabbit\", # Looking for treats had become her habit.\n",
    "    \"He claimed he could pull a hat from a rabbit\", # Showing off magic tricks was his daily habit.\n",
    "    \"The children giggled as they chased the rabbit\", # Running through meadows became their favorite habit.\n",
    "    \"I planted carrots to attract a rabbit\", # Gardening in spring is my cherished habit.\n",
    "    \"My thoughts multiply faster than a rabbit\", # Overthinking has become my worst habit.\n",
    "    \"The speedy win went to the tortoise, not the rabbit\", # Victory comes from persistence, not just habit.\n",
    "    \"In the moonlight hopped a silver rabbit\", # Stargazing at night is now my habit.\n",
    "    \"They built a cozy hutch for their new rabbit\", # Creating homes for pets is a wonderful habit.\n",
    "    \"The chef prepared a savory stew with rabbit\", # Cooking wild game had become his habit.\n",
    "    \"Through tall grass I spotted a cottontail rabbit\", # Hiking through fields is my weekend habit.\n",
    "    \"The magician waved his wand and vanished the rabbit\", # Astonishing crowds had become his habit.\n",
    "    \"I sketched the ears and whiskers of a rabbit\", # Drawing animals is my creative habit.\n",
    "    \"The farmer chased away the vegetable-stealing rabbit\", # Protecting his crops was a necessary habit.\n",
    "    \"At dawn the fox was hunting for a rabbit\", # Early rising became his daily habit.\n",
    "    \"In the story, Peter was a mischievous rabbit\", # Reading fables became our bedtime habit.\n",
    "    \"Her fear made her timid just like a rabbit\", # Avoiding confrontation was her lifelong habit.\n",
    "    \"The child's stuffed toy was a velveteen rabbit\", # Carrying comfort objects was her childhood habit.\n",
    "    \"The dog barked loudly at the wild rabbit\", # Alert guarding is his protective habit.\n",
    "    \"The hunter set a snare to catch a rabbit\", # Living off the land was his family habit.\n",
    "    \"The camera captured a leaping snow-white rabbit\", # Photography in winter is my seasonal habit.\n",
    "    \"A clever fox can easily outfox a rabbit\", # Strategic thinking is my professional habit.\n",
    "    \"The full moon illuminated the jackrabbit\", # Evening walks became our romantic habit.\n",
    "    \"Under the bush was hiding a frightened rabbit\", # Finding secret spaces was her peculiar habit.\n",
    "    \"Into his hat disappeared the magical rabbit\", # Performing illusions was his lucrative habit.\n",
    "    \"My daughter begged for a pet dwarf rabbit\", # Collecting small animals became her expensive habit.\n",
    "    \"The naturalist observed the rare desert rabbit\", # Scientific inquiry was her passionate habit.\n",
    "    \"Tales of Brer Fox always included a rabbit\", # Telling folk stories was grandfather's evening habit.\n",
    "    \"She embroidered the silhouette of a rabbit\", # Creating handcrafted gifts was her generous habit.\n",
    "    \"Through the forest hopped a nimble rabbit\", # Morning exercises became his energizing habit.\n",
    "    \"We watched with awe the jumping jackrabbit\", # Desert exploration became our vacation habit.\n",
    "    \"The painting depicted a wild mountain rabbit\", # Collecting wildlife art was his expensive habit.\n",
    "    \"In the field I photographed a rare pygmy rabbit\", # Documenting endangered species is my conservation habit.\n",
    "    \"The child's first pet was a Dutch lop rabbit\", # Learning responsibility became her formative habit.\n",
    "    \"On Easter morning appeared a chocolate rabbit\", # Holiday traditions became our family habit.\n",
    "    \"The scientist studied the behavior of the arctic rabbit\", # Meticulous observation was her scientific habit.\n",
    "    \"The birthday gift was an Angora rabbit\", # Surprising loved ones is my thoughtful habit.\n",
    "    \"Never try to outrun a frightened rabbit\", # Setting realistic goals is my productive habit.\n",
    "    \"Into the brush disappeared the elusive rabbit\", # Playing hide-and-seek was their childhood habit.\n",
    "    \"The young boy dreamed of owning a rabbit\", # Wishful thinking became his daydreaming habit.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621eda32-8683-46fe-a89e-df9748f51d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_that_rhyme_with_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2fd51a-a099-40b8-aac8-f6d2b847bda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_that_rhyme_with_pain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d9e79d-2df4-4789-aa1f-e643807a3f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-2-9b-it\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d762654e12f47c48b9f14d510312369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# ## 3. Load Model and Tokenizer\n",
    "\n",
    "# +\n",
    "# Configure quantization if needed\n",
    "quantization_config = None\n",
    "if USE_4BIT_QUANTIZATION:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Recommended for new models\n",
    "    )\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "\n",
    "# Determine device and dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32 # BF16 recommended on Ampere+\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not present\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically distribute across GPUs if available\n",
    "    # use_auth_token=YOUR_HF_TOKEN, # Add if model requires authentication\n",
    "    trust_remote_code=True # Gemma requires this for some versions/variants\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device(s): {model.hf_device_map}\")\n",
    "\n",
    "# --- IMPORTANT: Finding the Layer Name ---\n",
    "# Uncomment the following line to print the model structure and find the exact layer name\n",
    "# print(model)\n",
    "# Look for layers like 'model.layers[INDEX].mlp...' or 'model.layers[INDEX].self_attn...'\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fa1d86-d218-4a5c-811d-a1ef55f1db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newline token: '\n",
      "', ID: 108\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Hooking and Activation Handling Functions\n",
    "\n",
    "# +\n",
    "# Global storage for captured activations\n",
    "activation_storage = {}\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Helper function to get a module object from its name string.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "\n",
    "def capture_activation_hook(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "def capture_activation_hook_fast(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "\n",
    "def get_activations_fast(model, tokenizer, prompts: List[str], layer_name: str) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model and captures activations from the target layer.\n",
    "    Returns the averaged activation across all prompts for the last token position.\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {} # Clear previous activations\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        lambda module, input, output: capture_activation_hook_fast(module, input, output, layer_name)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        # We only need the forward pass, not generation here\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        if layer_name in activation_storage:\n",
    "                # Assuming batch size is 1 when processing one prompt at a time\n",
    "            last_token_activations = activation_storage[layer_name] # Shape (num_prompts, hidden_dim)\n",
    "            del activation_storage[layer_name] # Clear for next prompt\n",
    "        else:\n",
    "            print(f\"Warning: Activation for layer {layer_name} not captured for prompts: '{prompts}'\")\n",
    "                \n",
    "    hook_handle.remove() # Clean up the hook\n",
    "\n",
    "    # Stack and average activations across all prompts\n",
    "    # Resulting shape: (num_prompts, hidden_dim) -> (hidden_dim)\n",
    "    avg_activation = last_token_activations.mean(dim=0).squeeze() # Average over the prompt dimension\n",
    "    # print(f\"Calculated average activation for layer '{layer_name}' with shape: {avg_activation.shape}\")\n",
    "    return avg_activation\n",
    "# %%\n",
    " # --- Steering Hook during Generation ---\n",
    "\n",
    "# Global variable to hold the steering vector during generation\n",
    "steering_vector_internal = None\n",
    "steering_multiplier_internal = 1.0\n",
    "all_positions=False\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal, all_positions\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            if output.shape[1] != 1:\n",
    "                output[:, -1, :] += (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "            # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0]\n",
    "            # print(modified_tensor.shape)\n",
    "            if modified_tensor.shape[1] != 1  or all_positions:\n",
    "                modified_tensor[:, -1, :] += (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            modified_output = output + (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return modified_output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "             # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0] + (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal, all_positions\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            if output.shape[1] != 1:\n",
    "                output[:, -1, :] += (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "            # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0]\n",
    "            # print(modified_tensor.shape)\n",
    "            if modified_tensor.shape[1] != 1  or all_positions:\n",
    "                modified_tensor[:, -1, :] += (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "newline_tokens = tokenizer.tokenize('\\n')\n",
    "if len(newline_tokens) == 1:\n",
    "    newline_token_id = tokenizer.convert_tokens_to_ids(newline_tokens[0])\n",
    "    print(f\"Newline token: '{newline_tokens[0]}', ID: {newline_token_id}\")\n",
    "elif len(newline_tokens) > 1:\n",
    "    # This is less common but possible if '\\n' is split, e.g. into 'Ġ' and 'n'\n",
    "    # The current approach might only target the first token in the split.\n",
    "    # Handling multi-token sequences requires more complex mask generation.\n",
    "    print(f\"Warning: Newline ('\\\\n') tokenizes into multiple tokens: {newline_tokens}. Steering will target the first token ID: {tokenizer.convert_tokens_to_ids(newline_tokens[0])}\")\n",
    "    newline_token_id = tokenizer.convert_tokens_to_ids(newline_tokens[0])\n",
    "else:\n",
    "    # Should not happen for '\\n'\n",
    "    print(\"Error: Could not find token ID for newline '\\\\n'.\")\n",
    "    newline_token_id = -1 # Indicate error or use a placeholder\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            modified_output = output + (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return modified_output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "             # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0] + (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "@contextmanager\n",
    "def apply_steering(model, layer, steering_vector, multiplier):\n",
    "    \"\"\"Context manager to temporarily apply the steering hook.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    layer_name = f\"model.layers.{layer}\"\n",
    "\n",
    "    # Ensure previous hook (if any) on the same layer is removed\n",
    "    # This basic implementation assumes only one steering hook at a time on this layer\n",
    "    # More robust solutions might track handles explicitly.\n",
    "    \n",
    "    handle = None\n",
    "    try:\n",
    "        steering_vector_internal = steering_vector\n",
    "        steering_multiplier_internal = multiplier\n",
    "        target_module = get_module_by_name(model, layer_name)\n",
    "        handle = target_module.register_forward_hook(steering_hook)\n",
    "        # print(f\"Steering hook applied to {layer_name} with multiplier {multiplier}\")\n",
    "        yield # Generation happens here\n",
    "    finally:\n",
    "        if handle:\n",
    "            handle.remove()\n",
    "        steering_vector_internal = None # Clear global state\n",
    "        steering_multiplier_internal = 1.0\n",
    "        # print(f\"Steering hook removed from {layer_name}\")\n",
    "        gc.collect() # Suggest garbage collection\n",
    "        torch.cuda.empty_cache() # Clear cache if using GPU\n",
    "\n",
    "def generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    inputs = tokenizer([generation_prompt] * batch_size, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    if steering_vector is None:\n",
    "        # print(inputs.input_ids.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                pad_token_id=tokenizer.eos_token_id # Important for generation\n",
    "            )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            # Apply the steering hook using the context manager\n",
    "            with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                outputs = model.generate(\n",
    "                    **inputs, # Use the same input tokens\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "    text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    del outputs, inputs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_steered_output(steering_vector, model, tokenizer, generation_prompts, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    # Ensure generation_prompts is a list\n",
    "    if isinstance(generation_prompts, str):\n",
    "        generation_prompts = [generation_prompts] \n",
    "    generation_prompts = generation_prompts * batch_size\n",
    "    \n",
    "    # Process in batches of 500\n",
    "    MAX_BATCH_SIZE = 500\n",
    "    all_texts = []\n",
    "    \n",
    "    for i in range(0, len(generation_prompts), MAX_BATCH_SIZE):\n",
    "        #batch_prompts = generation_prompts[i:i + MAX_BATCH_SIZE]\n",
    "        # Get batch of prompts, handling case where remaining prompts < MAX_BATCH_SIZE\n",
    "        current_batch_size = min(MAX_BATCH_SIZE, len(generation_prompts) - i)\n",
    "        batch_prompts = generation_prompts[i:i + current_batch_size]\n",
    "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        \n",
    "        if steering_vector is None:\n",
    "            print(inputs.input_ids.shape)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    pad_token_id=tokenizer.eos_token_id # Important for generation\n",
    "                )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Apply the steering hook using the context manager\n",
    "                with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                    outputs = model.generate(\n",
    "                        **inputs, # Use the same input tokens\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        temperature=TEMPERATURE,\n",
    "                        do_sample=DO_SAMPLE,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "        \n",
    "        batch_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_texts.extend(batch_texts)\n",
    "\n",
    "        del outputs, inputs\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "def generate_outputs(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    assert steering_vector is not None\n",
    "    text_baseline = generate_steered_output(None, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_steered = generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_negsteered = generate_steered_output(-steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    return text_baseline, text_steered, text_negsteered\n",
    "\n",
    "# %%\n",
    "# ## Compute the Steering Vector\n",
    "def get_steering_vector_fast(model, tokenizer, positive_prompts, negative_prompts, layer=20):\n",
    "    target_layer_name = f\"model.layers.{layer}\"\n",
    "    # print(\"Calculating activations for POSITIVE prompts...\")\n",
    "    avg_pos_activation = get_activations_fast(model, tokenizer, positive_prompts, target_layer_name)\n",
    "\n",
    "    # print(\"\\nCalculating activations for NEGATIVE prompts...\")\n",
    "    avg_neg_activation = get_activations_fast(model, tokenizer, negative_prompts, target_layer_name)\n",
    "\n",
    "    steering_vector = None\n",
    "    if avg_pos_activation is not None and avg_neg_activation is not None:\n",
    "        steering_vector = avg_pos_activation - avg_neg_activation\n",
    "        # print(f\"\\nSteering vector computed successfully. Shape: {steering_vector.shape}\")\n",
    "        # Optional: Normalize the steering vector (can sometimes help)\n",
    "        # steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "        # print(\"Steering vector normalized.\")\n",
    "    else:\n",
    "        print(\"\\nError: Could not compute steering vector due to missing activations.\")\n",
    "    del avg_pos_activation\n",
    "    del avg_neg_activation\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return steering_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8c4abb-36f5-4e5f-b8a8-7726c2bb3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Hooking and Activation Handling Functions\n",
    "\n",
    "# +\n",
    "# Global storage for captured activations\n",
    "activation_storage = {}\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Helper function to get a module object from its name string.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "\n",
    "def capture_activation_hook(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "def capture_activation_hook_fast(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "\n",
    "def get_activations_fast(model, tokenizer, prompts: List[str], layer_name: str) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model and captures activations from the target layer.\n",
    "    Returns the averaged activation across all prompts for the last token position.\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {} # Clear previous activations\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        lambda module, input, output: capture_activation_hook_fast(module, input, output, layer_name)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        # We only need the forward pass, not generation here\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        if layer_name in activation_storage:\n",
    "                # Assuming batch size is 1 when processing one prompt at a time\n",
    "            last_token_activations = activation_storage[layer_name] # Shape (num_prompts, hidden_dim)\n",
    "            del activation_storage[layer_name] # Clear for next prompt\n",
    "        else:\n",
    "            print(f\"Warning: Activation for layer {layer_name} not captured for prompts: '{prompts}'\")\n",
    "                \n",
    "    hook_handle.remove() # Clean up the hook\n",
    "\n",
    "    # Stack and average activations across all prompts\n",
    "    # Resulting shape: (num_prompts, hidden_dim) -> (hidden_dim)\n",
    "    avg_activation = last_token_activations.mean(dim=0).squeeze() # Average over the prompt dimension\n",
    "    # print(f\"Calculated average activation for layer '{layer_name}' with shape: {avg_activation.shape}\")\n",
    "    return avg_activation\n",
    "# %%\n",
    " # --- Steering Hook during Generation ---\n",
    "\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer # For type hinting\n",
    "from typing import List, Optional, Dict, Any, Union # Added Union\n",
    "import tqdm # Using tqdm directly\n",
    "\n",
    "# --- Assume these are defined globally or passed appropriately ---\n",
    "# MAX_NEW_TOKENS = 60\n",
    "# TEMPERATURE = 0.7\n",
    "# DO_SAMPLE = True\n",
    "# STEERING_MULTIPLIER = 1.5 # Example default - Make sure this is defined or passed\n",
    "# tokenizer: PreTrainedTokenizer = None # Must be initialized\n",
    "# model: PreTrainedModel = None # Must be initialized\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# --- Globals for Steering ---\n",
    "steering_vector_internal: Optional[torch.Tensor] = None\n",
    "steering_multiplier_internal: float = 1.0\n",
    "initial_input_ids_global: Optional[torch.Tensor] = None # Stores prompt IDs for the hook\n",
    "newline_token_id_global: Optional[int] = None # Stores the newline token ID\n",
    "\n",
    "# --- Find Newline Token ID (Run Once after loading tokenizer) ---\n",
    "def find_newline_token_id(tokenizer: PreTrainedTokenizer) -> Optional[int]:\n",
    "    \"\"\"Finds the token ID for the newline character and sets the global.\"\"\"\n",
    "    global newline_token_id_global\n",
    "    newline_tokens = tokenizer.tokenize('\\n')\n",
    "    _newline_token_id = -1 # Default to error state\n",
    "    if len(newline_tokens) == 1:\n",
    "        _newline_token_id = tokenizer.convert_tokens_to_ids(newline_tokens[0])\n",
    "        print(f\"Newline token: '{newline_tokens[0]}', ID: {_newline_token_id}\")\n",
    "    elif len(newline_tokens) > 1:\n",
    "        _newline_token_id = tokenizer.convert_tokens_to_ids(newline_tokens[0])\n",
    "        print(f\"Warning: Newline ('\\\\n') tokenizes into multiple tokens: {newline_tokens}. Steering will target the first token ID: {_newline_token_id}\")\n",
    "    else:\n",
    "        print(\"Error: Could not find token ID for newline '\\\\n'.\")\n",
    "        newline_token_id_global = None # Ensure global reflects failure\n",
    "        return None\n",
    "\n",
    "    newline_token_id_global = _newline_token_id # Set the global\n",
    "    return _newline_token_id\n",
    "\n",
    "# --- Helper to find module by name (Assume exists) ---\n",
    "def get_module_by_name(model, name):\n",
    "    \"\"\"Gets a module from model using its dotted name.\"\"\"\n",
    "    for n in name.split(\".\"):\n",
    "        try:\n",
    "             model = getattr(model, n)\n",
    "        except AttributeError:\n",
    "             print(f\"Error: Module part '{n}' not found in '{name}'\")\n",
    "             return None\n",
    "    return model\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- Modified Steering Hook ---\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"\n",
    "    Hook function to modify activations, applying steering *only* at the position\n",
    "    of the second newline token. Handles internal batch expansion during generation.\n",
    "    \"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    global initial_input_ids_global, newline_token_id_global\n",
    "\n",
    "    if (steering_vector_internal is not None and\n",
    "        initial_input_ids_global is not None and\n",
    "        newline_token_id_global is not None and\n",
    "        newline_token_id_global != -1):\n",
    "\n",
    "        output_tensor = None\n",
    "        original_output_structure = output\n",
    "\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            output_tensor = output\n",
    "        elif isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], torch.Tensor):\n",
    "            output_tensor = output[0]\n",
    "        else:\n",
    "            return output # Cannot steer\n",
    "\n",
    "        output_batch_size, seq_len, hidden_dim = output_tensor.shape\n",
    "        device = output_tensor.device\n",
    "        dtype = output_tensor.dtype\n",
    "        initial_input_batch_size = initial_input_ids_global.shape[0]\n",
    "\n",
    "        # --- Handle Batch Size Mismatch due to num_return_sequences/beam search ---\n",
    "        effective_input_ids = initial_input_ids_global\n",
    "        if output_batch_size != initial_input_batch_size:\n",
    "            # Check if output batch size is a multiple of input batch size\n",
    "            # This often happens with num_return_sequences or beam search\n",
    "            if output_batch_size % initial_input_batch_size == 0:\n",
    "                num_repeats = output_batch_size // initial_input_batch_size\n",
    "                # print(f\"Hook Info: Output batch ({output_batch_size}) != Input batch ({initial_input_batch_size}). Repeating input IDs {num_repeats} times.\")\n",
    "                # Repeat each input ID sequence 'num_repeats' times\n",
    "                effective_input_ids = initial_input_ids_global.repeat_interleave(num_repeats, dim=0)\n",
    "            else:\n",
    "                # If not a clean multiple, we cannot reliably map inputs to outputs\n",
    "                print(f\"Warning: Output batch size ({output_batch_size}) is not a multiple of hook input batch size ({initial_input_batch_size}). Skipping steering for this pass.\")\n",
    "                return original_output_structure\n",
    "\n",
    "        # Now, effective_input_ids should have batch size matching output_batch_size\n",
    "        if effective_input_ids.shape[0] != output_batch_size:\n",
    "             # This check should ideally not fail after the repeat logic\n",
    "             print(f\"Error: Effective input IDs batch size ({effective_input_ids.shape[0]}) still doesn't match output ({output_batch_size}). Skipping steering.\")\n",
    "             return original_output_structure\n",
    "\n",
    "        # --- Calculate Mask based on potentially repeated input IDs ---\n",
    "        mask = torch.zeros((output_batch_size, seq_len), device=device, dtype=dtype)\n",
    "\n",
    "        for b in range(output_batch_size): # Iterate through the potentially expanded batch\n",
    "            # Limit sequence length check to the minimum of input and output lengths\n",
    "            check_len = min(seq_len, effective_input_ids.shape[1])\n",
    "            prompt_ids = effective_input_ids[b, :check_len] # Use the potentially repeated IDs\n",
    "\n",
    "            newline_indices = torch.where(prompt_ids == newline_token_id_global)[0]\n",
    "\n",
    "            if len(newline_indices) >= 2:\n",
    "                second_newline_idx = newline_indices[1].item()\n",
    "                if second_newline_idx < seq_len: # Check against output seq len\n",
    "                    mask[b, second_newline_idx] = 1.0\n",
    "\n",
    "        # --- Apply masked steering ---\n",
    "        steering_addition = (steering_vector_internal.to(device, dtype=dtype) *\n",
    "                             steering_multiplier_internal)\n",
    "        mask_expanded = mask.unsqueeze(-1)\n",
    "        modified_tensor = output_tensor + (steering_addition * mask_expanded)\n",
    "\n",
    "        # Return modified output in the original structure\n",
    "        if isinstance(original_output_structure, torch.Tensor):\n",
    "            return modified_tensor\n",
    "        elif isinstance(original_output_structure, tuple):\n",
    "            return (modified_tensor,) + original_output_structure[1:]\n",
    "        else: return original_output_structure # Should not be reached\n",
    "\n",
    "    return output # Return original if steering not active\n",
    "\n",
    "\n",
    "# --- apply_steering (Signature unchanged) ---\n",
    "@contextmanager\n",
    "def apply_steering(model, layer, steering_vector, multiplier):\n",
    "    \"\"\"\n",
    "    Context manager to temporarily apply the steering hook.\n",
    "    Signature kept identical to user's original code.\n",
    "    Relies on globals for hook configuration beyond vector/multiplier.\n",
    "    \"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    # Does NOT manage initial_input_ids_global or newline_token_id_global here\n",
    "\n",
    "    layer_name = f\"model.layers.{layer}\"\n",
    "    handle = None\n",
    "    target_module = None\n",
    "\n",
    "    if newline_token_id_global is None:\n",
    "         print(\"Warning: apply_steering called but newline_token_id_global is not set. Hook might not work correctly.\")\n",
    "\n",
    "    try:\n",
    "        target_module = get_module_by_name(model, layer_name)\n",
    "        if target_module is None:\n",
    "            raise ValueError(f\"Layer {layer_name} not found in model.\")\n",
    "\n",
    "        # Set ONLY the globals managed by this context manager\n",
    "        steering_vector_internal = steering_vector\n",
    "        steering_multiplier_internal = multiplier\n",
    "\n",
    "        handle = target_module.register_forward_hook(steering_hook)\n",
    "\n",
    "        yield # Generation happens here\n",
    "\n",
    "    finally:\n",
    "        # Clean up hook and ONLY the globals set by this context manager\n",
    "        if handle:\n",
    "            handle.remove()\n",
    "        steering_vector_internal = None\n",
    "        steering_multiplier_internal = 1.0\n",
    "\n",
    "        # DO NOT clear initial_input_ids_global here, it's managed by the caller\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- generate_steered_output (Signature unchanged, logic adapted) ---\n",
    "def generate_steered_output(\n",
    "    steering_vector: Optional[torch.Tensor],\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    generation_prompts: Union[str, List[str]], # Keep original flexibility\n",
    "    batch_size: int, # Interpreting this as num_return_sequences per prompt\n",
    "    layer: int = 20,\n",
    "    steering_multiplier: float = 1.5, # Example default\n",
    "    # Add other generation params if needed\n",
    "    max_new_tokens = 60,\n",
    "    temperature = 0.7,\n",
    "    do_sample = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generates text outputs, applying steering only at the second newline\n",
    "    of the input prompts if steering_vector is provided.\n",
    "    Keeps the original function signature. Manages input_ids global.\n",
    "\n",
    "    Args:\n",
    "        steering_vector: The steering vector tensor, or None for no steering.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer.\n",
    "        generation_prompts: A *single* prompt string or a list of *unique* prompt strings.\n",
    "        batch_size: Number of outputs to generate for EACH prompt (num_return_sequences).\n",
    "        layer: Layer index for steering.\n",
    "        steering_multiplier: Multiplier for the steering vector.\n",
    "\n",
    "    Returns:\n",
    "        A list of generated text strings.\n",
    "    \"\"\"\n",
    "    global initial_input_ids_global # Declare we modify the global\n",
    "\n",
    "    if isinstance(generation_prompts, str):\n",
    "        unique_prompts = [generation_prompts]\n",
    "    else:\n",
    "        unique_prompts = generation_prompts\n",
    "\n",
    "    num_return_sequences = batch_size # Use original batch_size arg as num_return_sequences\n",
    "    all_texts = []\n",
    "    total_unique_prompts = len(unique_prompts)\n",
    "    INTERNAL_MODEL_BATCH_SIZE = 500 # Process this many unique prompts per tokenizer/model call\n",
    "\n",
    "    if steering_vector is not None and newline_token_id_global is None:\n",
    "         print(\"Error: Newline token ID not set. Call find_newline_token_id(tokenizer) first.\")\n",
    "         find_newline_token_id(tokenizer) # Attempt to set it\n",
    "         if newline_token_id_global is None:\n",
    "              raise ValueError(\"Cannot proceed without newline token ID for steering.\")\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, total_unique_prompts, INTERNAL_MODEL_BATCH_SIZE), desc=\"Generating Batches\"):\n",
    "        batch_prompts = unique_prompts[i : i + INTERNAL_MODEL_BATCH_SIZE]\n",
    "        if not batch_prompts: continue\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "             max_new_tokens=max_new_tokens, temperature=temperature, do_sample=do_sample,\n",
    "             pad_token_id=tokenizer.pad_token_id, num_return_sequences=num_return_sequences\n",
    "        )\n",
    "\n",
    "        outputs = None\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                if steering_vector is None:\n",
    "                    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "                else:\n",
    "                    # Set global input IDs BEFORE apply_steering\n",
    "                    initial_input_ids_global = inputs['input_ids']\n",
    "                    # apply_steering signature unchanged\n",
    "                    with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        finally:\n",
    "            # Clear global input IDs AFTER apply_steering finishes\n",
    "            initial_input_ids_global = None\n",
    "\n",
    "        if outputs is not None:\n",
    "            batch_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            all_texts.extend(batch_texts)\n",
    "\n",
    "        del outputs, inputs, batch_texts\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_outputs(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    assert steering_vector is not None\n",
    "    text_baseline = generate_steered_output(None, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_steered = generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_negsteered = generate_steered_output(-steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    return text_baseline, text_steered, text_negsteered\n",
    "\n",
    "# %%\n",
    "# ## Compute the Steering Vector\n",
    "def get_steering_vector_fast(model, tokenizer, positive_prompts, negative_prompts, layer=20):\n",
    "    target_layer_name = f\"model.layers.{layer}\"\n",
    "    # print(\"Calculating activations for POSITIVE prompts...\")\n",
    "    avg_pos_activation = get_activations_fast(model, tokenizer, positive_prompts, target_layer_name)\n",
    "\n",
    "    # print(\"\\nCalculating activations for NEGATIVE prompts...\")\n",
    "    avg_neg_activation = get_activations_fast(model, tokenizer, negative_prompts, target_layer_name)\n",
    "\n",
    "    steering_vector = None\n",
    "    if avg_pos_activation is not None and avg_neg_activation is not None:\n",
    "        steering_vector = avg_pos_activation - avg_neg_activation\n",
    "        # print(f\"\\nSteering vector computed successfully. Shape: {steering_vector.shape}\")\n",
    "        # Optional: Normalize the steering vector (can sometimes help)\n",
    "        # steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "        # print(\"Steering vector normalized.\")\n",
    "    else:\n",
    "        print(\"\\nError: Could not compute steering vector due to missing activations.\")\n",
    "    del avg_pos_activation\n",
    "    del avg_neg_activation\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return steering_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca255d0c-59a1-4a2a-8820-a55077421799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ## Functions to analyze the generations\n",
    "def get_last_word(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    if len(lines) < 3:\n",
    "        print(f\"Failed to get last word: {text}\")\n",
    "        return \"\"\n",
    "    second_line = lines[2]\n",
    "    second_line_words = second_line.split(\" \")\n",
    "    if len(second_line_words) == 0:\n",
    "        print(f\"Failed to get last word: {text}\")\n",
    "        return \"\"\n",
    "    last_word = second_line_words[-1]\n",
    "    if last_word == \"\":\n",
    "        if len(second_line_words) == 1:\n",
    "            print(f\"Failed to get last word: {text}\")\n",
    "            return \"\"\n",
    "        last_word = second_line_words[-2]\n",
    "    return last_word\n",
    "\n",
    "def get_second_line(text,include_prompt=True,try_third=False):\n",
    "    lines = text.split(\"\\n\")\n",
    "    if len(lines) < 3:\n",
    "        print(f\"Failed to get second line: {text}\")\n",
    "        return text\n",
    "    second_line = lines[2]\n",
    "    if second_line=='' and try_third: second_line = lines[3]\n",
    "    second_line=second_line.strip(' ')\n",
    "    second_line_words = second_line.split(\" \")\n",
    "    if len(second_line_words) > 0:\n",
    "        second_line=' '.join(second_line_words[:-1])\n",
    "    if include_prompt: second_line='\\n'.join(lines[:2]+[second_line])\n",
    "    return second_line\n",
    "\n",
    "def get_last_word_fraction(texts, words):\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "    last_words = [get_last_word(line) for line in texts]\n",
    "    return len([w for w in last_words if any(w2.lower() in w.lower() for w2 in words)]) / len(last_words)\n",
    "\n",
    "def get_prompts(lines):\n",
    "    return [f'A rhymed couplet:\\n{line}\\n' for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e970d8-0567-419c-94bc-c58fc2834f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1094, 0.6250, 1.0312,  ..., 0.2266, 0.1875, 0.4473],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "steering_vector = torch.load('steering_vector_from_quick_to_pain.pt')\n",
    "print(steering_vector )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308117da-99ed-410f-b572-e857b6425d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=20, steering_multiplier=STEERING_MULTIPLIER):\n",
    "    assert steering_vector is not None\n",
    "    text_baseline = generate_steered_output(None, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    text_steered = generate_steered_output(steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    #text_negsteered = generate_steered_output(-steering_vector, model, tokenizer, generation_prompt, batch_size, layer=layer, steering_multiplier=steering_multiplier)\n",
    "    return text_baseline, text_steered#, text_negsteered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbeb9089-1377-41e1-9be0-a78efc584b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_PROMPT=f'A rhymed couplet:\\n{lines_that_rhyme_with_pain[1]}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5983e788-d4d5-40aa-8d61-c1eaec3f9121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A rhymed couplet:\\nThe wizard's knowledge was profoundly arcane\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERATION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b347a5-7c2c-4c4f-be2f-c36ee00f1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6016,  2.9062, -0.4375,  ..., -0.5156,  0.3516, -1.1406],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "steering_vector = torch.load('steering_vectors_quick_to_pain/steering_vector_quick_to_pain_layer_27.pt')\n",
    "print(steering_vector )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81b8c3d-3988-4d8f-8b60-96868d159919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches:   0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Generating Batches: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Newline token ID not set. Call find_newline_token_id(tokenizer) first.\n",
      "Newline token: '\n",
      "', ID: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "examplegen=generate_output(steering_vector, model, tokenizer, GENERATION_PROMPT, 2, layer=27, steering_multiplier=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ba9348-bed3-40e6-8987-47477496c875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"A rhymed couplet:\\nThe wizard's knowledge was profoundly arcane\\nAnd with a flick of his wand, he'd rearrange.\\n\\n\\nLet me know if you want to explore more rhyming couplets! I'm happy to experiment with different themes and styles.\\n\",\n",
       "  \"A rhymed couplet:\\nThe wizard's knowledge was profoundly arcane\\nA secret whispered in a hidden, mystic lane. \\n\\n\\nLet me know if you'd like some more!  I can write about a variety of topics.  \\n\\n\"],\n",
       " [\"A rhymed couplet:\\nThe wizard's knowledge was profoundly arcane\\nAnd spells he wove were truly quite unique. \\n\\n\\nLet me know if you'd like more!\\n\",\n",
       "  \"A rhymed couplet:\\nThe wizard's knowledge was profoundly arcane\\nHe wielded power with a flick and a click. \\n\\n\\nLet me know if you'd like more couplets! \\n\"])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bd60e1-9819-4074-9947-565a3779daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "toanalyze=get_second_line(examplegen[0][0],include_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f76f0ca-a309-4026-9ea5-3b3166d512c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b2a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"line_catalog.json\",'r') as f:\n",
    "    line_catalog=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "782ae3c5-17e4-4cec-9881-9b5eb3968ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get next token probabilities for all positions in a single pass\n",
    "def get_all_next_token_probs(input_text, steering_multiplier, layer, steering_vector):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "            # Set output_hidden_states=True to access all hidden states\n",
    "            outputs = model(input_ids, output_hidden_states=True)\n",
    "            \n",
    "        # Get logits for all positions\n",
    "        logits = outputs.logits[:, :, :]  \n",
    "        # Convert to probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert to float32 before converting to numpy\n",
    "        probs_float = probs.to(torch.float32)\n",
    "        \n",
    "        # Get all probabilities at once\n",
    "        all_probs = probs_float.cpu().numpy()[0]  \n",
    "        \n",
    "    return all_probs\n",
    "\n",
    "def get_all_next_token_probs(\n",
    "    input_text: str,\n",
    "    steering_multiplier: float,\n",
    "    layer: int, # Layer index for steering\n",
    "    steering_vector: Optional[torch.Tensor], # Steering vector (can be None)\n",
    "    # Implicitly uses global model and tokenizer\n",
    "    ) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates next-token probabilities for a single input text, compatible\n",
    "    with the position-specific steering mechanism using global variables.\n",
    "\n",
    "    Args:\n",
    "        input_text: The input text string.\n",
    "        steering_multiplier: The multiplier for the steering vector.\n",
    "        layer: Identifier for the layer to apply steering.\n",
    "        steering_vector: The steering vector tensor, or None for no steering.\n",
    "        model: The language model instance.\n",
    "        tokenizer: The tokenizer instance.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (sequence_length, vocab_size)\n",
    "        containing probabilities (float32), or None if an error occurs.\n",
    "    \"\"\"\n",
    "    global initial_input_ids_global # Declare intention to modify global\n",
    "\n",
    "    if not input_text:\n",
    "        print(\"Warning: input_text is empty.\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Check prerequisite for steering\n",
    "    if steering_vector is not None and newline_token_id_global is None:\n",
    "        print(\"Error: Steering vector provided but newline_token_id_global not set.\")\n",
    "        print(\"Call find_newline_token_id(tokenizer) first.\")\n",
    "        # Optionally attempt to set it here if needed, but it's better practice externally\n",
    "        # find_newline_token_id(tokenizer)\n",
    "        # if newline_token_id_global is None:\n",
    "        #     return None # Cannot proceed\n",
    "        return None # Indicate failure\n",
    "\n",
    "\n",
    "    outputs = None\n",
    "    all_probs_np = None\n",
    "\n",
    "    try:\n",
    "        # Tokenize the input text (no padding needed for single string)\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        input_ids = inputs[\"input_ids\"] # Shape: (1, sequence_length)\n",
    "\n",
    "        # --- Set the global input IDs BEFORE the model call ---\n",
    "        initial_input_ids_global = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Conditionally apply steering context manager\n",
    "            if steering_vector is not None and steering_multiplier != 0:\n",
    "                 # Use apply_steering (signature unchanged)\n",
    "                 with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                     # Model call *inside* context - hook will use global input_ids\n",
    "                     # Set output_hidden_states=False as we only need logits here\n",
    "                     outputs = model(input_ids=input_ids, output_hidden_states=False)\n",
    "            else:\n",
    "                 # No steering, direct model call\n",
    "                 outputs = model(input_ids=input_ids, output_hidden_states=False)\n",
    "\n",
    "            # Check if model call was successful\n",
    "            if outputs is None or not hasattr(outputs, 'logits'):\n",
    "                 print(\"Error: Model did not return expected outputs.\")\n",
    "                 return None\n",
    "\n",
    "            # Get logits: shape (1, sequence_length, vocab_size)\n",
    "            logits = outputs.logits\n",
    "            # Convert to probabilities using softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # Convert to float32 before moving to CPU and NumPy\n",
    "            probs_float32 = probs.to(torch.float32)\n",
    "\n",
    "            # Move to CPU, convert to numpy, and remove the batch dimension (index 0)\n",
    "            all_probs_np = probs_float32.cpu().numpy()[0] # Shape: (sequence_length, vocab_size)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during probability calculation for text '{input_text[:50]}...': {e}\")\n",
    "        all_probs_np = None # Ensure None is returned on error\n",
    "    finally:\n",
    "        # --- IMPORTANT: Clear the global input IDs ---\n",
    "        initial_input_ids_global = None\n",
    "        # Optional cleanup\n",
    "        del outputs, inputs\n",
    "        if 'logits' in locals(): del logits\n",
    "        if 'probs' in locals(): del probs\n",
    "        if 'probs_float32' in locals(): del probs_float32\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_probs_np\n",
    "\n",
    "# Function to calculate KL divergence between two probability distributions\n",
    "def calculate_kl_divergence(p, q):\n",
    "    # Ensure no zeros that could cause division by zero in KL calculation\n",
    "    p = np.maximum(p, 1e-10)\n",
    "    q = np.maximum(q, 1e-10)\n",
    "    \n",
    "    # Normalize to ensure they sum to 1\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    \n",
    "    # Calculate KL(p||q)\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "# Main function to run the analysis\n",
    "def analyze_steering_impact(input_text, steering_vector, layer_idx=20):\n",
    "    \n",
    "    # Get probability distributions for different steering multipliers in a single pass\n",
    "    print(f\"Processing with steering_multiplier=0...\")\n",
    "    probs_no_steering = get_all_next_token_probs(input_text, 0, layer_idx, steering_vector)\n",
    "    \n",
    "    print(f\"Processing with steering_multiplier=1.5...\")\n",
    "    probs_with_steering = get_all_next_token_probs(input_text, 1.5, layer_idx, steering_vector)\n",
    "    \n",
    "    # Calculate KL divergence for each token position\n",
    "    kl_divergences = []\n",
    "    for i in range(probs_no_steering.shape[0]):\n",
    "        kl_div_value = calculate_kl_divergence(probs_with_steering[i], probs_no_steering[i])\n",
    "        kl_divergences.append(kl_div_value)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(kl_divergences, marker='o')\n",
    "    plt.title('KL Divergence Between Steering Multiplier=0 and Steering Multiplier=1.5')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('KL Divergence')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Get tokens for x-axis labels (excluding the last token as we're looking at next token predictions)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer(input_text).input_ids)\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'steering_kl_divergence_{input_text}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the calculated divergences and token mapping for further analysis\n",
    "    return {\n",
    "        'kl_divergences': kl_divergences,\n",
    "        'tokens': tokens,\n",
    "        'probs_no_steering': probs_no_steering,\n",
    "        'probs_with_steering': probs_with_steering\n",
    "    }\n",
    "\n",
    "def analyze_steering_impact_last_line(input_text, steering_vector, layer_idx=20, save_plot=True):\n",
    "    print(\"====Prompt:====\")\n",
    "    print(input_text)\n",
    "    # Find the position of the last newline character in the input text\n",
    "    last_newline_pos = input_text.rfind('\\n')\n",
    "    \n",
    "    # If there's a newline, take only the text after it; otherwise, use the entire text\n",
    "    if last_newline_pos != -1:\n",
    "        last_line = input_text[last_newline_pos + 1:]\n",
    "        full_text = input_text  # Keep the full text for context\n",
    "        print(f\"Processing the last line: '{last_line}'\")\n",
    "    else:\n",
    "        last_line = input_text\n",
    "        full_text = input_text\n",
    "        print(\"No newline found, processing the entire text.\")\n",
    "    \n",
    "    # Tokenize both the full text and the last line\n",
    "    full_tokens = tokenizer(full_text, return_tensors=\"pt\").input_ids[0]\n",
    "    last_line_tokens = tokenizer(last_line, return_tensors=\"pt\").input_ids[0]\n",
    "    \n",
    "    # Find where the last line begins in the full tokenized sequence\n",
    "    # This is a simplified approach - in practice, you might need a more robust method\n",
    "    # to handle subword tokenization edge cases\n",
    "    last_line_start_idx = len(full_tokens) - len(last_line_tokens)\n",
    "    \n",
    "  \n",
    "    # Get probability distributions for different steering multipliers\n",
    "    print(f\"Processing with steering_multiplier=0...\")\n",
    "    probs_no_steering = get_all_next_token_probs(full_text, 0, layer_idx, steering_vector)\n",
    "    \n",
    "    print(f\"Processing with steering_multiplier=1.5...\")\n",
    "    probs_with_steering = get_all_next_token_probs(full_text, 1.5, layer_idx, steering_vector)\n",
    "    \n",
    "    # Extract only the probabilities for the last line (excluding the very last token)\n",
    "    last_line_probs_no_steering = probs_no_steering[last_line_start_idx+1:, :]\n",
    "    last_line_probs_with_steering = probs_with_steering[last_line_start_idx+1:, :]\n",
    "    \n",
    "    # Calculate KL divergence for each token position in the last line\n",
    "    kl_divergences = []\n",
    "    for i in range(last_line_probs_no_steering.shape[0]):\n",
    "        kl_div_value = calculate_kl_divergence(last_line_probs_with_steering[i], last_line_probs_no_steering[i])\n",
    "        kl_divergences.append(kl_div_value)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(kl_divergences, marker='o')\n",
    "    plt.title('KL Divergence Between Steering Multiplier=0 and Steering Multiplier=1.5 (Last Line Only)')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('KL Divergence')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Get tokens for x-axis labels for the last line\n",
    "    last_line_token_ids = tokenizer(last_line).input_ids[1:]  # Exclude the last token\n",
    "    last_line_tokens = tokenizer.convert_ids_to_tokens(last_line_token_ids)\n",
    "    \n",
    "    plt.xticks(range(len(last_line_tokens)), last_line_tokens, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_plot: plt.savefig(f'steering_kl_divergence_last_line_{input_text}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the calculated divergences and token mapping for further analysis\n",
    "    return {\n",
    "        'kl_divergences': kl_divergences,\n",
    "        'tokens': last_line_tokens,\n",
    "        'probs_no_steering': last_line_probs_no_steering,\n",
    "        'probs_with_steering': last_line_probs_with_steering,\n",
    "        'last_line': last_line\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5194d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "A rhyming tercet about spring cleaning:\n",
      "\n",
      "The dust bunnies flee, the cobwebs depart,\n",
      "As sunshine streams in, a fresh, hopeful start.\n",
      "The house gleams anew, a clean, purging art. \n",
      "\n",
      "Let me know if you'd like more!  \n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "So choose them wise, and you'll feel joy and gain! \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore more rhyming couplets! \n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "For a healthier life, let good habits reign.\n",
      "\n",
      "\n",
      "Let me know if you'd like more! 😊\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "Let's start with the basics, shall we? \n",
      "\n",
      "**Defining \"Good Habits\"**\n",
      "\n",
      "Good habits are essentially behaviors that benefit your physical, mental, and emotional well-being. They're actions you repeat consistently, making them automatic and ingrained in your daily routine. \n",
      "\n",
      "**Here's how good habits can help maintain and sustain good health:**\n",
      "\n",
      "* **Physical Health:**\n",
      "    * **Regular Exercise:**  Boosts cardiovascular health, strengthens muscles and bones, improves sleep, and reduces the risk of chronic diseases.\n",
      "    * **Balanced Diet:** Provides your body with essential nutrients, helps maintain a healthy weight, and reduces the risk of chronic diseases.\n",
      "    * **Adequate Sleep:** Allows your body to\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "\n",
      "Her compass guided her through life's hurricane.\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "\n",
      "So what does it mean?\n",
      "\n",
      "The meaning of the couplet depends entirely on the context!\n",
      "\n",
      "**Here are some possibilities:**\n",
      "\n",
      "* **Clarity:** The message was very clear and easy to understand.\n",
      "* **Honesty:** The message was truthful and straightforward.\n",
      "* **Simplicity:** The message was not complicated or hidden.\n",
      "* **Ominousness:** The message could be a warning or a threat, and its simplicity could make it even more unsettling.\n",
      "\n",
      "**To understand the true meaning, we need more information:**\n",
      "\n",
      "* **What is the message about?**\n",
      "* **Who wrote it?**\n",
      "* **Where was it written?**\n",
      "* **Who is reading it?**\n",
      "* **What\n",
      "Failed to get last word: A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "\n",
      "The steady hum, a torment that drove me insane. \n",
      "\n",
      "Let me know if you'd like to see more!\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "\n",
      "A shift in the air, a change unforeseen. \n",
      "\n",
      "\n",
      "Let me know if you'd like more!\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "\n",
      "The path we must take, a twist and a strain\n",
      "\n",
      "\n",
      "\n",
      "1500 poems processed, 481 selected\n"
     ]
    }
   ],
   "source": [
    "with open(\"texts_baseline_pain.json\",'r') as f:\n",
    "    pain_texts=json.load(f)\n",
    "    \n",
    "with open(\"texts_baseline_quick.json\",'r') as f:\n",
    "    quick_texts=json.load(f)\n",
    "    \n",
    "with open(\"all_outputs.json\",'r') as f:\n",
    "        all_outputs=json.load(f)\n",
    "\n",
    "i=0\n",
    "candidate_lines=[]\n",
    "for t in pain_texts:\n",
    "    for poem in t:\n",
    "        w=get_last_word(poem.replace(\"\\\\n\", \"\\n\"))\n",
    "        if all(w in x for x in all_outputs[\"base\"][\"baseline_pain\"][i]):\n",
    "            candidate_lines.append(poem)\n",
    "        i+=1\n",
    "print(i,\"poems processed,\", len(candidate_lines),\"selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a01796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "A rhyming tercet about spring cleaning:\n",
      "\n",
      "The dust bunnies flee, the cobwebs depart,\n",
      "As sunshine streams in, a fresh, hopeful start.\n",
      "The house gleams anew, a clean, purging art. \n",
      "\n",
      "Let me know if you'd like more!  \n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "So choose them wise, and you'll feel joy and gain! \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore more rhyming couplets! \n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "For a healthier life, let good habits reign.\n",
      "\n",
      "\n",
      "Let me know if you'd like more! 😊\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "\n",
      "\n",
      "Let's start with the basics, shall we? \n",
      "\n",
      "**Defining \"Good Habits\"**\n",
      "\n",
      "Good habits are essentially behaviors that benefit your physical, mental, and emotional well-being. They're actions you repeat consistently, making them automatic and ingrained in your daily routine. \n",
      "\n",
      "**Here's how good habits can help maintain and sustain good health:**\n",
      "\n",
      "* **Physical Health:**\n",
      "    * **Regular Exercise:**  Boosts cardiovascular health, strengthens muscles and bones, improves sleep, and reduces the risk of chronic diseases.\n",
      "    * **Balanced Diet:** Provides your body with essential nutrients, helps maintain a healthy weight, and reduces the risk of chronic diseases.\n",
      "    * **Adequate Sleep:** Allows your body to\n",
      "Failed to get last word: A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "\n",
      "Her compass guided her through life's hurricane.\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "\n",
      "So what does it mean?\n",
      "\n",
      "The meaning of the couplet depends entirely on the context!\n",
      "\n",
      "**Here are some possibilities:**\n",
      "\n",
      "* **Clarity:** The message was very clear and easy to understand.\n",
      "* **Honesty:** The message was truthful and straightforward.\n",
      "* **Simplicity:** The message was not complicated or hidden.\n",
      "* **Ominousness:** The message could be a warning or a threat, and its simplicity could make it even more unsettling.\n",
      "\n",
      "**To understand the true meaning, we need more information:**\n",
      "\n",
      "* **What is the message about?**\n",
      "* **Who wrote it?**\n",
      "* **Where was it written?**\n",
      "* **Who is reading it?**\n",
      "* **What\n",
      "Failed to get last word: A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "\n",
      "The steady hum, a torment that drove me insane. \n",
      "\n",
      "Let me know if you'd like to see more!\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "\n",
      "A shift in the air, a change unforeseen. \n",
      "\n",
      "\n",
      "Let me know if you'd like more!\n",
      "\n",
      "Failed to get last word: A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "\n",
      "The path we must take, a twist and a strain\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "candidate_texts = [get_second_line(text.replace(\"\\\\n\", \"\\n\"),try_third=True) for text in candidate_lines]\n",
    "candidate_last_tokens = [get_last_word(text.replace(\"\\\\n\", \"\\n\")) for text in candidate_lines]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaef918b-b108-40d8-93e2-739d9cc09e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Prompt:====\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "They'd build a tower to reach the moon's\n",
      "Processing the last line: 'They'd build a tower to reach the moon's'\n",
      "Processing with steering_multiplier=0...\n",
      "Processing with steering_multiplier=1.5...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAsPlJREFUeJzs3Xd8U/X+x/F3mqbpnoAUaNmKDC2icBUBFVkiDlSEXq8D18U9r3q9KriuW/R6xXXVqwiK+7pFf4rgAISWLUOBlj1KF11pcn5/tAkNnWmTnI7X8/HoA3JymnzySXKavvv9fo/FMAxDAAAAAAAAQBCFmF0AAAAAAAAA2h5CKQAAAAAAAAQdoRQAAAAAAACCjlAKAAAAAAAAQUcoBQAAAAAAgKAjlAIAAAAAAEDQEUoBAAAAAAAg6AilAAAAAAAAEHSEUgAAAAAAAAg6QikAfrdlyxZZLBa9/vrrZpcCtFjTp0+XxWIxu4yAOeWUU3TKKac0aN9LL71U3bp1a9T91NTHbt266dJLL23U7bVlFotF06dPN7sMv/PltdgS+fK8NeW9cXgf+SxQXWFhoTp06KC33nrL7FL86vXXX5fFYtGWLVvMLsWvvv/+e1ksFn3//fc+f++f/vQn/e1vf/N/UUArRCgFtGDuDwG//vqr1/a8vDwNHjxY4eHh+vLLLyUd+sVs3759Pt+PxWLxfIWGhioxMVGDBg3SjTfeqLVr1/rlsaDhunXr5vWchIeHq3fv3rr99tuVk5PTqNtcu3atpk+f3uo+UFZVVlamZ555RgMHDlRsbKzi4+PVr18/XXXVVfrtt988+/3000+aPn26cnNzzSvWZJdeeqksFotiY2NVXFxc7fqNGzd6Xn9PPPGEX+5zx44dmj59ujIzM/1yey2Jy+XSY489pu7duys8PFzHHHOM5s6da3ZZjbZ3717deOON6tOnjyIiItShQwcNHjxYd9xxhwoLCz37zZkzRzNnzjSv0GbAfTw//fTTa7z+5Zdf9rzXDv9Z31ht9Ri3c+dO3XnnnTr11FMVExPjc9jg/hx1+Fd4eHiDb+OZZ55RTEyMJk+eXO12G/P5rDF8ff4vvfRSRUdHB7aoJjIMQ2+++aaGDx+u+Ph4RUZGasCAAbr//vt18OBBU2q644479O9//1u7du0y5f6BliTU7AIA+Fd+fr5Gjx6tlStX6sMPP9TYsWP9crujRo3SxRdfLMMwlJeXpxUrVui///2vnn/+eT366KO65ZZbPPt27dpVxcXFstlsfrlvVJeWlqZbb71VklRSUqJly5Zp5syZWrBggZYsWeLz7a1du1YzZszQKaec0ugRKc3deeedpy+++EJTpkzRlVdeKYfDod9++02ffvqpTjrpJPXp00dSxQf2GTNm6NJLL1V8fLxp9f7jH//QnXfeadr9h4aGqqioSJ988okmTZrkdd1bb72l8PBwlZSU+O3+duzYoRkzZqhbt25KS0vzuu7ll1+Wy+Xy232tX79eISHN5+9yd999tx555BFdeeWVOuGEE/Txxx8rPT1dFovF65fXliAnJ0fHH3+88vPzNXXqVPXp00f79+/XypUrNWvWLE2bNs3zC+6cOXO0evVq3XTTTabW/PXXX5t6/+Hh4fruu++0a9cudezY0eu6QLzX6jrG+fO90dw+C6xfv16PPvqoevfurQEDBujnn39u1O3MmjXLK6SxWq0N+j6Hw6FnnnlGN998c4O/JxAC8TPuL3/5iyZPniy73e6X2/OF0+lUenq65s2bp2HDhmn69OmKjIzUwoULNWPGDL377rv65ptvdMQRRwS1rrPPPluxsbF6/vnndf/99wf1voGWhlAKaEUKCgo0ZswYZWZm6oMPPtC4ceP8dttHHnmkLrroIq9tjzzyiCZMmKBbb71Vffr00RlnnCFJPv/l0F8OHjyoqKiooN+vGTp37uz1fFxxxRWKjo7WE088oY0bN6p3794mVtf8LF26VJ9++qkeeugh/f3vf/e67rnnnmtWIwbcr+PQ0FCFhpr3Y9put2vo0KGaO3dutVBqzpw5Gj9+vN5///2g1OLvX2r9+YtTeXm5XC6XwsLCGvX927dv15NPPqlrr71Wzz33nKSK9/OIESN0++2364ILLjD1F1hf/ec//1FWVpZ+/PFHnXTSSV7X5efnN7pPgVBUVKTIyEjTaxo6dKiWLl2qd955RzfeeKNn+7Zt27Rw4UKde+65QXuv+fO94e/PAk39GT9o0CDt379fiYmJeu+993TBBRc06nbOP/98tWvXzufv+/TTT7V3795qx9PWwGq1mnaceuyxxzRv3jzddtttevzxxz3br7rqKk2aNEnnnHOOLr30Un3xxRdBrSskJETnn3++3njjDc2YMaNVT8cHmqr5/JkQQJMUFhZq7NixWr58ud5//32NHz8+4PeZlJSkt99+W6GhoXrooYc82w9fR+KJJ56QxWLR1q1bq93GXXfdpbCwMB04cMCzbfHixRo7dqzi4uIUGRmpESNG6Mcff/T6Pvdw97Vr1yo9PV0JCQk6+eSTJVVMhZk+fbo6deqkyMhInXrqqVq7dm2Na2Xk5ubqpptuUkpKiux2u3r16qVHH33Ua1SG+/E88cQTeumll9SzZ0/Z7XadcMIJWrp0abXH9Ntvv2nSpElq3769IiIidNRRR+nuu+/22mf79u2aOnWqjjjiCNntdvXr10+vvvpqwxpfC/df2A8PMn777Tedf/75SkxMVHh4uI4//nj973//81z/+uuvez6cn3rqqZ4pCd9//71uueUWJSUlyTAMz/7XX3+9LBaLnn32Wc+23bt3y2KxaNasWZ5tpaWluu+++9SrVy/Z7XalpKTob3/7m0pLS6vVPnv2bA0aNEgRERFKTEzU5MmTlZ2d7bXPKaecov79+2vt2rU69dRTFRkZqc6dO+uxxx6rtze///67pIpf/g5ntVqVlJQkqeJ1dfvtt0uSunfv7ulF1WmNDalVavrruKa1kCwWi6677jp99NFH6t+/v+e1456mW9X333+v448/XuHh4erZs6defPFFn9epSk9P1xdffOEV2i1dulQbN25Uenp6tf1ru/361hv5/vvvdcIJJ0iSLrvsMk/f3ceQw9eUqvqefPrpp9W1a1dFRERoxIgRWr16db2Pyx/HgpkzZ3qOBU2Zxvzxxx/L4XDommuu8WyzWCyaNm2atm3bVu9ojpUrV+rSSy9Vjx49FB4ero4dO2rq1Knav3+/137u52bTpk2eERJxcXG67LLLVFRU5LVvaWmpbr75ZrVv314xMTE666yztG3btgY9nt9//11Wq1V/+tOfql0XGxvrCSlOOeUUffbZZ9q6davn+a76HAfq+LFs2TINHz5ckZGRnoD68LWQ3OvIzJs3Tw899JC6dOmi8PBwjRw5Ups2bap2///+97/Vo0cPRUREaPDgwVq4cKFP61SFh4dr4sSJmjNnjtf2uXPnKiEhQWPGjKn2PbXdfn3rr9V3jDv8veF+7/7www+6+uqrlZSUpNjYWF188cVeP7drUtuaUvX9TKp6vwsWLNA111yjDh06qEuXLnXeX31iYmKUmJjYpNuQKqaK5efne/1cbIiPPvpI3bp1U8+ePX2+z5ycHN12220aMGCAoqOjFRsbq3HjxmnFihXV9v3Xv/6lfv36KTIyUgkJCTr++OM9r62G/IxrjJqO8d26ddOZZ56pRYsWeZaU6NGjh954441q39+Q429NiouL9fjjj+vII4/UP//5z2rXT5gwQZdccom+/PJL/fLLL42qrar77rtPNptNe/furXbdVVddpfj4eK9RjaNGjdLWrVvb5LR0wBeMlAJagYMHD2rcuHFaunSp3nvvPZ155plBu+/U1FSNGDFC3333nfLz8xUbG1ttn0mTJulvf/ub5s2b5/kw5DZv3jyNHj1aCQkJkqT/+7//07hx4zRo0CDdd999CgkJ0WuvvabTTjtNCxcu1ODBg72+/4ILLlDv3r318MMPez4g3nXXXXrsscc0YcIEjRkzRitWrNCYMWOqTX8oKirSiBEjtH37dl199dVKTU3VTz/9pLvuuks7d+6sttbJnDlzVFBQoKuvvloWi0WPPfaYJk6cqD/++MMzkmPlypUaNmyYbDabrrrqKnXr1k2///67PvnkE09wt3v3bv3pT3/yBAzt27fXF198ocsvv1z5+fkNmsricDg860+UlJQoIyNDTz31lIYPH67u3bt79luzZo2GDh2qzp07684771RUVJTmzZunc845R++//77OPfdcDR8+XDfccIOeffZZ/f3vf9fRRx8tSTr66KN14MABPf3001qzZo369+8vSVq4cKFCQkK0cOFC3XDDDZ5tkjR8+HBJFcHgWWedpUWLFumqq67S0UcfrVWrVunpp5/Whg0b9NFHH3lqfOihh3TPPfdo0qRJuuKKK7R3717961//0vDhw5WRkeE1veDAgQMaO3asJk6cqEmTJum9997THXfcoQEDBtQ5MrBr166SKqbCDB06tNYRSBMnTtSGDRs0d+5cPf30056/hrdv396nWv3xOq7NokWL9MEHH+iaa65RTEyMnn32WZ133nnKysryhGsZGRkaO3askpOTNWPGDDmdTt1///2ex9FQEydO1F//+ld98MEHmjp1qqSK90GfPn103HHH+XRbdTn66KN1//33695779VVV12lYcOGSVK1kTaHe+ONN1RQUKBrr71WJSUleuaZZ3Taaadp1apVPk3V8PVY8Nprr6mkpERXXXWV7Ha75xfdhq4JExMT4xmRkpGRoaioKM/7zs39GsnIyPAElTWZP3++/vjjD1122WXq2LGj1qxZo5deeklr1qzRL7/8Ui0knDRpkrp3765//vOfWr58uV555RV16NBBjz76qGefK664QrNnz1Z6erpOOukk/d///V+D/9DRtWtXOZ1Ovfnmm7rkkktq3e/uu+9WXl6etm3bpqefflqSPFOiAnX82L9/v8aNG6fJkyfroosuqvc18sgjjygkJES33Xab8vLy9Nhjj+nPf/6zFi9e7Nln1qxZuu666zRs2DDdfPPN2rJli8455xwlJCT4FKKkp6dr9OjR+v333z2hxZw5c3T++ef7daRgfce42lx33XWKj4/X9OnTtX79es2aNUtbt271BHgN1ZCfSVVdc801at++ve69917P2kAOh0N5eXkNur/ExES/T9Xt0aOHCgsLFRUVpXPOOUdPPvlkg443P/30U6OPm3/88Yc++ugjXXDBBerevbt2796tF198USNGjNDatWvVqVMnSRVTnW+44Qadf/75uvHGG1VSUqKVK1dq8eLFSk9Pb/Tz31ibNm3S+eefr8svv1yXXHKJXn31VV166aUaNGiQ+vXrJ8n3429VixYt0oEDB3TjjTfW+nP94osv1muvvaZPP/3UKyxvSG2H+8tf/qL7779f77zzjq677jrP9rKyMr333ns677zzvEYHDho0SJL0448/auDAgQ3uG9DmGABarNdee82QZHTt2tWw2WzGRx99VOu+9913nyHJ2Lt3r8/3I8m49tpra73+xhtvNCQZK1asMAzDMDZv3mxIMl577TXPPieeeKIxaNAgr+9bsmSJIcl44403DMMwDJfLZfTu3dsYM2aM4XK5PPsVFRUZ3bt3N0aNGlXt8UyZMsXrNnft2mWEhoYa55xzjtf26dOnG5KMSy65xLPtgQceMKKioowNGzZ47XvnnXcaVqvVyMrK8no8SUlJRk5Ojme/jz/+2JBkfPLJJ55tw4cPN2JiYoytW7d63WbVx3P55ZcbycnJxr59+7z2mTx5shEXF2cUFRUZdenatashqdrX0KFDq93myJEjjQEDBhglJSVetZx00klG7969PdveffddQ5Lx3XffeX3/nj17DEnG888/bxiGYeTm5hohISHGBRdcYBxxxBGe/W644QYjMTHR8zjffPNNIyQkxFi4cKHX7b3wwguGJOPHH380DMMwtmzZYlitVuOhhx7y2m/VqlVGaGio1/YRI0Z4vV4MwzBKS0uNjh07Guedd16dPXO5XJ7vP+KII4wpU6YY//73v6s9T4ZhGI8//rghydi8ebPX9obW6o/XcdXrqpJkhIWFGZs2bfJsW7FihSHJ+Ne//uXZNmHCBCMyMtLYvn27Z9vGjRuN0NDQardZk0suucSIiooyDMMwzj//fGPkyJGGYRiG0+k0OnbsaMyYMcPzvnj88cfrrNkwDh2rqvZ0xIgRxogRIzyXly5dWu24UbWerl27ei677zsiIsLYtm2bZ/vixYsNScbNN99cZ01du3Zt0rEgNjbW2LNnT7U6a3pf1vRV9TGOHz/e6NGjR7XbOnjwoCHJuPPOO6tdV1VNx4u5c+cakowffvihWh+mTp3qte+5555rJCUleS5nZmYakoxrrrnGa7/09HRDknHffffVWc+uXbuM9u3bG5KMPn36GH/961+NOXPmGLm5udX2HT9+vNfz6hbI48cLL7xQ7f4Ofy1+9913hiTj6KOPNkpLSz3bn3nmGUOSsWrVKsMwKo4/SUlJxgknnGA4HA7Pfq+//rohyes2a9O1a1dj/PjxRnl5udGxY0fjgQceMAzDMNauXWtIMhYsWOB5/yxdurTWmt0Of68YhlHteavtGOeup+p7w33fgwYNMsrKyjzbH3vsMUOS8fHHH9daU02fBRr6M8l9vyeffLJRXl7uVaP7+WnIV02P0TBq/5lXl5kzZxrXXXed8dZbbxnvvfeeceONNxqhoaFG7969jby8vDq/1+FwGBaLxbj11lurXdeQz2clJSWG0+n02rZ582bDbrcb999/v2fb2WefbfTr16/OWup6/mtS9edBbWo6xrs/q1Q9Du3Zs8ew2+1efWjo8bcmM2fONCQZH374Ya375OTkGJKMiRMn+lyb+7VW9XVy4oknGkOGDPG6jw8++KDW11NYWJgxbdq0WusDYBhM3wNagd27dys8PFwpKSmm3L/7r9sFBQW17nPhhRdq2bJlnmlUkvTOO+/Ibrfr7LPPliRlZmZ6pgXt379f+/bt0759+3Tw4EGNHDlSP/zwQ7Wh3H/961+9Ln/77bcqLy/3mgojVUw5O9y7776rYcOGKSEhwXNf+/bt0+mnny6n06kffvih2mNwj+iS5BnN8ccff0iqOOPUDz/8oKlTpyo1NdXre91/STYMQ++//74mTJggwzC87nfMmDHKy8vT8uXLa+2j25AhQzR//nzNnz/fs1bSmjVrdNZZZ3nOlpaTk6P/+7//06RJk1RQUOC5n/3792vMmDHauHGjtm/fXuf9tG/fXn369PH04scff5TVatXtt9+u3bt3a+PGjZIqRkqdfPLJnsf57rvv6uijj1afPn28HuNpp50mSfruu+8kSR988IFcLpcmTZrktV/Hjh3Vu3dvz35u0dHRXmtphYWFafDgwZ7noDYWi0VfffWVHnzwQSUkJGju3Lm69tpr1bVrV1144YUNWlOqobX643Vcl9NPP91r+scxxxyj2NhYTw+cTqe++eYbnXPOOZ6/nktSr169GrXOXHp6ur7//nvt2rVL//d//6ddu3bVOHXPDOecc446d+7suTx48GANGTJEn3/+uU+34+ux4LzzzqtxZIH7PVnfV9XpWMXFxTWu4+P+a3tNZz+sKiIiwvP/kpIS7du3zzMaoKZjyeGvtWHDhmn//v3Kz8+XJE/v3KMg3Rq6GPkRRxyhFStW6K9//asOHDigF154Qenp6erQoYMeeOCBBk15CtTxw26367LLLmvQ45AqppJWXW/q8GP+r7/+qv379+vKK6/0GqXx5z//2etnRUNYrVZNmjTJc9bFt956SykpKZ77NNtVV13lNWJr2rRpCg0N9em91pifSVdeeWW1tYqOPfbYBr/XDl84viluvPFG/etf/1J6errOO+88zZw5U//973+1ceNGPf/88/U+dsMwfH5duNntds+IL6fTqf379ys6OlpHHXWU1/s8Pj5e27Ztq3FpATP07dvX6zXcvn17HXXUUV4/s309/lbl/twZExNT6z7u69zHOF9qq8nFF1+sxYsXe32edb9fR4wYUW1/9+MCUDum7wGtwIsvvqhbbrlFY8eO1cKFC3XUUUcF9f7dp/iu60PBBRdcoFtuuUXvvPOO/v73v8swDL377rsaN26cZ8qfO+Coa8pHXl6e14e6qlPVJHnWrerVq5fX9sTExGofBjdu3KiVK1fWOmx9z549XpcPD5rct+deV8P9QcY9za0me/fuVW5url566SW99NJLDbrfmrRr187rFOLjx4/XUUcdpfPPP1+vvPKKrr/+em3atEmGYeiee+7RPffcU+t9Vf2lvibDhg3z/OKxcOFCHX/88Tr++OOVmJiohQsXen4JrRpUbNy4UevWrau3txs3bpRhGLUuzH74tJUuXbpUmyqSkJCglStX1vkYpIoP9Xfffbfuvvtu7dy5UwsWLNAzzzyjefPmyWazafbs2XV+f0Nr9cfruC6Hvw6lih64X4d79uxRcXFxtfeAVP190RBnnHGGYmJi9M477ygzM1MnnHCCevXq1eQ1SPyhpufiyCOP1Lx583y6HV+PBbU9X1Xfkw0VERFR4zpJ7unGVUOnmuTk5GjGjBl6++23q9VZ0xSnuo5jsbGx2rp1q0JCQqqte+PLz5Xk5GTNmjVLzz//vDZu3KivvvpKjz76qO69914lJyfriiuuqPP7A3X86Ny5s0+Lmtd3zK/t501oaGijzmKanp6uZ599VitWrNCcOXM0efLkZrM48uE9jo6OVnJysk/Hgcb8TKrpvZaQkNCo91ogpKen69Zbb9U333zToLOlNiSUrYnL5dIzzzyj559/Xps3b5bT6fRc5562LUl33HGHvvnmGw0ePFi9evXS6NGjlZ6eXuN6isFQ388ryffjb1Xuz511/VG0tuCqIbXV5MILL9RNN92kt956S/fee6/y8vL06aef6uabb67x/WoYRrN5HwPNFaEU0Ar07dtXn3/+uUaOHKlRo0bpxx9/DOqoqdWrV8tqtdb5i3WnTp00bNgwzZs3T3//+9/1yy+/KCsry2sdE/fokccff7zaKeHdqp6GWar/F7a6uFwujRo1Sn/7299qvP7II4/0ulzbmWV8+ZDpfowXXXRRraHFMccc0+Dbq2rkyJGSpB9++EHXX3+9575uu+22GhfKlRoWUpx88sl6+eWX9ccff2jhwoUaNmyYLBaLTj75ZC1cuFCdOnWSy+Xy+oujy+XSgAED9NRTT9V4m+7Xp8vlksVi0RdffFFjfw9/vv3xHEgVvzRPnjxZ5513nvr166d58+bp9ddfr/Nsdw2tNdCvY3/1oKHsdrsmTpyo//73v/rjjz80ffr0Wvet7YN31V+gmiNfjwW1PV+7du1q0P3FxcV5biM5OVnfffddtV9cdu7cKUleo91qMmnSJP3000+6/fbblZaWpujoaLlcLo0dO7bGRYKD+fqxWCw68sgjdeSRR2r8+PHq3bu33nrrrXpDqUAdP3z9eRHs99qQIUPUs2dP3XTTTdq8eXOdIxItFkuNdTTn91pjfibV9JyVlZUpJyenQffZvn37gJ8VLiUlpd56EhMTZbFY6g08avPwww/rnnvu0dSpU/XAAw941sq66aabvN7nRx99tNavX69PP/1UX375pd5//309//zzuvfeezVjxoxG3XdTNOQ95Ovxtyr3WnwrV67UOeecU+M+7j9a9e3b1+faapKQkKAzzzzTE0q99957Ki0trXaGarfc3NxGna0RaEsIpYBWYvDgwfroo480fvx4jRo1SgsXLgzYwpVVZWVlacGCBTrxxBPrHCklVfx16ZprrtH69ev1zjvvKDIyUhMmTPBc7/7LfGxsbKP/Cupe0HrTpk1eIdn+/furfRjs2bOnCgsL/fYX1x49ekhSnWf/cp/Nyul0+v0vveXl5ZIOjVxz12Oz2eq9r7r+iucOm+bPn6+lS5d6/ho8fPhwzZo1S506dVJUVJRnQU+porcrVqzQyJEj67ztnj17yjAMde/evc4PnoFis9l0zDHHaOPGjZ5pP7XV29Ba/fE6booOHTooPDy8xrOE1bStIdLT0/Xqq68qJCREkydPrnU/90iS3NxcrwWmazrz5uEa85dk96i0qjZs2ODzKBV/HQuSk5MbtN9rr73mOcNZWlqaXnnlFa1bt87rlyb3Ytq1BZtSxYidb7/9VjNmzNC9997r2V5TXxqqa9eucrlc+v33371GR61fv77RtylVHI8SEhI8YZtU+3PeUo4fVX/enHrqqZ7t5eXl2rJlS6P+wDBlyhQ9+OCDOvroo+t87hMSEmqcZhTI91rVx1hYWKidO3fqjDPOaPBt+PIzqS4//fSTVy112bx5c6NGrTWUYRjasmVLvYtYh4aGqmfPntq8eXOj7ue9997Tqaeeqv/85z9e22sKPKKionThhRfqwgsvVFlZmSZOnKiHHnpId911l8LDw5vdqJ2mHH9PPvlkxcfHa86cObr77rtrDJrcZ9Tz50mALr74Yp199tlaunSp3nrrLQ0cOLDGxdG3b9+usrKyaieyAOCNNaWAVmTkyJGaO3euNm3apLFjx1abP+9vOTk5mjJlipxOp+6+++569z/vvPNktVo1d+5cvfvuuzrzzDMVFRXluX7QoEHq2bOnnnjiCU+wUlVNp+A93MiRIxUaGqpZs2Z5bX/uueeq7Ttp0iT9/PPP+uqrr6pdl5ub6wl5Gqp9+/YaPny4Xn31VWVlZXld5/7Lm9Vq1Xnnnaf333+/xvCqIY+xNp988omkivU2pIpw4pRTTtGLL77o9YtgTfflfh5qWlupe/fu6ty5s55++mk5HA7PNIBhw4bp999/13vvvac//elPXqOMJk2apO3bt+vll1+udnvFxcWesyhNnDhRVqtVM2bMqPbXScMwqp3WvrE2btxY7TmRKh7vzz//rISEBE+IW1svGlqrP17HTWG1WnX66afro48+0o4dOzzbN23apC+++KJRt3nqqafqgQce0HPPPVfnGi3uQK7qGiAHDx7Uf//733rvo67XYG0++ugjrzVolixZosWLF/u8dpa/jgWNWVPq7LPPls1m81qTxjAMvfDCC+rcuXOdZyB0/wJ2+OuxrrNV1cfdu2effbZRt7l48WLP+7uqJUuWaP/+/V5BV1RUVI1TDJvb8aM2xx9/vJKSkvTyyy97vUbeeuutRo+IueKKK3TffffpySefrHO/nj176rfffvM6nqxYsUI//vhjvffRmPfaSy+9JIfD4bk8a9YslZeX+/Re8+VnUl2CsaZUVlaWfvvtt3rrmzVrlvbu3auxY8fWe5snnniifv3110bVY7Vaq73G33333WprcB3+mg8LC1Pfvn1lGIbn+WvM8x9ITTn+RkZG6rbbbtP69etr/Bz62Wef6fXXX9eYMWO8zrzXVOPGjVO7du306KOPasGCBbWOklq2bJmk+s8kC7R1jJQCWplzzz1XL7/8sqZOnaqzzjpLX375pdfpaZ966ilFRkZ6fU9ISIj+/ve/13m7GzZs0OzZs2UYhvLz87VixQq9++67Kiws1FNPPdWgD2QdOnTQqaeeqqeeekoFBQW68MILq9XxyiuvaNy4cerXr58uu+wyde7cWdu3b9d3332n2NhYT/BSmyOOOEI33nijnnzySZ111lkaO3asVqxYoS+++ELt2rXz+gvh7bffrv/9738688wzPacBPnjwoFatWqX33ntPW7Zs8XnI9bPPPquTTz5Zxx13nK666ip1795dW7Zs0WeffabMzExJFacZ/+677zRkyBBdeeWV6tu3r3JycrR8+XJ98803DZqWsH37ds8aSGVlZVqxYoVefPFFtWvXzmtR93//+986+eSTNWDAAF155ZXq0aOHdu/erZ9//lnbtm3TihUrJFWMxrBarXr00UeVl5cnu92u0047TR06dJBUEUC9/fbbGjBggGc0zHHHHaeoqCht2LCh2jSTv/zlL5o3b57++te/6rvvvtPQoUPldDr122+/ad68efrqq690/PHHq2fPnnrwwQd11113eU6lHhMTo82bN+vDDz/UVVddpdtuu82n56Am7jWvxo0bp2HDhikxMVHbt2/Xf//7X+3YsUMzZ870/ILvHvF19913a/LkybLZbJowYUKDa/XH67ippk+frq+//lpDhw7VtGnT5HQ69dxzz6l///6e16EvQkJC9I9//KPe/UaPHq3U1FRdfvnluv3222W1WvXqq6+qffv2NYaCVfXs2VPx8fF64YUXFBMTo6ioKA0ZMqTOacG9evXSySefrGnTpqm0tFQzZ85UUlJSrdNAauOvY0Fj/tLfpUsX3XTTTXr88cflcDh0wgkn6KOPPtLChQv11ltv1Tn1KDY2VsOHD9djjz0mh8Ohzp076+uvv270aAyp4lgwZcoUPf/888rLy9NJJ52kb7/9tsGj7N5880299dZbOvfcczVo0CCFhYVp3bp1evXVVxUeHu71s2bQoEF65513dMstt+iEE05QdHS0JkyY0OyOH7UJCwvT9OnTdf311+u0007TpEmTtGXLFr3++uvq2bNno0akdO3atc4psm5Tp07VU089pTFjxujyyy/Xnj179MILL6hfv371/kGqtmNc1T8SHa6srEwjR47UpEmTtH79ej3//PM6+eSTddZZZ/n0+Br6M6kuTVlT6sEHH5QkrVmzRlLF63XRokWS5HWMu/jii7VgwQKvIMh9YowBAwYoPDxcixYt0ttvv620tDRdffXV9d732WefrTfffFMbNmyocWRfXZ/PzjzzTN1///267LLLdNJJJ2nVqlV66623PKPP3EaPHq2OHTtq6NChOuKII7Ru3To999xzGj9+vGc0e2Oef4fD4eldVYmJidVOLOOrph5/77zzTmVkZOjRRx/Vzz//rPPOO08RERFatGiRZs+eraOPPrpBfxjxhc1m0+TJk/Xcc8/JarVqypQpNe43f/58paam1juSDmjzAnlqPwCBVdNpot2eeOIJQ5Jx5plnGg6Hw3PK4Zq+rFZrnfdTdd+QkBAjPj7eGDhwoHHjjTcaa9asqbZ/TaeBdnv55ZcNSUZMTIxRXFxc4/1lZGQYEydONJKSkgy73W507drVmDRpkvHtt9969qnrFMrl5eXGPffcY3Ts2NGIiIgwTjvtNGPdunVGUlKS8de//tVr34KCAuOuu+4yevXqZYSFhRnt2rUzTjrpJOOJJ57wnP7a/Xgef/zxGntz+CnSV69ebZx77rlGfHy8ER4ebhx11FHGPffc47XP7t27jWuvvdZISUkxbDab0bFjR2PkyJHGSy+9VGNPqnKfyrjqc9KhQwdjypQpxqZNm6rt//vvvxsXX3yx0bFjR8NmsxmdO3c2zjzzTOO9997z2u/ll182evToYVit1mqnNv73v/9tSKp2WuPTTz/dkOT13LiVlZUZjz76qNGvXz/DbrcbCQkJxqBBg4wZM2ZUO332+++/b5x88slGVFSUERUVZfTp08e49tprjfXr13v2GTFiRI2nuq7pFOiH2717t/HII48YI0aMMJKTk43Q0FAjISHBOO2006r1wTAqTlHduXNnIyQkpNpprhtSq2E0/XXsvq4qSca1115bbd/DT+NuGIbx7bffGgMHDjTCwsKMnj17Gq+88opx6623GuHh4XX2yjAadgrw2t4Xy5YtM4YMGWKEhYUZqampxlNPPVXj6cJrOqX9xx9/bPTt29cIDQ31OoYc/hxXve8nn3zSSElJMex2uzFs2DBjxYoVXrdZUx9r6ldTjwVN4XQ6jYcfftjo2rWrERYWZvTr18+YPXt2g75327ZtnuNNXFycccEFFxg7duyodmyq7bVW03NTXFxs3HDDDUZSUpIRFRVlTJgwwcjOzq7xeHe4lStXGrfffrtx3HHHGYmJiUZoaKiRnJxsXHDBBcby5cu99i0sLDTS09ON+Ph4Q5LXcxys44f7uqqvRfdp4N99912v/Wr72fbss88aXbt2Nex2uzF48GDjxx9/NAYNGmSMHTu2zl4ZRsVrcfz48XXuU9vP+tmzZxs9evQwwsLCjLS0NOOrr76q8XhY0/NW2zHu8PeG+74XLFhgXHXVVUZCQoIRHR1t/PnPfzb279/vdZuH97G2fjXkZ1Jdn2+aorbPQYcfI0aMGFFt2xVXXGH07dvXiImJMWw2m9GrVy/jjjvuMPLz8xt036WlpUa7du2MBx54wGt7Qz6flZSUGLfeequRnJxsREREGEOHDjV+/vnnaj1/8cUXjeHDh3t+7vTs2dO4/fbbq71n6voZd7hLLrmk1vp69uxpGEbNx5HaXts1Hfsbcvyti9PpNF577TVj6NChRmxsrBEeHm7069fPmDFjhlFYWFht/4bW5j4WVP085LZkyRJDkjF69Ohaa0pOTjb+8Y9/1Fs/0NZZDCNAqzUCQDOSm5urhIQEPfjggw2aagi0Ruecc47WrFnTpDWHmoMtW7aoe/fuevzxxwM6EgZoDJfLpfbt22vixIk1TkFsSV5//XVddtllWrp0qY4//nizy2nxHnjgAb322mvauHFjwBdgR2CtWLFCaWlpeuONN/SXv/yl2vUfffSR0tPT9fvvvzd4vUGgrWJNKQCtTnFxcbVt7vVQTjnllOAWA5jk8PfBxo0b9fnnn/MeAPyopKSk2lo/b7zxhnJycnivoZqbb75ZhYWFevvtt80uBU308ssvKzo6WhMnTqzx+kcffVTXXXcdgRTQAKwpBaDVeeedd/T666/rjDPOUHR0tBYtWqS5c+dq9OjRnkW6gdauR48euvTSS9WjRw9t3bpVs2bNUlhYmM/rLQGo3S+//KKbb75ZF1xwgZKSkrR8+XL95z//Uf/+/XXBBReYXR6amejoaO3Zs8fsMtAEn3zyidauXauXXnpJ1113Xa1rcf38889BrgxouQilALQ6xxxzjEJDQ/XYY48pPz/fs/h5TYt0Aq3V2LFjNXfuXO3atUt2u10nnniiHn74YfXu3dvs0oBWo1u3bkpJSdGzzz6rnJwcJSYm6uKLL9YjjzyisLAws8sD4GfXX3+9du/erTPOOEMzZswwuxygVWBNKQAAAAAAAAQda0oBAAAAAAAg6AilAAAAAAAAEHQtek0pl8ulHTt2KCYmRhaLxexyAAAAAAAA2jzDMFRQUKBOnTopJKT28VAtOpTasWOHUlJSzC4DAAAAAAAAh8nOzlaXLl1qvb5Fh1IxMTGSKh5kbGysydU0ncPh0Ndff63Ro0fLZrOZXU6bQM/NQd+Dj56bg74HHz03B30PPnpuDvoefPTcHPTdHK2p7/n5+UpJSfHkNrVp0aGUe8pebGxsqwmlIiMjFRsb2+JfgC0FPTcHfQ8+em4O+h589Nwc9D346Lk56Hvw0XNz0HdztMa+17fUEgudAwAAAAAAIOgIpQAAAAAAABB0hFIAAAAAAAAIOkIpAAAAAAAABB2hFAAAAAAAAIKOUAoAAAAAAABBRygFAAAAAACAoCOUAgAAAAAAQNARSgEAAAAAACDoCKUAAAAAAAAQdIRSAAAAAAAACDpCKQAAAAAAAAQdoRQAAAAAAACCjlAKAAAAAAAAQUcoBQAAAAAAYCKny9DizTlats+ixZtz5HQZZpcUFKFmFwAAAAAAANBWfbl6p2Z8slY780okWfXGxl+VHBeu+yb01dj+yWaXF1CMlAIAAAAAADDBl6t3atrs5ZWB1CG78ko0bfZyfbl6p0mVBYepoZTT6dQ999yj7t27KyIiQj179tQDDzwgw2gbw9QAAAAAAEDb5HQZmvHJWtWUgLi3zfhkbaueymfq9L1HH31Us2bN0n//+1/169dPv/76qy677DLFxcXphhtuMLM0AAAAAACAgFmyOafaCKmqDEk780q0ZHOOTuyZFLzCgsjUUOqnn37S2WefrfHjx0uSunXrprlz52rJkiVmlgUAAAAAABBQewpqD6Qas19LZGooddJJJ+mll17Shg0bdOSRR2rFihVatGiRnnrqqRr3Ly0tVWlpqedyfn6+JMnhcMjhcASl5kByP4bW8FhaCnpuDvoefPTcHPQ9+Oi5Oeh78NFzc9D34KPn5qDvwZEU2bBIJikytMU9Fw2t12KYuICTy+XS3//+dz322GOyWq1yOp166KGHdNddd9W4//Tp0zVjxoxq2+fMmaPIyMhAlwsAAAAAAOAXLkOasdyq3DJJstSwh6H4MOm+45wKqenqZqyoqEjp6enKy8tTbGxsrfuZGkq9/fbbuv322/X444+rX79+yszM1E033aSnnnpKl1xySbX9axoplZKSon379tX5IFsKh8Oh+fPna9SoUbLZbGaX0ybQc3PQ9+Cj5+ag78FHz81B34OPnpuDvgcfPTcHfQ+er9bs1nVvr6i23Z1B/WvysRrT74jgFuUH+fn5ateuXb2hlKnT926//Xbdeeedmjx5siRpwIAB2rp1q/75z3/WGErZ7XbZ7fZq2202W6t6o7S2x9MS0HNz0Pfgo+fmoO/BR8/NQd+Dj56bg74HHz03B30PvDPTuuh/K3fp67W7vbZ3jAvXfRP6amz/ZJMqa5qGvm5MDaWKiooUEhLitc1qtcrlcplUEQAAAAAAQPDkFVesv3TF0G4q2/O7Rg8bohN7dZC1pc3ZawRTQ6kJEybooYceUmpqqvr166eMjAw99dRTmjp1qpllAQAAAAAABFy506WV2/IkSROP66SNv27SkO6JbSKQkkwOpf71r3/pnnvu0TXXXKM9e/aoU6dOuvrqq3XvvfeaWRYAAAAAAEDAbdhdqGKHUzH2UPVsF6WNZhcUZKaGUjExMZo5c6ZmzpxpZhkAAAAAAABBl5F9QJJ0bEq8QtrI6KiqQurfBQAAAAAAAP6WkZUrSRqYGm9qHWYhlAIAAAAAADBBZnauJEIpAAAAAAAABElesUOb9hRKko7tEm9uMSYhlAIAAAAAAAiyFZWjpLomRSop2m5uMSYhlAIAAAAAAAgyz3pSKfGm1mEmQikAAAAAAIAgy6w8814aoRQAAAAAAACCwTAMZXgWOU8wtxgTEUoBAAAAAAAE0Zb9RcotcigsNERHJ8eaXY5pCKUAAAAAAACCyD11b0DnOIWFtt1opu0+cgAAAAAAABO4Fzlvy+tJSYRSAAAAAAAAQeU5815qvKl1mI1QCgAAAAAAIEhKHE6t25kvqW0vci4RSgEAAAAAAATN6u15KncZah9jV6e4cLPLMRWhFAAAAAAAQJB4pu6lxMtisZhbjMkIpQAAAAAAAIIko/LMe2196p5EKAUAAAAAABA0mSxy7kEoBQAAAAAAEAS78kq0I69EIRZpQOc4s8sxHaEUAAAAAABAEGRWTt07qmOsouyhJldjPkIpAAAAAACAIMhg6p4XQikAAAAAAIAgyMjOlSSlpcSbWkdzQSgFAAAAAAAQYOVOl1Zuy5UkHcdIKUmEUgAAAAAAAAH3264ClThcigkPVY920WaX0ywQSgEAAAAAAARYZpWpeyEhFnOLaSYIpQAAAAAAAALMs8g560l5EEoBAAAAAAAEWEb2AUnSwNQEkytpPgilAAAAAAAAAiivyKE/9h6UxJn3qiKUAgAAAAAACKDMyrPudUuKVEJUmLnFNCOEUgAAAAAAAAGUkcXUvZoQSgEAAAAAAASQZ5Hz1HhT62huCKUAAAAAAAACxDAMZWbnSpIGpjBSqipCKQAAAAAAgADZvO+g8oodsoeGqE9yjNnlNCuEUgAAAAAAAAHinro3oHOcbFZimKroBgAAAAAAQIBkZLsXOY83t5BmiFAKAAAAAAAgQNzrSaWxnlQ1hFIAAAAAAAABUFzm1LqdBZIYKVUTQikAAAAAAIAAWLU9T06XoSNi7UqOCze7nGaHUAoAAAAAACAAMt3rSaUkyGKxmFxN80MoBQAAAAAAEADuM++lMXWvRoRSAAAAAAAAAeAOpQamxJtaR3NFKAUAAAAAAOBnO/OKtSu/RNYQiwZ0iTO7nGaJUAoAAAAAAMDPMitHSR11RIwiw0LNLaaZIpQCAAAAAADws4zsXEnSQNaTqpWpoVS3bt1ksViqfV177bVmlgUAAAAAANAkGVmVZ95LTTC5kubL1PFjS5culdPp9FxevXq1Ro0apQsuuMDEqgAAAAAAABrP4XRp1fY8SYyUqoupoVT79u29Lj/yyCPq2bOnRowYYVJFAAAAAAAATbN+V4FKHC7Fhoeqe1KU2eU0W81mTamysjLNnj1bU6dOlcViMbscAAAAAACARnFP3UtLTVBICBlHbZrN8u8fffSRcnNzdemll9a6T2lpqUpLSz2X8/PzJUkOh0MOhyPQJQac+zG0hsfSUtBzc9D34KPn5qDvwUfPzUHfg4+em4O+Bx89Nwd9b7plW3IkScd2jmlwH1tT3xv6GCyGYRgBrqVBxowZo7CwMH3yySe17jN9+nTNmDGj2vY5c+YoMjIykOUBAAAAAAA0yEMZVu0psejqPk71TWgWsUtQFRUVKT09XXl5eYqNja11v2YRSm3dulU9evTQBx98oLPPPrvW/WoaKZWSkqJ9+/bV+SBbCofDofnz52vUqFGy2Wxml9Mm0HNz0Pfgo+fmoO/BR8/NQd+Dj56bg74HHz03B31vmgNFZRr8z+8lSUvuOkUJkWEN+r7W1Pf8/Hy1a9eu3lCqWUzfe+2119ShQweNHz++zv3sdrvsdnu17TabrcU/YVW1tsfTEtBzc9D34KPn5qDvwUfPzUHfg4+em4O+Bx89Nwd9b5w1uyrWk+rRLkod4nxf5Lw19L2h9Zu+0LnL5dJrr72mSy65RKGhzSIjAwAAAAAAaJTMrFxJUlpqvKl1tASmh1LffPONsrKyNHXqVLNLAQAAAAAAaJKM7FxJ0sCUeFPraAlMH5o0evRoNYNlrQAAAAAAAJrE5TKUmVUxfW9gaoLJ1TR/po+UAgAAAAAAaA3+2HdQ+SXlCreF6KiOMWaX0+wRSgEAAAAAAPhBZuXUvQGd42SzErnUhw4BAAAAAAD4QQZT93xCKAUAAAAAAOAHGZVn3mOR84YhlAIAAAAAAGiiorJyrd9dIImRUg1FKAUAAAAAANBEq7blyeky1DE2XB3jws0up0UglAIAAAAAAGiijMpFzgemxptaR0tCKAUAAAAAANBEhxY5jze3kBaEUAoAAAAAAKAJDMPwLHKelsJ6Ug1FKAUAAAAAANAEO/NKtKegVNYQiwZ0jjO7nBaDUAoAAAAAAKAJ3KOkjk6OUUSY1dxiWhBCKQAAAAAAgCbIzK5cT4qpez4hlAIAAAAAAGiCQ+tJxZtaR0tDKAUAAAAAANBIZeUurdqeJ4kz7/mKUAoAAAAAAKCRftuVr9Jyl+IibOreLsrscloUQikAAAAAAIBGyszOlVQxdc9isZhbTAtDKAUAAAAAANBI7vWkmLrnO0IpAAAAAACARsrIqjzzXipn3vMVoRQAAAAAAEAjHDhYpi37iyRJaV3izS2mBSKUAgAAAAAAaAT3elI92kcpLtJmbjEtEKEUAAAAAABAI3im7qUwda8xCKUAAAAAAAAaIaNypBSLnDcOoRQAAAAAAICPXC7DM30vLSXe1FpaKkIpAAAAAAAAH/2xr1AFJeUKt4WoT8cYs8tpkQilAAAAAAAAfLQ8K1eSdEyXeIVaiVcag64BAAAAAAD4KJP1pJqMUAoAAAAAAMBHGZUjpQaynlSjEUoBAAAAAAD44GBpudbvypckDUxNMLmalotQCgAAAAAAwAcrt+XJZUid4sJ1RGy42eW0WIRSAAAAAAAAPnCvJ5XGelJNQigFAAAAAADgg4ysA5KkgSlM3WsKQikAAAAAAIAGMgxDGZx5zy8IpQAAAAAAABpoR16J9haUKjTEov6d48wup0UjlAIAAAAAAGgg99S9o5NjFW6zmlxNy0YoBQAAAAAA0EAZWbmSmLrnD4RSAAAAAAAADeRZ5JxQqskIpQAAAAAAABqgrNyl1TvyJUlpnHmvyQilAAAAAAAAGmDdznyVlbsUH2lTt6RIs8tp8QilAAAAAAAAGsAzdS8lXhaLxeRqWj5CKQAAAAAAgAbIzM6VJA1MZeqePxBKAQAAAAAANEBGZSiVlhJvah2tBaEUAAAAAABAPfYXlmrr/iJJ0rGEUn5BKAUAAAAAAFAP99S9Xh2iFRdhM7eYVsL0UGr79u266KKLlJSUpIiICA0YMEC//vqr2WUBAAAAAAB4ZDJ1z+9CzbzzAwcOaOjQoTr11FP1xRdfqH379tq4caMSElgwDAAAAAAANB8ZWbmSpIGp8abW0ZqYGko9+uijSklJ0WuvvebZ1r17dxMrAgAAAAAA8OZyGVrhPvNeCgNp/MXUUOp///ufxowZowsuuEALFixQ586ddc011+jKK6+scf/S0lKVlpZ6Lufn50uSHA6HHA5HUGoOJPdjaA2PpaWg5+ag78FHz81B34OPnpuDvgcfPTcHfQ8+em4O+l7dxj2FKigtV2SYVd0T7QHpTWvqe0Mfg8UwDCPAtdQqPDxcknTLLbfoggsu0NKlS3XjjTfqhRde0CWXXFJt/+nTp2vGjBnVts+ZM0eRkZEBrxcAAAAAALQ9v+yxaO7vVvWKNXR9P6fZ5TR7RUVFSk9PV15enmJjY2vdz9RQKiwsTMcff7x++uknz7YbbrhBS5cu1c8//1xt/5pGSqWkpGjfvn11PsiWwuFwaP78+Ro1apRsNlbyDwZ6bg76Hnz03Bz0PfjouTnoe/DRc3PQ9+Cj5+ag79X94+M1eufX7bpqWDfdPvrIgNxHa+p7fn6+2rVrV28oZer0veTkZPXt29dr29FHH63333+/xv3tdrvsdnu17TabrcU/YVW1tsfTEtBzc9D34KPn5qDvwUfPzUHfg4+em4O+Bx89Nwd9P2TFtorlgwZ1Swp4T1pD3xtaf0iA66jT0KFDtX79eq9tGzZsUNeuXU2qCAAAAAAA4JDC0nJt2F0gSRqYEm9uMa2MqaHUzTffrF9++UUPP/ywNm3apDlz5uill17Stddea2ZZAAAAAAAAkqSV23LlMqTO8RHqEBtudjmtiqmh1AknnKAPP/xQc+fOVf/+/fXAAw9o5syZ+vOf/2xmWQAAAAAAAJKkjKxcSVJaarypdbRGpq4pJUlnnnmmzjzzTLPLAAAAAAAAqCYzO1cSU/cCwdSRUgAAAAAAAM2VYRiekVIDGSnld4RSAAAAAAAANdh2oFj7Cktls1rUr1Oc2eW0OoRSAAAAAAAANcionLrXNzlW4TarucW0QoRSAAAAAAAANch0L3LOelIBQSgFAAAAAABQg4zsA5KkgakJJlfSOhFKAQAAAAAAHKa03Kk12/Mlsch5oBBKAQAAAAAAHGbdzgKVOV1KjApTamKk2eW0SoRSAAAAAAAAh8nIqpi6l5YSL4vFYnI1rROhFAAAAAAAwGEyKhc5H8gi5wFDKAUAAAAAAHAYFjkPPEIpAAAAAACAKvYVlio7p1gWi3RMSpzZ5bRahFIAAAAAAABVZFZO3evVPlqx4TZzi2nFCKUAAAAAAACqODR1L97cQlo5QikAAAAAAIAqMrNzJbGeVKARSgEAAAAAAFRyugytyM6TJKVx5r2AIpQCAAAAAACotGlPoQpLyxUZZtWRR8SYXU6rRigFAAAAAABQKSOrYj2pY7vEyxpiMbma1o1QCgAAAAAAoJJ7Pak0FjkPOEIpAAAAAACAShlZuZKkgawnFXCEUgAAAAAAAJIKShzasKdAEiOlgoFQCgAAAAAAQNKqbXkyDKlLQoQ6xISbXU6rRygFAAAAAAAgKcO9nhRT94KCUAoAAAAAAECHzrw3MDXB5EraBkIpAAAAAADQ5hmGcWiRc9aTCgpCKQAAAAAA0OZtO1Cs/QfLZLNa1Dc51uxy2gRCKQAAAAAA0OYtr5y617dTnMJtVpOraRsIpQAAAAAAQJvnmbrHIudBQygFAAAAAADavMzKM++xnlTwEEoBAAAAAIA2rbTcqbU78iVJA1M4816wEEoBAAAAAIA2bc2OfJU5XUqKClNKYoTZ5bQZhFIAAAAAAKBN86wnlRovi8VibjFtCKEUAAAAAABo09zrSaWxyHlQEUoBAAAAAIA2LSPrgCRpYCrrSQUToRQAAAAAAGiz9haUatuBYlks0jFd4swup00hlAIAAAAAAG2We+rekR1iFBNuM7eYNoZQCgAAAAAAtFnuqXusJxV8hFIAAAAAAKDNqnrmPQQXoRQAAAAAAGiTnC5DK7flSmKRczMQSgEAAAAAgDZp454CHSxzKirMql4dos0up80hlAIAAAAAAG2Se+resSnxsoZYzC2mDSKUAgAAAAAAbZJ7kXPWkzIHoRQAAAAAAGiTMrNzJUkDU1hPygymhlLTp0+XxWLx+urTp4+ZJQEAAAAAgDYgv8ShjXsKJUlpjJQyRajZBfTr10/ffPON53JoqOklAQAAAACAVm5ldp4MQ0pJjFC7aLvZ5bRJpidAoaGh6tixo9llAAAAAACANsSznhRT90xjeii1ceNGderUSeHh4TrxxBP1z3/+U6mpqTXuW1paqtLSUs/l/Px8SZLD4ZDD4QhKvYHkfgyt4bG0FPTcHPQ9+Oi5Oeh78NFzc9D34KPn5qDvwUfPzdFW+r48K0eSNKBzTLN4rK2p7w19DBbDMIwA11KrL774QoWFhTrqqKO0c+dOzZgxQ9u3b9fq1asVExNTbf/p06drxowZ1bbPmTNHkZGRwSgZAAAAAAC0cIYh3f2rVQfLLbq5f7m6VY8g0ARFRUVKT09XXl6eYmNja93P1FDqcLm5uerataueeuopXX755dWur2mkVEpKivbt21fng2wpHA6H5s+fr1GjRslms5ldTptAz81B34OPnpuDvgcfPTcHfQ8+em4O+h589NwcbaHvW3OKdPrTi2SzWpTxj5Gyh5p6HjhJravv+fn5ateuXb2hlOnT96qKj4/XkUceqU2bNtV4vd1ul91effExm83W4p+wqlrb42kJ6Lk56Hvw0XNz0Pfgo+fmoO/BR8/NQd+Dj56bozX3fc3OirPu9e8cp+iI5rXIeWvoe0PrNz8KrKKwsFC///67kpOTzS4FAAAAAAC0UhlZuZKktJR4U+to6xoVSuXm5uqVV17RXXfdpZycioXBli9fru3bt/t0O7fddpsWLFigLVu26KefftK5554rq9WqKVOmNKYsAAAAAACAennOvJfKmffM5PP0vZUrV+r0009XXFyctmzZoiuvvFKJiYn64IMPlJWVpTfeeKPBt7Vt2zZNmTJF+/fvV/v27XXyySfrl19+Ufv27X0tCwAAAAAAoF4lDqfW7syXJA1kpJSpfA6lbrnlFl166aV67LHHvM6Qd8YZZyg9Pd2n23r77bd9vXsAAAAAAIBGW7MjXw6noXbRYeqSEGF2OW2az9P3li5dqquvvrra9s6dO2vXrl1+KQoAAAAAACAQ3FP30lISZLFYTK6mbfM5lLLb7crPz6+2fcOGDUy7AwAAAAAAzVpGdq4kaWBqvKl1oBGh1FlnnaX7779fDodDkmSxWJSVlaU77rhD5513nt8LBAAAAAAA8JfMyjPvEUqZz+dQ6sknn1RhYaE6dOig4uJijRgxQr169VJMTIweeuihQNQIAAAAAADQZHvyS7Q9t1gWi3RMl3izy2nzfF7oPC4uTvPnz9ePP/6oFStWqLCwUMcdd5xOP/30QNQHAAAAAADgF+6pe0cdEaNou8+RCPys0c/A0KFDNXToUH/WAgAAAAAAEDAZTN1rVnyevnfDDTfo2Wefrbb9ueee00033eSPmgAAAAAAAPwuM9t95r14cwuBpEaEUu+//36NI6ROOukkvffee34pCgAAAAAAwJ/KnS6t3JYnSRqYmmByNZAaEUrt379fcXFx1bbHxsZq3759fikKAAAAAADAnzbsLlRRmVMx9lD1ah9tdjlQI0KpXr166csvv6y2/YsvvlCPHj38UhQAAAAAAIA/ZVYucn5sSrxCQizmFgNJjVjo/JZbbtF1112nvXv36rTTTpMkffvtt3ryySc1c+ZMf9cHAAAAAADQZBlZrCfV3PgcSk2dOlWlpaV66KGH9MADD0iSunXrplmzZuniiy/2e4EAAAAAAABNlVE5Uooz7zUfPodSkjRt2jRNmzZNe/fuVUREhKKjmYsJAAAAAACap7xihzbtKZTESKnmpFGhlFv79u39VQcAAAAAAEBArNyWK0lKTYxUUrTd3GLg4fNC57t379Zf/vIXderUSaGhobJarV5fAAAAAAAAzUlGVq4kpu41Nz6PlLr00kuVlZWle+65R8nJybJYWLEeAAAAAAA0X+5Fzgcyda9Z8TmUWrRokRYuXKi0tLQAlAMAAAAAAOA/hmEo07PIeYK5xcCLz9P3UlJSZBhGIGoBAAAAAADwq637i3SgyKGw0BAdnRxrdjmowudQaubMmbrzzju1ZcuWAJQDAAAAAADgPxnZFVP3+neKVViozzEIAsjn6XsXXnihioqK1LNnT0VGRspms3ldn5OT47fiAAAAAAAAmuLQIudM3WtufA6lZs6cGYAyAAAAAAAA/M+9nlQai5w3Oz6HUpdcckkg6gAAAAAAAPCrEodTa3fkS5IGpsabWwyqadRkyt9//13/+Mc/NGXKFO3Zs0eS9MUXX2jNmjV+LQ4AAAAAAKCxVm/PU7nLUPsYuzrHR5hdDg7jcyi1YMECDRgwQIsXL9YHH3ygwsJCSdKKFSt03333+b1AAAAAAACAxnBP3RuYEi+LxWJuMajG51Dqzjvv1IMPPqj58+crLCzMs/20007TL7/84tfiAAAAAAAAGsu9yHkaU/eaJZ9DqVWrVuncc8+ttr1Dhw7at2+fX4oCAAAAAABoqoysA5KkgSmcea858jmUio+P186dO6ttz8jIUOfOnf1SFAAAAAAAQFPszi/RjrwShVikY7rEmV0OauBzKDV58mTdcccd2rVrlywWi1wul3788UfddtttuvjiiwNRIwAAAAAAgE/cU/eOPCJGUfZQc4tBjXwOpR5++GH16dNHKSkpKiwsVN++fTV8+HCddNJJ+sc//hGIGgEAAAAAAHySkV05dS+VqXvNlc9RYVhYmF5++WXdc889Wr16tQoLCzVw4ED17t07EPUBAAAAAAD4zD1SaiCLnDdbjR6/lpqaqtTUVH/WAgAAAAAA0GTlTpdWbcuTJB1HKNVs+RxK3XLLLTVut1gsCg8PV69evXT22WcrMTGxycUBAAAAAAD4av3uAhU7nIoJD1WPdtFml4Na+BxKZWRkaPny5XI6nTrqqKMkSRs2bJDValWfPn30/PPP69Zbb9WiRYvUt29fvxcMAAAAAABQF/fUvbSUeIWEWMwtBrXyeaHzs88+W6effrp27NihZcuWadmyZdq2bZtGjRqlKVOmaPv27Ro+fLhuvvnmQNQLAAAAAABQJ896UinxptaBuvkcSj3++ON64IEHFBsb69kWFxen6dOn67HHHlNkZKTuvfdeLVu2zK+FAgAAAAAANERm5Zn30lhPqlnzOZTKy8vTnj17qm3fu3ev8vPzJUnx8fEqKytrenUAAAAAAAA+yCty6Pe9ByVJaSkJJleDujRq+t7UqVP14Ycfatu2bdq2bZs+/PBDXX755TrnnHMkSUuWLNGRRx7p71oBAAAAAADqlLktV5LULSlSiVFh5haDOvm80PmLL76om2++WZMnT1Z5eXnFjYSG6pJLLtHTTz8tSerTp49eeeUV/1YKAAAAAABQj0z3elKpjJJq7nwKpZxOp5YvX67HHntMTz/9tP744w9JUo8ePRQdfegUi2lpaX4tEgAAAAAAoCEy3OtJsch5s+dTKGW1WjV69GitW7dO3bt31zHHHBOougAAAAAAAHxiGMahM++xyHmz5/OaUv379/eMkAIAAAAAAGguNu87qLxih+yhIerTMdbsclAPn0OpBx98ULfddps+/fRT7dy5U/n5+V5fAAAAAAAAZsjMzpUk9e8cp7BQnyMPBJnPz9AZZ5yhFStW6KyzzlKXLl2UkJCghIQExcfHKyGh8YuIPfLII7JYLLrpppsafRsAAAAAAKDt8kzdYz2pFsHns+999913fi9i6dKlevHFF1mjCgAAAAAANJp7kXPOvNcy+BxKjRgxwq8FFBYW6s9//rNefvllPfjgg369bQAAAAAA0DYUlzn1284CSSxy3lI0aoLlwoULddFFF+mkk07S9u3bJUlvvvmmFi1a5PNtXXvttRo/frxOP/30xpQCAAAAAACg1TvyVO4y1CHGruS4cLPLQQP4PFLq/fff11/+8hf9+c9/1vLly1VaWipJysvL08MPP6zPP/+8wbf19ttva/ny5Vq6dGmD9i8tLfXcnyTPwuoOh0MOh8OHR9E8uR9Da3gsLQU9Nwd9Dz56bg76Hnz03Bz0PfjouTnoe/DRc3O01L7/unm/JOnYLnEqLy83uRrftdS+16Shj8FiGIbhyw0PHDhQN998sy6++GLFxMRoxYoV6tGjhzIyMjRu3Djt2rWrQbeTnZ2t448/XvPnz/esJXXKKacoLS1NM2fOrPF7pk+frhkzZlTbPmfOHEVGRvryMAAAAAAAQCvy6voQrcgJ0VmpTo3s7FPUAT8rKipSenq68vLyFBsbW+t+PodSkZGRWrt2rbp16+YVSv3xxx/q27evSkpKGnQ7H330kc4991xZrVbPNqfTKYvFopCQEJWWlnpdJ9U8UiolJUX79u2r80G2FA6HQ/Pnz9eoUaNks9nMLqdNoOfmoO/BR8/NQd+Dj56bg74HHz03B30PPnpujpba92GPL9Cu/FLNnnq8hnRPNLscn7XUvtckPz9f7dq1qzeU8nn6XseOHbVp0yZ169bNa/uiRYvUo0ePBt/OyJEjtWrVKq9tl112mfr06aM77rijWiAlSXa7XXa7vdp2m83W4p+wqlrb42kJ6Lk56Hvw0XNz0Pfgo+fmoO/BR8/NQd+Dj56boyX1fWdesXbllyrEIh3XLUk2m89xR7PRkvpem4bW7/OzdOWVV+rGG2/Uq6++KovFoh07dujnn3/WbbfdpnvuuafBtxMTE6P+/ft7bYuKilJSUlK17QAAAAAAALXJzMqVJPXpGKvIsJYbSLU1Pj9Td955p1wul0aOHKmioiINHz5cdrtdt912m66//vpA1AgAAAAAAFCrzOxcSdLA1HhT64BvfA6lLBaL7r77bt1+++3atGmTCgsL1bdvX0VHRze5mO+//77JtwEAAAAAANqWjMqRUmkp8abWAd+E+PoNs2fPVlFRkcLCwtS3b18NHjzYL4EUAAAAAACArxxOl1Zuz5UkDUxNMLcY+MTnUOrmm29Whw4dlJ6ers8//1xOpzMQdQEAAAAAANRr/a4ClThcig0PVY92UWaXAx/4HErt3LlTb7/9tiwWiyZNmqTk5GRde+21+umnnwJRHwAAAAAAQK0yKteTOjYlXiEhFnOLgU98DqVCQ0N15pln6q233tKePXv09NNPa8uWLTr11FPVs2fPQNQIAAAAAABQo4ysA5KYutcSNek8iZGRkRozZowOHDigrVu3at26df6qCwAAAAAAoF6ZlYucc+a9lsfnkVKSVFRUpLfeektnnHGGOnfurJkzZ+rcc8/VmjVr/F0fAAAAAABAjXKLyvTHvoOSpLQu8eYWA5/5PFJq8uTJ+vTTTxUZGalJkybpnnvu0YknnhiI2gAAAAAAAGqVWbmeVPd2UUqICjO3GPjM51DKarVq3rx5GjNmjKxWayBqAgAAAAAAqFeGe+peSrypdaBxfA6l3nrrrUDUAQAAAAAA4BP3mfdYT6plalAo9eyzz+qqq65SeHi4nn322Tr3veGGG/xSGAAAAAAAQG1cLkMrKkOptBTOvNcSNSiUevrpp/XnP/9Z4eHhevrpp2vdz2KxEEoBAAAAAICA27z/oPKKHbKHhqhPcozZ5aARGhRKbd68ucb/AwAAAAAAmMG9ntQxXeJks4aYWwwahWcNAAAAAAC0OJnZByRJA1OZutdS+RRKHTx4UPfee6/69++v6OhoxcTE6JhjjtH999+voqKiQNUIAAAAAADgxT1SKo0z77VYDT77XllZmUaMGKHVq1dr3LhxmjBhggzD0Lp16/TQQw/piy++0A8//CCbzRbIegEAAAAAQBtXVFau33YVSOLMey1Zg0OpWbNmadu2bVqxYoWOOuoor+t+++03nXLKKXrhhRd0/fXX+71IAAAAAAAAt1Xb8uR0GeoYG67kuAizy0EjNXj63gcffKB77rmnWiAlSX369NHdd9+t9957z6/FAQAAAAAAHC4zO1cSU/daugaHUmvXrtUpp5xS6/Wnnnqq1q5d64+aAAAAAAAAauVeT4qpey1bg0Op3NxcJSUl1Xp9UlKS8vLy/FIUAAAAAABAbTI4816r0OBQyuVyyWq11n5DISFyOp1+KQoAAAAAAKAmO/OKtTu/VNYQiwZ0jjO7HDRBgxc6NwxDI0eOVGhozd9SXl7ut6IAAAAAAABq4p6616djjCLCah88g+avwaHUfffdV+8+5513XpOKAQAAAAAAqEtGlnvqXry5haDJ/BpKAQAAAAAABJJnkfMU1pNq6Rq8phQAAAAAAICZHE6XVm2vOMlaGiOlWjxCKQAAAAAA0CL8trNApeUuxUXY1D0pyuxy0ESEUgAAAAAAoEXIyK5YTyotJV4hIRaTq0FTEUoBAAAAAIAWIdO9nhRT91oFv4VS27Zt01VXXeWvmwMAAAAAAPCSkZ0rqWKkFFo+v4VS+/fv13/+8x9/3RwAAAAAAIDHgYNl2rzvoCRCqdaC6XsAAAAAAKDZy6wcJdWjfZTiI8PMLQZ+QSgFAAAAAACaPabutT6EUgAAAAAAoNnLyKo4897A1ASTK4G/hDZ0x4kTJ9Z5fW5ublNrAQAAAAAAqMblMjzT9wYyUqrVaHAoFRcXV+/1F198cZMLAgAAAAAAqOqPfQdVUFKucFuI+nSMMbsc+EmDQ6nXXnut3n0KCwubVAwAAAAAAMDh3FP3jukcr1ArKxG1Fg1+Jp9++uk6ry8oKNCYMWOaXBAAAAAAAEBV7kXOB6bGm1oH/KvBodTf//53vfHGGzVeV1hYqLFjx2r//v1+KwwAAAAAAECSMrJyJRFKtTYNDqXefPNNXX311frf//7ntf3gwYMaO3as9u7dq++++87vBQIAAAAAgLarqKxc63flS5LSUjjzXmvS4DWlzj//fOXm5mrKlCn67LPPdMopp3gCqd27d2vBggVKTk4OZK0AAAAAAKCNWbktTy5DSo4LV8e4cLPLgR81OJSSpCuuuEI5OTk6++yz9fHHH+vee+/Vjh07tGDBAnXq1ClQNQIAAAAAgDaKqXutl0+hlCT97W9/U05OjkaOHKlu3brp+++/V5cuXQJRGwAAAAAAaOMysyvOvDeQqXutToNDqYkTJ3pdttlsateunW688Uav7R988IF/KgMAAAAAAG2aYRhaXjlSKo2RUq1Og0OpuLg4r8tTpkzxezEAAAAAAABuO/JKtLegVKEhFvXvFFf/N6BFaXAo9dprr/n9zmfNmqVZs2Zpy5YtkqR+/frp3nvv1bhx4/x+XwAAAAAAoGXJyKqYund0cqwiwqwmVwN/CzHzzrt06aJHHnlEy5Yt06+//qrTTjtNZ599ttasWWNmWQAAAAAAoBnIdE/dS4k3tQ4Ehs8LnfvThAkTvC4/9NBDmjVrln755Rf169fPpKoAAAAAAEBzkJGdK4kz77VWpoZSVTmdTr377rs6ePCgTjzxxBr3KS0tVWlpqedyfn6+JMnhcMjhcASlzkByP4bW8FhaCnpuDvoefPTcHPQ9+Oi5Oeh78NFzc9D34KPn5mgufS8rd2nV9jxJ0oBO0abXE2jNpe/+0NDHYDEMwwhwLXVatWqVTjzxRJWUlCg6Olpz5szRGWecUeO+06dP14wZM6ptnzNnjiIjIwNdKgAAAAAACJKsQunJVaGKDDX08PFOWSxmV4SGKioqUnp6uvLy8hQbG1vrfqaHUmVlZcrKylJeXp7ee+89vfLKK1qwYIH69u1bbd+aRkqlpKRo3759dT7IlsLhcGj+/PkaNWqUbDab2eW0CfTcHPQ9+Oi5Oeh78NFzc9D34KPn5qDvwUfPzdFc+v7mL1m6/7PfNKJ3O71y8XGm1REszaXv/pCfn6927drVG0qZPn0vLCxMvXr1kiQNGjRIS5cu1TPPPKMXX3yx2r52u112u73adpvN1uKfsKpa2+NpCei5Oeh78NFzc9D34KPn5qDvwUfPzUHfg4+em8Psvq/cXrFkz3FdE9vU82923/2hofWbeva9mrhcLq/RUAAAAAAAoO1hkfPWz9SRUnfddZfGjRun1NRUFRQUaM6cOfr+++/11VdfmVkWAAAAAAAwUc7BMm3dXyRJOjYl3txiEDCmhlJ79uzRxRdfrJ07dyouLk7HHHOMvvrqK40aNcrMsgAAAAAAgIkysw9Iknq2j1JcRMueyobamRpK/ec//zHz7gEAAAAAQDOUkZUrSRqYmmBuIQioZremFAAAAAAAaNsyWU+qTSCUAgAAAAAAzYbLZSizcqRUGutJtWqEUgAAAAAAoNn4fW+hCkrLFWGz6qgjYswuBwFEKAUAAAAAAJoN93pSx3SJU6iV2KI149kFAAAAAADNRkblelJprCfV6hFKAQAAAACAZiMj64AkaWAKZ95r7QilAAAAAABAs3CwtFwbdhdI4sx7bQGhFAAAAAAAaBZWbsuTy5A6x0foiNhws8tBgBFKAQAAAACAZiEju2LqXlpKvLmFICgIpQAAAAAAQLPgPvMeU/faBkIpAAAAAABgOsMwCKXaGEIpAAAAAABguu25xdpXWKrQEIv6dYozuxwEAaEUAAAAAAAwnXuUVN9OsQq3Wc0tBkFBKAUAAAAAAEznmbrHIudtBqEUAAAAAAAwXWblmfcGpiaYXAmChVAKAAAAAACYqrTcqdU78iVJaYyUajMIpQAAAAAAgKnW7SxQWblLCZE2dU2KNLscBAmhFAAAAAAAMFVG1qGpexaLxeRqECyEUgAAAAAAwFSZ2bmSmLrX1hBKAQAAAAAAU3nOvJcab2odCC5CKQAAAAAAYJr9haXKyimSxSIdy0ipNoVQCgAAAAAAmMY9da9X+2jFhtvMLQZBRSgFAAAAAABM4566x3pSbQ+hFAAAAAAAME1G9qEz76FtIZQCAAAAAACmcLoMrcjOk8Qi520RoRQAAAAAADDF73sLVVharsgwq448IsbschBkhFIAAAAAAMAUGVkVU/eO6RIna4jF5GoQbIRSAAAAAADAFO5FzllPqm0ilAIAAAAAAKbIzM6VJA3kzHttEqEUAAAAAAAIusLScq3fXSBJSmOR8zaJUAoAAAAAAATdyuxcGYbUOT5CHWLCzS4HJiCUAgAAAAAAQZfhnrrHKKk2i1AKAAAAAAAEnXuR8zTWk2qzCKUAAAAAAEBQGYahzOwDkjjzXltGKAUAAAAAAIJq24Fi7Sssk81qUb9OsWaXA5MQSgEAAAAAgKByryfVt1Ocwm1Wc4uBaQilAAAAAABAUGVkVU7dYz2pNo1QCgAAAAAABJV7kXPOvNe2EUoBAAAAAICgKS13au2OfEnSwBQWOW/LCKUAAAAAAEDQrN2RrzKnS4lRYUpJjDC7HJiIUAoAAAAAAASNZ+peSrwsFou5xcBUhFIAAAAAACBo3GfeYz0pmBpK/fOf/9QJJ5ygmJgYdejQQeecc47Wr19vZkkAAAAAACCAMrMrz7yXynpSbZ2podSCBQt07bXX6pdfftH8+fPlcDg0evRoHTx40MyyAAAAAABAAOwtKFV2TrEsFumYLnFmlwOThZp5519++aXX5ddff10dOnTQsmXLNHz4cJOqAgAAAAAAgZBZOXWvd4doxYTbzC0GpmtWa0rl5eVJkhITE02uBAAAAAAA+FtGVuXUvRSm7sHkkVJVuVwu3XTTTRo6dKj69+9f4z6lpaUqLS31XM7Pz5ckORwOORyOoNQZSO7H0BoeS0tBz81B34OPnpuDvgcfPTcHfQ8+em4O+h589Nwcgey7O5Qa0DmG5/Uwren13tDHYDEMwwhwLQ0ybdo0ffHFF1q0aJG6dOlS4z7Tp0/XjBkzqm2fM2eOIiMjA10iAAAAAABoJJch3bnEqlKXRXccU65OUWZXhEApKipSenq68vLyFBsbW+t+zSKUuu666/Txxx/rhx9+UPfu3Wvdr6aRUikpKdq3b1+dD7KlcDgcmj9/vkaNGiWbjbm1wUDPzUHfg4+em4O+Bx89Nwd9Dz56bg76Hnz03ByB6vv6XQU6898/KyrMqmV3nyZriMVvt90atKbXe35+vtq1a1dvKGXq9D3DMHT99dfrww8/1Pfff19nICVJdrtddru92nabzdbin7CqWtvjaQnouTnoe/DRc3PQ9+Cj5+ag78FHz81B34OPnpvD331ftbNQknRsSrzC7WF+u93WpjW83htav6mh1LXXXqs5c+bo448/VkxMjHbt2iVJiouLU0REhJmlAQAAAAAAP8rMypUkpaXEm1oHmg9Tz743a9Ys5eXl6ZRTTlFycrLn65133jGzLAAAAAAA4GcZ2ZVn3kvlzHuoYPr0PQAAAAAA0LoVlDi0cU/F9D1GSsHN1JFSAAAAAACg9Vu5LU+GIXVJiFD7mOprRaNtIpQCAAAAAAABlZHF1D1URygFAAAAAAACKqNykfOBTN1DFYRSAAAAAAAgYAzDUEZ2riRpYGq8qbWgeSGUAgAAAAAAAZOdU6ycg2UKs4aob6dYs8tBM0IoBQAAAAAAAiYju2I9qb6dYmUPtZpcDZoTQikAAAAAABAwnvWkmLqHwxBKAQAAAACAgHGvJ5XGIuc4DKEUAAAAAAAIiBKHU2t35EmSjktNMLkaNDeEUgAAAAAAICDW7MiXw2moXXSYuiREmF0OmhlCKQAAAAAAEBAZWRWLnKelJMhisZhcDZobQikAAAAAABAQmZXrSbHIOWpCKAUAAAAAAALCc+Y9FjlHDQilAAAAAACA3+0pKNH23GJZLNIxhFKoAaEUAAAAAADwu8zKUVJHdohRtD3U3GLQLBFKAQAAAAAAv8tgPSnUg1AKAAAAAAD4nfvMe4RSqA2hFAAAAAAA8Cuny9DKbXmSpIGpCSZXg+aKUAoAAAAAAPjVht0FKipzKtoeqp7to80uB80UoRQAAAAAAPCrjMpFzo9NiZM1xGJuMWi2CKUAAAAAAIBfedaTSmHqHmpHKAUAAAAAAPwqs/LMe2kp8abWgeaNUAoAAAAAAPhNXrFDG/cUSpLSOPMe6kAoBQAAAAAA/GbltlxJUmpipNpF280tBs0aoRQAAAAAAPAb9yLnAxklhXoQSgEAAAAAAL9hPSk0FKEUAAAAAADwC8MwDp15L5Uz76FuhFIAAAAAAMAvtu4v0oEih8JCQ9Q3OdbsctDMEUoBAAAAAAC/cE/d69cpVmGhRA6oG68QAAAAAADgF56peylM3UP9CKUAAAAAAIBfZFSOlOLMe2gIQikAAAAAANBkJQ6n1u7Il0QohYYhlAIAAAAAAE22Zkeeyl2G2kXb1Tk+wuxy0AIQSgEAAAAAgCbLyMqVVDFKymKxmFsMWgRCKQAAAAAA0GRVQymgIQilAAAAAABAk2VWLnKelhJvah1oOQilAAAAAABAk+zOL9H23GKFWKRjusSbXQ5aCEIpAAAAAADQJO6pe0ceEaNoe6i5xaDFIJQCAAAAAABNkpF9QJI0MDXB5ErQkhBKAQAAAACAJsl0L3LOelLwAaEUAAAAAABotHKnSyu35UnizHvwDaEUAAAAAABotPW7C1TscCrGHqqe7aPNLgctCKEUAAAAAABotMzsXEnSsSnxCgmxmFsMWhRTQ6kffvhBEyZMUKdOnWSxWPTRRx+ZWQ4AAAAAAPCR+8x7TN2Dr0wNpQ4ePKhjjz1W//73v80sAwAAAAAANFJGlvvMe/HmFoIWJ9TMOx83bpzGjRtnZgkAAAAAAKCR8ooc+n3vQUlSWkqCydWgpTE1lPJVaWmpSktLPZfz8/MlSQ6HQw6Hw6yy/Mb9GFrDY2kp6Lk56Hvw0XNz0Pfgo+fmoO/BR8/NQd+Dj56bw5e+L9u6T5KUmhihmDALz1UTtKbXe0Mfg8UwDCPAtTSIxWLRhx9+qHPOOafWfaZPn64ZM2ZU2z5nzhxFRkYGsDoAAAAAAHC4L7Mt+mKbVYPauXRxb5fZ5aCZKCoqUnp6uvLy8hQbG1vrfi1qpNRdd92lW265xXM5Pz9fKSkpGj16dJ0PsqVwOByaP3++Ro0aJZvNZnY5bQI9Nwd9Dz56bg76Hnz03Bz0PfjouTnoe/DRc3P40vcP3lguaZ/GD+mrM/6UGpwCW6nW9Hp3z2yrT4sKpex2u+x2e7XtNputxT9hVbW2x9MS0HNz0Pfgo+fmoO/BR8/NQd+Dj56bg74HHz03R319NwxDK7bnSZIGdUviOfKT1vB6b2j9pp59DwAAAAAAtExb9hcpt8ihsNAQHZ3c8mcvIfhMHSlVWFioTZs2eS5v3rxZmZmZSkxMVGoqw/4AAAAAAGiuMrIOSJIGdI5TWChjXuA7U0OpX3/9Vaeeeqrnsnu9qEsuuUSvv/66SVUBAAAAAID6ZGTlSpIGpsSbWgdaLlNDqVNOOUXN5OR/AAAAAADAB5nZuZKktNR4U+tAy8X4OgAAAAAA4JPiMqfW7aw4w9rA1ASTq0FLRSgFAAAAAAB8snpHnspdhjrE2NUpLtzsctBCEUoBAAAAAACfZFauJ5WWEi+LxWJuMWixCKUAAAAAAIBPMrIrzrzH1D00BaEUAAAAAADwiefMeyxyjiYglAIAAAAANDtOl6HFm3O0bJ9FizfnyOnizO3Nxa68Eu3MK1GIRTqmS5zZ5aAFCzW7AAAAAAAAqvpy9U7N+GStduaVSLLqjY2/KjkuXPdN6Kux/ZPNLq/Ny6ycundUx1hFhhEroPEYKQUAAAAAaDa+XL1T02YvrwykDtmVV6Jps5fry9U7TaoMbkzdg78QSgEAAAAAmoWisnLd8/Ea1TRRz71txidrmcpnMk8olRJvah1o+RhnBwAAAAAIGJfLUG6xQ3sLSrW3oFR7Cko8/99bWKo9+RX/7i0oVV6xo87bMiTtzCvRks05OrFnUnAeALyUO11auT1XEiOl0HSEUgAAAAAAn5U4nJUhU6n2Vgma9lQJnPYWlGpfYakcTv+ObHr++02Kj7Tp6ORYv94u6vfbrgKVOFyKCQ9Vj3bRZpeDFo5QCgAAAAAgqWJUU05RmdfopZpGN+3NL1VBablPt50QaVP7GLs6xISrfYy94ivarg6xFf+2j7Fry/6DuvKNZfXe1sKN+zTumYVKS4nXlMEpOvOYToqy8+ttMGRk50qS0lLiFRJiMbcYtHi8awEAAACglSsqK68zaHKPbtp/sMyn9ZrsoSFeoZJX6FS5rUOsXUlRdoWF1r+kcY/20UqOC9euvJIa15WySEqICtOQ7gmav3aPMrNzlZmdqwc+Xaez0jopfXCq+neOa3hj4LOMrIoz7w1MTTC5ErQGhFIAAAAAUA+ny9DizTlats+ipM05OrFXB1lNHiXidBnaX1gZKFUJm2oKnQ6WORt8uxaLlBQVpnY1BU0xdnWIObQ9xh4qi8V/fbCGWHTfhL6aNnu5LJJXMOW+l4fP7a+x/ZO1t6BU7y/fpreXZGnL/iLNWZylOYuzNKBznCYPTtFZx3ZSTLjNb7WhQmblSCkWOYc/EEoBAAAAQB2+XL1TMz5Zq515JZKsemPjr0qOC9d9E/pqbP9kv96XYRgqLC2vdX2mPVVCp5yDpfLlJHQRNqtnVFOto5ti7EqMCpPNat6J2sf2T9asi46r0vMKHQ/refsYu/46oqeuHt5DP/+xX28vydaXq3dp1fY8rfowTw99tk4TjumkyYNTlJYS79fwrK3KLSrTH3sPSqqYvgc0FaEUAAAAANTiy9U7NW328mpTyXbllWja7OWaddFxDQqmHE6X9heW1Xv2ub0FpSp2NHxUU4hFSoquHjRVjGbyHt3UktZcGts/WaP6dtTPm/bo64WLNXrYkFpHp1ksFp3Us51O6tlOOQfL9MHybZq7JEu/7z2od37N1ju/ZqtPxxilD0nV2WmdFRfB6KnGco+S6t4uSglRYeYWg1ah5RyVAAAAACCInC5DMz5ZW+PaRoYqppPd97816t4uSvsPllWbPld1dFPOwTKf7jvaHqoOMXa1q2VBcPfopsSoMNOnEQaKNcSiId0TtX+doSHdExv0OBOjwnTFsB66/OTu+nXrAc1dnKXPVu3Ub7sKdO/Ha/Tw5+s0fkAnTRmcokFdExg95aPMKoucA/5AKAUAAAC0IM1xbaOWwDAMOZyGSsqdKilzqsThUkm5U8VlTpU4nCp2VGwrrbJt/a4Cr+lj1W5T0u78Uo2ZubBBNYSGWA5bp8lew1nowtUuJkyRYfyq1hQWi0UndEvUCd0Sdd+EfvowY5vmLsnW+t0Fen/5Nr2/fJt6d4jW5MGpmjiwM6N+GigjK1eSNDA13tQ60HpwpAMAAABaiGCubRQMhmGozOlSSVmVgKi8Ihzy/L/y3+Iyl0ocVbe5POGR+/+lVW6juOywkKnc5dNZ5XwRYQtRp/iIamszeYKmyv8nRIYphAAx6OIibbp0aHddclI3ZWTn6u0lWfpkxU5t3FOoBz5dq0e//E3j+nfUlMGpGtI9kdFTtXC5jCqLnHPmPfgHoRQAAADQAvhrbaP6GIah0nKX1+ihQ2GR+8vlFf549q8aIFUJlKqHTIdu3whMTlQni6Vi0e9wm1URNqvsthCvy+G2ENltVhWUOPTDhn313t6rlw7WiT2TglA5msJisei41AQdl5qgf5zZV//L3KG5S7K0Zke+Ps7coY8zd6hHuyhNHpyi847roqRou9klNyub9x9UXrFD9tAQ9UmOMbsctBKEUgAAAEAzV9/aRpJ05werlF9SrtJyl0odh406ctQcKJU4XJ7txVWuN0NIZVAUEWaVPbTi33BbiMJDD9sWGlJ5XcX/w8Osnn3CK8MluydcOrQtvMrlMGtIg0bDOF2GTn70/7Qrr6TG3ltUcUa4wd0T/d4PBFZsuE0X/amrLvpTV63alqc5S7L0v8zt+mPfQT38+W96/Kv1Gt2vo6ackKqTeiYxwk1SZuXUvQGd40w9OyNaF0IpAAAAoJkpcTiVnVOk7ANFys4p1uI/9te5tpEk5RY59Lf3Vvq1jtAQy6GQJ6wiIKprdFF4lfAnovKyO1CqGg5FVAZJ4WGV3xNqlc1qaXbTpqwhFt03oa+mzV4ui+QVTLkrvW9CX9b0auEGdInTP7sM0D/GH61PVuzQ3KXZWpGdq89W7tRnK3cqNTFSF56QoguO76IOMeFml2uajOwDklhPCv5FKAUAAAAEmdNlaFd+ibJzipSVU6Rtlf9mHyhWVk6R9haUNup2j+4Yo9SkyCpBUU2jhUKqBEe1hUoV/2c0hDS2f7JmXXRclbW8KnRswWt5oWZR9lBNHpyqyYNTtWZHnt5ekq2PMrYrK6dIj3+1Xk/P36CRR3fQ5MGpGt67fZsLIw8tcs56UvAfQikAAADAzwzDUF6xoyJoyimuDJyKKkY/5RRpe26xHM66F1OKtocqJTFSKQkRCg2x6PPVu+q933sn9GNtowAY2z9Zo/p21M+b9ujrhYs1etgQznrYyvXrFKcHzonTXWf00Wcrd+rtpdlatvWAvlqzW1+t2a3O8RG68IQUTTo+RR3jWv/oqeIyp37bVSBJSkuJN7cYtCqEUgAAAEAjlDic2naguMo0uyJPCJWdU6SC0vI6vz80xKLOCRFKTYxUl4RIpSZGKiWx4nJKQqTiI22e6WxOl6EM1jYylTXEoiHdE7V/naEh3RMJpNqIyLBQXXB8ii44PkUbdhdo7pIsfbB8u7bnFuup+Rs085sNOq1PB00+IVWnHNVeoa10dOGq7XlyugwdEWtXchsI4RA8hFIAAABADVxVpti5p9UdmmZXpN359U+xax9jV0pl8JTi/kqIVGpSpDrGhjc42GBtI8B8Rx4Ro/sm9NMdY/voy9W7NGdJlpZsztE36/bom3V71DE2XJOO76JJJ6SoS0Kk2eX6VUZW5XpSKQnNbu03tGyEUgAAAGiz8oocyj7gHuF0aF2n7JwibT9QrDJn3WeiiwqzesKm1Mqpdu7/d0mIVESY1W+1srYR0DyE26w6Z2BnnTOwszbtKdQ7S7P0/vLt2pVfomf/b5P+9d0mDe/dXlMGp2rk0R1axdpsh9aTije1DrQ+hFIAAABotUocTm3PLfas5ZR9oFhZ+4s8QVRBSf1T7DrFR3im1h2aZlfxb0KVKXbBwNpGQPPSq0O07h7fV7eNOUpfr9mtt5dm6cdN+7Vgw14t2LBX7WPsumBQF00+IVWpSS139FRmdq4k1pOC/xFKAQAAoMVyuQztKSg9bKRTkbZVLi6+u6BERt3riatdtF0piREV0+oqwyf3NLvkuPBmt0YMaxsBzY891KoJx3bShGM7acu+g3rn12y9++s27S0o1fPf/67nv/9dJ/dqp8mDUzS6b0eFhTav40pdduYVa1d+iawhFg3oEmd2OWhlCKUAAADQaE6XocWbc7Rsn0VJm3MCMmonr9hRZaST99nsth0oVll53VPsIsOsSklwr+l0aCHx1KRIdUmIUGQYH4kB+E+3dlG6Y2wf3TLqSH27brfmLMnWwo17tWjTPi3atE9JUWE6b1AXTT4hRT3aR5tdbr3cU/f6dIzheAm/4xUFAACARvly9c4q6xtZ9cbGX5XciPWNyspd2p5b7Bnt5A6f3Geyyyt21Pn91hCLOsWHVxnp5F5QvCKASowKY2FeAEFns4ZobP9kje2frOycIs37NVvzfs3W7vxSvfTDH3rphz80pHui0oekaky/jgq3+W8NOn9i6h4CiVAKAAAAPvty9U5Nm71ch8+M25VXommzl2vWRcd5gimXy9DewtJD0+uqjHTKzinSrvz6p9glRYVVWVA8wjPyKTWxeU6xA4CqUhIjdevoo3TjyN76bv1evb0kS9+t36PFm3O0eHOO4iNtmjiwi6YMTlHvI2LMLteL58x7qQkmV4LWiFAKAAAAPnG6DM34ZG21QEqSZ9st81Zo7pIsbTtQrG0HilVazxS7CJvVM7WuS5XAyb3WU5Sdj60AWr5Qa4hG9T1Co/oeoZ15xZq3dJveWZqlHXklevXHzXr1x806vmuCJg9O1fgByX49g2djOJwurdyWJ4kz7yEw+OkOAAAAL6XlTuUWOZRzsEwHiso8/88tKlPOQYc27i6onLJXu6IypxZs2Oe5HGKROsW7RzhFHDbNLlLtopliB6BtSY6L0I2n99Z1p/XSDxv3au7iLH372x79uvWAft16QDM+WaNzB3bW5BNS1bdTrCk1rt9VqNJyl+IibOqeFGVKDWjdCKUAAABaseIypw4UVYRLBw46avx/ReB0KHg6WOb0y31feEKKJhzTqWKKXXy4bEyxA4BqrCEWnXpUB516VAftyS/Ru8u2eUaavvHzVr3x81YdmxKv9MEpOvOYTkEdObpiW64k6diUeIVwpk8EAKEUAABAC2AYhorKnIcCpCL3yKUyHShy6EDlqKbDA6cSR93T5moTYpESIsMUH2lTYlSY4iPDlBgZpvgomwqKHZqzJLve2zgnrbNO7JnUqPsHgLaoQ2y4rj21l6aN6Kkff9+nuUuy9PWa3VqRnasV2bl64NN1Oiutk6ackKoBXeICXk9mduXUPRY5R4AQSgEAgFbB6TK0eHOOlu2zKGlzjk7s1UHWZvpXXcMwVFBaXhkkHQqUagqcDo1gcqjM2biAKTTEooSoMCVE2pQQGVbxVXnZEzhF2RRfeV1iZJhiwkNr/au402Xou/V7tSuvpMZ1pSySOsaFa3D3xEbVCwBtXUiIRcN6t9ew3u21r7BU71eOntqyv0hzFmdpzuIs9e8cq8knpOrstE6KCbcFpI5M1pNCgBFKAQCAFu/L1Ts145O1lescWfXGxl+VHBeu+yb09ZwBLlBcLkP5JY6aRyxVvVxl9FJukUPlrnpON1eLsNCQihFLlYFSRcBUNWyq8v/Ky9H2UL+u12QNsei+CX01bfZyWSSvYMp9L/dN6NtsQ0EAaEnaRdt19Yieump4D/3yR47mLsnSl6t3afX2fP1j+2o99Nk6TTg2WVMGpyotJd5vx/uDDmnL/iJJUhojpRAghFIAgqoljWQA0DJ8uXqnps1eXm3Ezq68Ek2bvVyzLjquwcFUudOlvGKHJ1Cquri311S5KuFSblGZGpkvKcJmrRypVHWKnK1y5NLhwVPFyKYIm7VZLAg+tn+yZl10XJUwsELHIIWBANDWWCwWndgzSSf2TFLOwTJ9sLxi9NTvew9q3q/bNO/XberTMUZTBqfqnIGdFRfRtNFTWwsrftb0aBel+MgwfzwEoBpCKQBBY+ZIBgCtk9NlaMYna2ucQubeds9HaxQbbqsMmyoDpYNllVPkDl0+UORQXrGj0bXE2EMV7zVKyVYZJFWZKhcZ5hU4hdvMPdV3U43tn6xRfTvq50179PXCxRo9bAh/bACAIEiMCtMVw3ro8pO769etBzR3cZY+W7VTv+0q0H3/W6OHP1+n8cdUjJ46vmtCo/6YsaUylEpj6h4CiFCqmWD0CFo7f45kANC6GYahEodLBaUOFZaUq7C0XIUl5Sqo/LewtOKroKRcm/YUeI3SqcnewlKlv7LYpxpiw0OrrLVUOWLJEy4dHjjZFB8RprDQtnlmOWuIRUO6J2r/OkNDuify+QUAgshiseiEbok6oVui7pvQTx9mbNPbS7P1264CfbB8uz5Yvl29OkRr8gkpOu+4LkqIaviIp60FFf8OTE0IUPUAoVSzwOgRcxAEBoZhGCotd6mozKlih1PFZU4dLC3X3R+urnskw8dr1LdTnGLsoYoIs8oeGtIspqcAjdUWjzEul6GDZbWESF6XHZ5QqbDKPgVVAidnY+fD1aJDjF0piZGeQOnwxb0TqywCHhdhU6i1bQZMAICWKy7SpkuHdtclJ3VTRnau3l6SpU9W7NSmPYV68LN1euzL9Rrbv6OmDE7Vn3ok1vlZ2+UyPNP3OPMeAqlZhFL//ve/9fjjj2vXrl069thj9a9//UuDBw82u6ygYPSIOdpqEOhyGSoprwiKih1OlTicFeFRlcvFVba5LxeXuSr/La/41+FSiTt0chz6fve/jbG3oFTDH/vOcznEUrHWSkRYqCLDrIoMsyrcZvX8PyIsVJE2qyLCKr7c/48MC1VEWIgibKFV9q3c7tnHKlsb+oWzLYYjZmtpx5hyp0sHS50VI5OaGCj5k8UiRdtDFWMPVXR4qKLtoYoOt1VcrtyWW1Sm95dvr/e2npk8UCf2TPJrfQAANEcWi0XHpSbouNQE3XNmX32cuUNzl2RpzY58/W/FDv1vxQ51bxdVMXpqUBe1i7Z7fb/TZeijFTtV7LTIZrWoV4dokx4J2gLTQ6l33nlHt9xyi1544QUNGTJEM2fO1JgxY7R+/Xp16NDB7PICqiHrYNz94eqKKQG2ENlCQmQNqTgwhFpDFBpiUajVotCQQ/+3WSv2CQ2xMMqkFs01CHS6DE+wU1JL2OMVHpU5VVTf/g6nJzwqKnOqtLxxpxJvrLDQEEVUrpfSkHVarBaLnEbFM+MypINlTh0sa1zIVR+b1VIlpKoIrNwB1qH/NzAQC7Mq0hbquRxhszab0KelhSOtQTCPMaXlzuqjjNyXq4ZINUx9q7qtsWFybUJDLIoJdwdJtsNCpVCvUCnaHlqxr9122OWK9199P8ucLkM//b5fu/JKavx5alHFwtuDuyf69TECANASxITbdNGfuuqiP3XVqm15mrMkS//L3K7N+w7qn1/8pie+Xq/RfTtq8uAUDe3ZTl+v3eV1AguH09CpT3zPZ0cEjOmh1FNPPaUrr7xSl112mSTphRde0GeffaZXX31Vd955p8nVBdaSzTn1roOx/2CZJr/8S6NuPzTEUhliVQmzfAq2QmSzHroNr32tlsrLIbKFWGQ9fJv7+yq3134bFf8GK3CrLwi0SJrxyVqN6tvRK1Qod7qqhDwuFTnKDwuIXCoqK682sqhiJFK5ih2uesOjsiAHRvbQkIqAxWZVeOW/7pCm6r/hNu+gJtxW/fqqYY57//DQEM/0l59/368pDXgdz75iiE7oluDpTVHlV7Gj3GtEl2d75cgt93Xu7e6+F1XZv6QyxHNPCXI4DTmc5covKZdUGrD+RoaFKtwWUjmCq0qwZasY0VU1EDsUfoVWGeFl9QrPIn2Y2thcA9jWrKHHmJN7tVeRo+aRSAUljsNCpZrXUyosKVeZ07/HDXtoiCcQ8oRIdlu1bTGef21el93XB3P6rTXEovsm9NW02ctlkbx6767gvgl9m01QDACAWQZ0idM/uwzQP8YfrU9W7NDcpdlakZ2rz1bt1GerdiopOkz7C8uqfR+fHRFIpoZSZWVlWrZsme666y7PtpCQEJ1++un6+eefTawsOPYU1B1IuXWIsctuC1G501C5y1C501X5r6Fyl0sOZ83rbpS7KvYP9uiYYHCHU74GbgdLy+sMAg1JO/NKdNIj30qSJ+CorceBUlM4dCg8CqkMOtzXh1QJh0K9Lledrlb1cnioVSFB/AVtcPdEJceFN2gkgzXEohhriGLCm3YK25oYhqEyp6uGAKsyPPSEYFVCLncAWWVkmnvfwwOxqqNNSstdKi136UBR48/kVRtL5dRGT3DlHqVVZVu4LUSfr9pV50jMv723Uttzi2WRRUZlfyr6JBmq+v9D24wqN2gYRq3XG5XfXNt1nvvwfH8D76+O21Pltrrqd99f9ds7dNl9o8Zht1fr/VWpP6eorEHHmP7Tv6p1n8aICrPWOr2tptDIMzKpynVR9tAWu1D32P7JmnXRcV5/2ZUqjiv8ZRcAAG9R9lBNHpyqyYNTtXZHvt5emqUPlm2rMZCS6v7jPdBUpoZS+/btk9Pp1BFHHOG1/YgjjtBvv/1Wbf/S0lKVlh4a0ZCfny9Jcjgccjj8/4tfoCVFNqz9T10wQEPqmXbgrBpWVft/Tdtq+L97v/qud7rkdBlyVPu/IWdlSOas/F5H5fXV/1+xb7mz4ntrq7+2MMhZ+T2BCtx259c8csZikSI9gU+I9+ggW0UQUG2UkS3EeySSzarwyuCo6kgj9/cGfoSBIaezXM7AzIqr1d3jjtL1b6+odSTD3eOOkstZLleA6wqRFGWzKMoWKkX59xDoWbOrcmRc1SDLE3RVCbwOX8OrqOo6Xw7vsKvY4fKMpjMMeUK1psgvKdcDn67zx0NHI4RUrpcU7RUgWb23NWB7ZFiofz4cGk45/DyNL5hGHtVOp/Qepl9+36v/+3mZTjtxkP7Us72sIZYW+RmhpXH3mF4HDz03B30PPnoeWL3bR+ieM47SiF5JuvzN5bXu5/7D2s+b9tT7uykarzW93hv6GEyfvueLf/7zn5oxY0a17V9//bUiIyNNqKhpXIYUH2ZVbpl06FfzqgzFh0l71/6iz4P0e6NFkq3yq8FCKr8C9GpyGRVfzsqvGv/vklyq+PfQdku1/XcclObvsNZ7n+d1c6pHrKGwECksRLKFSHarZLVIFksDF/I1JJVVfh08tNlR+VXgcydavsuOtOiDLSHKLTv0eo8LMzSxm0vOrcv0+VYTiwuSyMovr+WWQ9Wg94/TkBxOqdQllTmlMlfll9OiMpdU6pQcrorrN+dblJlT/6iXrtEuJVVZ29Kdh1oqv1Tlste/Fu+jlnvfqvtZqlxpqWW/w+/r8Ptxbzv8+w/9Y1Rcf1idNT6WKtsOfyzVaq7hNqtdruE29xRL3zbgGHN1H6eOjjfqPp4YkkoqvyovFqhtHjt8NaidlLfxV3210exK2p758+ebXUKbQ8/NQd+Dj54H1rJ9Fkn1f4b5euFi7V8X3FkkbVFreL0XFRU1aD9TQ6l27drJarVq9+7dXtt3796tjh07Vtv/rrvu0i233OK5nJ+fr5SUFI0ePVqxsbEBrzcQbN126/q3V0iqafSIRQ9OPFZj+h1Rw3eiMZwuQ6c8+YN255fWMY3MrocuG86w1AA4Q9LfXEaNIxngX4s35+iiV3+td7+HJg3mr11+1NBjzM1TOMYEisPh0Pz58zVq1CjZbP6fBoya0ffgo+fmoO/BR8+DI2lzjt7YWP9nx9HDhvDZMYBa0+vdPbOtPqaGUmFhYRo0aJC+/fZbnXPOOZIkl8ulb7/9Vtddd121/e12u+x2e7XtNputxT5hZ6Z1UWiolXUwgsQmafpZ/epZELefwu1hwS+ujbBJGtq7g/I2Ghrau0OLfe82dyf26tCgdbxO7NWBcMSPOMY0Hy35s0FLRt+Dj56bg74HHz0PLD47Ni+t4fXe0PpNX9H0lltu0csvv6z//ve/WrdunaZNm6aDBw96zsbXFoztn6xFd5ym2VOP18W9nZo99XgtuuM0AqkAcS+I2zEu3Gt7x7hwziiBVsN9RjKp+uRgzkgWWBxjAABAS8NnR5jF9DWlLrzwQu3du1f33nuvdu3apbS0NH355ZfVFj9v7awhFg3pnqj96wwNqTwDGQJnbP9kjerbUT9v2qOvFy7W6GFDSP3R6nBGMvNwjAEAAC0Nnx1hBtNDKUm67rrrapyuBwQSQSDaAsIR83CMAQAALQ2fHRFszSKUAgAEDuEIAAAAGorPjggm09eUAgAAAAAAQNtDKAUAAAAAAICgI5QCAAAAAABA0BFKAQAAAAAAIOgIpQAAAAAAABB0hFIAAAAAAAAIOkIpAAAAAAAABB2hFAAAAAAAAIKOUAoAAAAAAABBRygFAAAAAACAoCOUAgAAAAAAQNARSgEAAAAAACDoCKUAAAAAAAAQdKFmF9AUhmFIkvLz802uxD8cDoeKioqUn58vm81mdjltAj03B30PPnpuDvoefPTcHPQ9+Oi5Oeh78NFzc9B3c7SmvrtzGnduU5sWHUoVFBRIklJSUkyuBACA/2/vzsOqqto2gN8HOAqIIEqi5lBkjphjoeGrQDjmlAKCqJlfzoKRGYaioDlimlEO6ZsECkiJYuQAOUukKIqa5hSWiQmOKCjDOc/3h7FfUCsnzha4f9flFeyz9/E5q+U+ez17rWcTEREREVFxN2/ehJWV1d++rpF/S1s9w/R6PTIyMlC1alVoNBq1w3li2dnZqFevHs6fPw9LS0u1w6kQ2ObqYLsbHttcHWx3w2Obq4Ptbnhsc3Ww3Q2Pba4Otrs6ylO7iwhu3ryJOnXqwMjo7ytHlemZUkZGRqhbt67aYTx1lpaWZb4DljVsc3Ww3Q2Pba4Otrvhsc3VwXY3PLa5Otjuhsc2VwfbXR3lpd3/aYZUERY6JyIiIiIiIiIig2NSioiIiIiIiIiIDI5JqWdI5cqVMX36dFSuXFntUCoMtrk62O6GxzZXB9vd8Njm6mC7Gx7bXB1sd8Njm6uD7a6OitjuZbrQORERERERERERlU2cKUVERERERERERAbHpBQRERERERERERkck1JERERERERERGRwTEoREREREREREZHBMSllAKwlTxVBREQE1q9fr3YYFUpSUpLaIRAZjF6vV36+ffu2ipEQERER0dPCpJQBaDQaAEBmZqbKkVQc8fHx+Pnnn9UOo8LIyclBeHg4QkJCsGnTJrXDqRBmzZqFsWPHYu3atWqHUiEVv9nAGw+GYWR095Llgw8+wNKlS5Gbm6tyROUb+zURUfnEcRI9a5iUMpDPPvsM06ZNA1Dybi89fZmZmVi0aBHCw8Nx+vRptcOpEKpUqYLw8HDUrVsXISEh+O6779QOqdzr378/6tWrhxUrViAqKkrtcCqc7OxsFBQUoLCwEBqNhuf1UlQ8OZKcnIyvv/4ar7/+OszNzVWMqnzT6/XKDbU7d+4wAWggBQUFys+FhYUqRlKx8Pytrps3b6odQoXCcZK60tLScOHCBbXDeOYwKWUglpaWCAsLw4kTJ5S7vVQ6atasiS+++ALnz5/H4sWLcebMGbVDKtdEBAUFBahduzaCgoJgZmaG+fPnY+vWrWqHVm7l5+ejadOmWLp0KczNzREREYGYmBi1w6owPvnkE7i7u8PZ2Rm+vr64evUqjIyMOLApJUXJkcWLF2PPnj0YP3482rdvr3JU5Zder1euU+bOnQt3d3fY29vjo48+QkJCgsrRlU8nT54EAGi1WgDA8uXL4evri+DgYF7DlLLi/f348ePYu3cvLl26hJycHJUjqxjmzZuH3r17w83NDV999ZXa4VQIHCepZ8OGDejZsyeWLl2KW7duqR3OM4XZkVJw77IOEUGvXr3g7OyMuLg4ALwrU9qaNGmCKVOm4ObNm7C0tFQ7nHJPq9UiJiYGwcHBuH79OlJTUzF+/Hgu5SsFer0elSpVAgCcOnUKdnZ2+OmnnzB37lzExsaqHF35FxAQgPnz56Nfv37w8PBAcnIyevXqhcuXLzMxVYpu3ryJ7777DpMnT1YuoLm8rHQUDdCnTJmCkJAQODk5wd3dHcnJyZg6dSqio6NVjrB88fX1xfDhw5GcnAzg7tLsiRMn4vr161iwYAFGjRqlXDvS0yUiSn8PCAjAgAED4OnpiTfffBMTJkzAuXPn1A2wnPv8888xb948vPHGG7h27Rq+/PJLTJw4Ue2wKgSOkwzv+++/x6BBgxAcHIxx48bBwsJC7ZCeLUKl5tatWyV+nzhxorz00ktSUFAgIiJ6vV6NsCqUvLw8tUOoEH766ScxNzeX//73v/LLL7/I6dOnxcnJSTp06CCbNm1SO7xy4d7zhb+/v9ja2srcuXMlODhYGjRoIB07dpTo6GiVIiz/NmzYIPb29vLTTz+JiMjGjRvFwsJC6tevL82bN5esrCwRESksLFQzzHLhQd+Pv/32m3h7e4ulpaWkpKSIiIhOpzN0aBXCqVOnpEWLFrJ582ZlW2pqqowcOVIcHR3l8OHDKkZXvhw8eFCaNWsmvXv3lk2bNomHh4ckJSWJiMiff/4pnTt3FhcXF1m/fr26gZZjCxcuFFtbW9m+fbuIiAwbNkxq1Kghe/bsUTmy8mvPnj3i7+8v33//vYiIZGdny4wZM6Rt27bi5+encnQVB8dJhnH79m1xd3eXgIAAERHJycmRs2fPyqxZs2T9+vWSnZ2tcoTq40ypUhIWFoYePXpg27ZtuHLlCoC70+BNTU0xd+5cAP9bkkClp2hGCZWutLQ0vPDCC/Dy8kLjxo3RsGFDrF69GsbGxvD19eWSjyeUk5NTom7RyZMnER0djf/+97/w9/fHtGnTsHXrVpiYmGDBggV8CmIpqVSpEnr27AkHBwd8//33eOeddzB37lysXLkSf/zxB/r06YNLly7B2NhY7VDLtOL1jDIyMnDy5Enk5uaifv36CA0NhaOjI3r06IHjx49zdtpTcm8bajQaZGRkIC8vT9nWunVrjBgxAhkZGTh16pShQyyXdDod2rRpg7Vr1+LUqVP49NNPceHCBbz44osAAFtbW4SHh0Ov1yM0NJQzpp4yEUFeXh527tyJKVOmwNnZGZs2bcK6deswe/ZsdOzYEXl5eVzK95Rt3boVo0ePRlRUFGxtbQEAVatWhY+PD/r164c9e/bggw8+UDnKioHjJMMQEaSnp+PmzZu4evUq/P39MXz4cCxduhSjR4/G4sWL1Q5RdUxKPSXy1zKCogs7nU6HF154AW5ubnjnnXcwf/583LlzB05OTjh9+jQvoqlcMTMzg06nU9ZHFxQU4Pnnn8eSJUvw559/YtKkSdiyZYvKUZZNH330EYYPH45r164pywyqVq0KEUF+fj6Au+ebxo0bY8WKFThz5gwWLlzI2gyloEePHvDz88Pt27cxf/58+Pr6Yty4cXBwcICdnR1SU1Px3nvvqR1mmSbFltMEBgbCzc0N7dq1w+DBgzFt2jRYW1tj1apVaN++PZycnJQ6jcKlfI8tMzNTafMvv/wS6enpqFKlCurUqYOzZ89Cp9Mp7duuXTvUqlUL+/btUzPkckGv1ysJbHt7e0RHR+PixYs4dOgQDhw4oOxXv359hIeHQ6PRIDAwELt371Yr5HJHo9HA2NgYt2/fhqOjIxITEzFw4ECEhIRg5MiRyM/PR3h4OPbv3692qOVK06ZN0alTJ9y8ebNEPcxq1arBx8cHb731Fr755huEhoaqGCXR02NmZgYfHx+sXLkSL774Ii5cuIDhw4fj/Pnz8PLywvbt2yt8boBJqaeg+F3d27dvAwD+7//+D+Hh4Vi3bh1ee+01LFiwAEOGDEF6ejoiIiIQHx+vZshET1WHDh3w22+/KRcQRcVa8/Pz0bZtWzRv3hzNmjVTM8QySURgYmKCP/74AwEBAbh27ZqyvVKlSjh48CAAKLOoGjZsiJYtW+Ls2bM4ceKEmqGXO0WD8lq1auHChQs4c+YMOnXqBADIzc1Fo0aNsGnTJqxZs0bNMMu8ou/Sjz/+GMuWLUNQUBCOHDkCAPjiiy9w5MgR2NraYvny5Wjfvj2aN2+Oc+fOcebxY0pKSoKdnR2OHz8OPz8/TJ06FRqNBrVr10b//v0xZcoUbNy4UXkSXHZ2NvLy8tCgQQOVIy/bihfX3rhxI37//Xe0atUKa9euxQsvvIBly5bhxx9/VPavV68eVq5cic6dO8PR0VGtsMu8Bw36TExMULlyZXh6esLNzQ2fffYZRo0aBQC4cuUKoqKiWAj6Katfvz6mTp0Kb29vbNu2DSEhIcprVlZWGDNmDGbMmIGxY8eqGCXR0zV06FAcOHAA3377LWJjYzF48GAAd2/k16tXr8TTVysktdYNlhfF61ksWrRI+vXrJ66urvLuu+9KTk6O8trVq1dlxowZMnjwYNFoNOLh4SHZ2dmsK0XlRkREhGi1WgkICJD09HS5du2aBAYGyttvvy03btxQO7wyZ+fOnSJy9xwTEhIiHTp0kFGjRsnly5dFRCQsLEyMjIxk6dKlyjF37tyRwYMHy7p161hrpxTduHFDHBwcpGvXrrJlyxZxdXUVV1dXpc1ZU+rx6fV6uXTpkjg7O0tsbKyIiCQmJkqVKlVk5cqVIiJKXcYLFy7IBx98wPZ+Anq9XgYOHCjW1tZStWpVOXr0aInXfXx8xNTUVIYNGyZ+fn7yxhtviL29vfL/gB5d8es+f39/adCggUyfPl25Zjx69Kg0adJEevfurdSWuhf7/KMr/p146NAhOX36tGRkZIiIyJkzZ6Rly5Zib28vInfrv1y9elV69OghHTt2ZHs/BVFRUTJr1iwJCgqS1NRUERG5ePGijBs3Tl577TUJCQl54HFseyqvTpw4IQEBAWJlZXXfd29FxKTUU+Lv7y/PPfecfPbZZ7Jy5UqpWbOmODg4SF5e3n2FzUNDQ6VGjRpy8uRJNUMmeqr0er1ERkaKhYWFvPjii/LSSy9J9erV5eDBg2qHVuZ8/PHH8tZbbynnDp1OJ3PnzpUOHTrIyJEjlcTUnDlzRKPRyMCBA2XMmDHSqVMneeWVV5SLbyamSodOp5OYmBhxcHCQl156SVxcXCQ/P195jR7NvW2WnZ0trVq1kl9++UXi4uLEwsJCSb7euXNHVqxYcd95hUmSxzdz5kzRaDRSvXp1ZbBY3JIlS2Tw4MHSpUsXGTNmjNLXOVh8MvPnz5caNWpISkqKXLt2TUT+92/hyJEj0rRpU+nXr59SfJseT1BQUIkHrkyaNEkaNGgg1apVkwEDBsjGjRtFRCQ2NlZq1qwpDRs2FAcHB+nQoYO0bt2a/f0pmDhxotSqVUv+85//SLt27USj0UhoaKiI3L25MG7cOHn99ddl2rRpKkdKZBgHDhwQLy8vadq0KR8c8hcmpZ6CY8eOSYsWLWTXrl0iIhIXFyeWlpbyxRdflNiv+Beao6OjfPjhhwaNk8gQ0tPTJS4uTqKjoyU9PV3tcMqk48ePK4Ps48ePi8j/ElPt27eXUaNGKYOY77//XgYOHCi9evWSt99+m8kRA9HpdJKTkyOnT59W2rqo7enxHDhwQAoKCiQrK0vatGkjgwYNkurVq5f4Lv3ll1+kR48esmHDBhUjLV+uXLkiR48elUGDBomNjY3yxLF7E33Fr2GYBHwyubm50rdvX/n0009FRB44yzItLU2sra15rfgE0tLSpH379tKlSxfZvXu3JCcni52dnezcuVPCwsLEw8ND2rVrp8zKzMrKkhkzZsi8efMkLCxM+f/B/v74vvvuO3nuueckNTVVacc5c+aIsbGxfP311yIi8vvvv4u3t7eMGDGCK0ioQsjNzZXdu3fL77//rnYozwwmpZ6CnTt3Sv369UVElLu6y5YtExGRmzdvytdff33fzAUnJyflsZBERA8SFxcntWrVkm+//VZESiamRo4cKVlZWSJyd6lBcbyAVg+TgY9Or9fLTz/9JBqNRpmpEx0dLSYmJuLh4aHsl52dLT179hQXFxfOWnhKirdjXl6euLm5iY2NjSQnJyvbp0+fLufOnVN+56DxyV27dk1q164ts2fPVrYVtWtOTo5kZmaKiMipU6fY159QYmKi9OrVS3r16iU+Pj4l2vzgwYMydOhQadu2rURHRz/weLb/k1m1apW8+uqrkpeXV6ItAwICpEaNGsqgPCsrS/n+5DmGqOJhofNHJMWe8FP0c/369WFvb4+QkBB4e3vjk08+UYokHj9+HPHx8Th69CiAu0VcU1NTsWvXLnh4eBj+AxDRM+veIqzVq1eHq6srgoODERsbCyMjI0yaNAn9+vXDsWPHEBgYiMuXL8PU1FQ5Rv4qjk6lq/h3QWxsLFatWgUASvFi+mfF+7pGo4GDgwPc3d0xduxYXL58GQMHDsSsWbPwzTff4K233kKfPn3Qu3dvnD9/Hlu2bIGxsTF0Op2Kn6B8KHr6G3D30eCRkZFwdnZG9+7dsWDBAjg5OeHbb79F3bp1lf1YVP7RFJ0rip8zKlWqBAcHB5w+fRqZmZkA/teuqamp8PPzw+XLl/Hyyy+zrz+moqLBrq6u8Pf3h4ggMjISV65cUfZp06YNJkyYAHt7eyxcuFA5jxdX/N8IPTq9Xo9jx44hJycHxsbGylODPTw8YGpqioyMDACAjY0NjIyMSjw8iogqDl49PwKdTlfiRFl0kWBhYYEbN27A398fH3zwAUaOHAng7pP4goKCoNPp0KJFCwB3LzratGmDP//8Ey1btjT8hyCiZ1ZRQmPt2rUAgI4dO8LHxwdt2rRBYGBgicRU37598cMPP9x3Ec2LudInIko7r1mzBm5ubpg+fbrydET6d0V9PTk5GTk5OQCADz/8EKampoiOjoZer8eHH36IrVu3onbt2qhbty769OmD1NRUaLVaFBYWcrBYCrRaLWJiYuDl5YUNGzbA2toahw4dgrGxcYV/XPXjKD7ALhqMA4C5uTm6dOmCqKgofP3118rA/Nq1a1iwYAGuXLmC6tWrK/uzrz+azMxM5SnAK1euRPPmzeHv74+WLVtiw4YN+OGHH5R9ixJTtra22Lt3r1ohl1u9e/dGixYtMHbsWGRmZqJSpUoA7v4bMDMzK5GsBXhjh6ii0si9ZwO6z8mTJ9G4cWPl9wULFiAlJQU6nQ7vv/8+Xn/9daSnp8PR0RFNmzZFp06dUKdOHURFReHy5cs4ePAgtFptiUcAExE9yK+//opGjRrBxcUFCQkJAIB9+/Zh6dKlSElJwccff4y33noLOp0OUVFR8PLy4oDFgO5NSA0ZMgTGxsbYv38/WrdurXJ0ZUtsbCzc3NwwePBgdOnSBUOGDEFgYCC2bt2KrVu3wtraGkDJNgfu3hBiny8dxds6KysLNjY20Gg0KCgoUAb59HCKX/N98cUX2LFjBwCgZcuWCAwMBADMnj0boaGhsLOzQ+XKlZGTk4Pbt2/zuvEJJCUloVu3bti/fz9WrFiBiIgIHDx4EA0aNMDOnTsREhKCgoICTJ48GS4uLspxp06dQsOGDdneTygmJgbnzp2DVqtFx44d8eqrryIqKgpffPEFqlSpguDgYOTn52P+/Pm4evUq9u7dyzYnIial/s2nn36K999/H3v27IGjoyOCgoKwZMkS9O3bF2fPnsWuXbsQERGBQYMG4dSpU5g5cybS0tJga2uLBg0aYNmyZTAxMUFhYSGX1BDRfe4dcBcWFiIxMRHvvvsuWrZsiU2bNgG4m5hatmwZDh48iMmTJ2PQoEHKMRykG8aDElImJibYv38/WrVqpW5wZcC9fX3btm0YPHgwOnToAAsLC9y5cweff/452rZti759++Lzzz8HwP5taEWXhffOumSC5PFMnjwZYWFhGDVqFAoKChATEwMHBwesWbMGABAfH48zZ84oN0DHjx/P68YnICLw8vJCQkICCgsL8eOPP8Le3l55PTExEYsXL0Z+fj4++ugjODs7lzie/fzxTZo0CatWrUK7du1w+PBh1KpVCwMGDEBgYCA2btyI0NBQ7NixA02bNoWNjQ0SEhKg1Wp5jicisND5vzh79qyMHj1aLCwsJCkpSaZPn648mSY3N1f8/f3FxMREIiIiROTu05dycnLkzp07ynuw6DARPYqCggLZvHmz2NraSo8ePZTt+/btk759+4q3t7eIsBioIRVv69WrV4tGoxGtVqsU5qaHd/LkSeXnqVOnyvPPPy9paWnSv39/eeONN6R3795SrVo1PmFPJcX7+rp16+Srr75SMZqyLSoqSho3biw//fSTiIh8++23Ym5uLlZWVtKzZ8+/PY7FtZ/MzJkzRaPRSPXq1R94jk5MTJQ+ffpIq1at5ODBgypEWP589913UqdOHaWvX79+XYKCgqRNmzYSEhKi7Hf06FH57bfflKLmHCMRkQgLnf+jNWvWICIiAgEBAejbty9cXFywevVqZT20mZkZZs6ciYkTJ2L48OGIjo6GVquFubk5KleuDIBFh4no3y1evBje3t7K7yYmJnB1dUVYWBiSk5Ph5uYGAHjttdcwe/ZshIeHA2D9KEMpXhem+Aypffv2ccneI/r2228xdOhQ+Pn5AQBmzpyJrl27YvPmzVi3bh08PDxgbm6OGzduICkpSeVoKx5hvbSnKjc3F+7u7nBwcEB8fDxGjBiBOXPmYPHixUhISMCQIUMeeBxnjTyZsWPH4siRI+jevTu6du2q1IoqqgXr6uqK999/Hy4uLpzl+pT89ttvqFGjhlIv18rKCmPHjoWjoyPi4uJw69YtAEDz5s1Rv359pag5x0hEBIAzpf7O8uXLRaPRyJYtW0RE5NKlS+Lj4yMajUa5e1s8yx8QECAajUZ++OEH1WImorInPz9fli5dKtWrV5fRo0eXeE2n08nEiRNFo9FIx44d73uNDCsyMpIzpB7RvbP5Tp48KYsWLZJGjRpJ27ZtZfPmzfLJJ5/I6NGj5dy5cyIicuHCBYmKiuIddAN70GxAExMT9vWH9HczV3/99Ve5cuWKtGvXTubMmSMiIunp6dKgQQPRaDTi5+dnyDDLveKzzPLy8sTNzU1sbGwkOTlZ2R4cHCxXr1594DH0aIquRVavXi3NmjWTX3/9VUT+9+/h0KFDotFo5Mcff1QtRiJ69nGm1ANERERg/PjxiI+PR7du3QAANWvWxJQpUzB06FAMGjQIP/74I4yMjJSZUEW1pjp37qxy9ET0LLv30d5arRaenp5YtGgR1q1bpzy9E7j7FBo7Ozu4u7ujdu3aJY5lzQvDiomJgbe3N0xNTTlD6iEVn2F2584d3Lx5E40aNcJ7772H5ORk1KpVC/PmzcOePXuwceNGrFy5EgBQp04deHp6KnV1qPTJ39RLS0lJYV9/CMX7+sWLF3Hx4kVkZWUBAF588UWcOXMGWVlZ6Nu3r7L/66+/jsTERISEhKgWd3lUfJZZpUqVEBkZCWdnZ3Tv3h0LFiyAk5MToqOjYWlp+cBj6NEUXYu0bt0af/zxBz777DPk5OQo/x4qVaqEFi1awMLCQs0wiegZxzmT9wgLC8Pw4cPh6uqKnj17AoBSbNLW1hYhISHQ6XTo2rUrEhIS8Prrr0NEoNVqMXr06BL7ExEVJyLKxW9ycjKys7PRuHFj1K1bF0OHDoVer4e/vz8AYNmyZbhx4wZ27doFR0dH+Pr6AmARVrWYm5ujWrVqSEhI4CD9IRTvp3PnzkVSUhKOHTuGgQMHwsnJCd27d0d8fDxWrlyJgwcP4uLFi5g1axacnJzwxhtvKO/D79LS93cJqX379nFp00MQEaWvBwcHY/v27Th9+jQcHBzQpUsXjB07FrVr14aIIDQ0FGPGjMGkSZNgamoKFxcXaDQaFnouRVqtFjExMRgzZgw2bNiA5557DomJiTA2Nub36RNYvnw5fv31Vzz33HPw8vJCs2bNEBYWBnd3d+Tk5KBnz55o0KABPvroI5iZmaF58+Zqh0xEzzA+fa+YFStWYPTo0Rg+fDg2bdoENzc3LF68GEDJRNPly5fx/vvvIy4uDnFxcXByclIxaiJ61g0bNgxDhgxRBtv+/v748ssvldo5AwcOxPjx49G6dWtERkbC19cXRkZGsLKygqmpKQ4dOgQTE5P7nl5GhpWdnV3i7jr9uylTpmDZsmUICAhAZmYm9u3bh9zcXPj6+mLw4MEAgKtXr2Lt2rVYt24dtm7dysG5ARUflLNe2pMJCgpCaGgoVq9ejerVq2Pu3LnYtGkTjh07hrp16+Krr77C3LlzYWxsjDp16mDXrl3QarVMjJSy4t+bWVlZsLGxgUajQUFBAbRarcrRlU3Tp0/H0qVL0axZM6XeXGxsLF566SVs2rQJH374Ia5duwZLS0vY2toiMTGRfZ2I/plKywafOYsWLRKNRiObNm0SEZFly5aJjY2N+Pr6KvsUr2+RlZUlb775przxxhsGj5WIyo7CwkJxcXGRmjVryt69e2Xnzp1iZ2cn27dvlytXrkhERIQ4OTlJv3795NixYyIikpGRIZ9++ql89dVXynmHNS+orDl16pS0aNFCNm/erGxLTU2VkSNHiqOj49/WKmJfNzzWS3syf/75p3Tq1Enp61u2bJGqVavKihUrSux36dIlSUlJ4ZPHDEyv1z+w5hdrMz46vV4vI0aMkAMHDoiIyN69e6Vr167ywgsvyOnTp0Xk7jXM2bNn5ejRo+zrRPRQOC/+L0UzFHr06AEA8PT0hEajwZQpUwDcfTpWUX0LExMT2NjYYM2aNahataqaYRPRM87Y2Bhbt26Ft7c33Nzc4Ovri/79+8PZ2RkAMHjwYFhZWSEwMBDr1q1D8+bNUbt2bUyYMEF5Dy7toLLg3rvgGo0GGRkZyMvLU7a1bt0aI0aMgIeHB86cOYPWrVsrMxmK/su+bljF66UlJSVxhtRDKN7Xs7OzYWJignPnzsHOzg7x8fHw8vJCSEgI3n33Xdy5cwcrV65E586d0aJFC9SsWVN5Dy5PNZyi2VKxsbG4ceMG3nnnHc7aeURpaWnQ6XQ4deqU8iRyR0dHzJ49GwEBAejSpQsSExPRsGHDEsexrxPRv+HZ+C+dO3eGp6cn5K/VjFZWVvD09MSsWbMQGRmpDBBNTExQUFCg7FP0SFMior9jYmKCNWvWwMnJCVOmTEFaWlqJgXrv3r3Ru3dvLFu2DLm5ufcdz0E6PesyMzOVAd6XX36J9PR0VKlSBXXq1MHZs2eh0+mU79d27dqhVq1a2LdvH4D/DRa5NFUdRfXSdu/ezYTUQyrq65MnT8b8+fNx48YNNG7cGEuXLsWQIUMQEhKi1BlNT0/HDz/8gIyMjAe+B5UuuadmmpubG6ZPn64sO6OHM3nyZHTq1AlDhw5Famoqrl+/rrzWtm1bzJ49G02bNsUrr7zCvk5Ej4xp63sUvyi2tLSEp6cnAGDq1KkwMjLCokWL7luDzpMtEf0bExMThIWFoUqVKoiMjMT27dvRvXt35ZxTNEMqPz8f5ubmKkdL9PCSkpLQrVs37N+/HytWrMCaNWvQtWtX1K5dG/3798eUKVPw4osvolevXtBqtcjOzkZeXh4aNGigdugEoFevXjh37hzrpT2E4gmOPXv2YM2aNYiNjYWdnR2cnZ0xZcoUjBgxQklI3bx5Ex988AHy8/Ph6uqqZugV0r0JqSFDhsDY2BhxcXGwtrZWObqyY+fOnVi/fj3WrVuHK1euICwsDAMGDMDu3bvRpEkTAHcTU4GBgWjSpAlsbW1VjpiIyhoWOn8I2dnZWLt2LUaNGoVFixaVWFZDRPQoCgoK4O3tjW3btmHJkiVo27YtrKys4OXlBQBITEzkjBEqU0QEXl5eSEhIQGFhIX788UfY29srr/v6+mLFihXw9PSEtbU1jhw5gkuXLikF/InKmtDQUFy/fh25ubmYM2eOst3Pzw/Lli2Dm5sbAOD8+fO4cuUKUlNTWejZwB6UkDIxMcH+/fv5VMlHEBoaiuzsbOh0OkybNg3A3X49evRopKSkYM+ePWjcuPF9x7HsABE9Cn4zPgRLS0u4u7tj/fr1GD9+vNrhEFEZptVqsWbNGnTp0gVeXl7o1KkTPvzwQ+Tl5WHz5s3QaDRcEkxlikajgb29Pa5fvw6tVqsscS/y2WefYeHChSgsLMSxY8fQqFEjpKamwsTEBDqdTqWoiR6PiGDLli2YPn06fv755xL9fdGiRVi8eLEy27Vbt244dOgQtFotCgsLmZAykL9LSO3bt48JqUdQ1NcDAwNx4sQJ5Xxdr149LF++HK+99hqcnZ1x7Nix+45lQoqIHgVnSj2GomLnRESPKz8/H35+fli6dCmSkpLQvn17aDQanl+oTLp69SoyMjIwZ84cJCQkYP369ejYseN9/bn43XP2dSqr8vLy4OPjg8jISMTGxqJr167/uD9njRhO8dlo9yakWDPt0eXl5WH8+PGIiYlBXFwcnJyclNf++OMPuLm5wcbGBvHx8eoFSURlHpNSREQqycvLQ3BwMGbOnAljY2MOXKhMKt5v8/Pz4e3tjZ07d+K7775D+/btAQBBQUF45513lDpSxWcyEJVFRUuxd+zYoSRhi7B/qy8qKgre3t5MSD0FBQUFGDRokFJbqnhfz8rKQo0aNTgLkIieCJNSRETPENYcobKuaLCekJCAqVOnIj4+HpcvX0ZaWhqTrlSuFBYWwsvLC7t27cL69evh6OiodkgEICYmBp6enjA1NUVSUhITUk/Bv/V1XrsQ0ZPg2YOISAXF60atW7cOq1atAsCneVLZp9VqERMTAy8vL2zYsAHW1tY4dOgQjI2NWS+NyhUTExNERUXB2dkZ//nPf3DkyBG1QyIA5ubmqFatGnbv3s2E1FPyb32d1y5E9CQ4U4qIyMAeVIS1bt26SEtL42Oqqcwr3r+zsrJgY2MDjUaDgoICaLValaMjevoKCgoQFBSEGTNmcDbgMyI7OxuWlpZqh1HusK8TUWlgUoqIyIAelJAyNjbG/v37eUeXyo2iS4t76+pwiQdVFOzrVFGwrxPRk+IZhIjIQP7uMdUpKSlMSFG5U9TXY2NjuTyVyq3iS1LZ16k8Y18notLCmVJERAbwdwkpPhWIyhsuT6WKgn2dKgr2dSIqTUxtExGVMr1ez4QUVQh/tzw1Li6OAxcqV9jXqaJgXyei0saZUkREBhIVFQVvb28mpKhc+rvZgPv370erVq3UDY7oKWJfp4qCfZ2IDIEzpYiIDCAmJgbe3t4wNTVlQorKnX9ansqBC5Un7OtUUbCvE5GhMClFRGQA5ubmqFatGnbv3s2EFJUrXJ5KFQX7OlUU7OtEZEhcvkdEZCDZ2dmwtLRUOwyiUsHlqVRRsK9TRcG+TkSGwKQUERERPZGYmBh4enrC1NQUSUlJHLhQucW+ThUF+zoRGQqX7xEREdET4fJUqijY16miYF8nIkPhTCkiIiJ6YlyeShUF+zpVFOzrRGQITEoREREREREREZHBcfkeEREREREREREZHJNSRERERERERERkcExKERERERERERGRwTEpRUREREREREREBsekFBEREVV4586dg0ajweHDh9UO5akJCwtDtWrV/nU/jUaDDRs2lHo8RERERPdiUoqIiIjKBY1G849/goKC1A7xPk5OTkp8pqamaNasGZYsWfJU3nvgwIE4deqU8ntQUBBatWp1334XL15Ejx49nsrfSURERPQoTNQOgIiIiOhpuHjxovLz2rVrMW3aNJw8eVLZZmFhoUZY/2rEiBGYMWMGcnNzER4ejnHjxsHa2hpeXl5P9L5mZmYwMzP71/1q1ar1RH8PERER0ePiTCkiIiIqF2rVqqX8sbKygkajUX6vWbMmFi5ciLp166Jy5cpo1aoVtmzZ8rfvpdPpMHz4cDRp0gS///47ACAuLg5t2rSBqakp7OzsEBwcjMLCQuUYjUaDlStX4q233oK5uTlefvllbNy48V/jNjc3R61atWBnZ4egoKASx/3+++/o27cvLCwsYGlpCQ8PD1y6dEk5Ni0tDc7OzqhatSosLS3Rtm1bHDhwAEDJ5XthYWEIDg5GWlqaMjMrLCxMibv48r2jR4/CxcUFZmZmqFGjBkaOHIlbt24prw8bNgz9+vXDggULULt2bdSoUQPjxo1DQUHBv35WIiIiouKYlCIiIqJyb/Hixfjkk0+wYMECHDlyBN26dUOfPn1w+vTp+/bNy8uDu7s7Dh8+jD179qB+/frYs2cPhg4digkTJuD48eNYvnw5wsLCMGvWrBLHBgcHw8PDA0eOHEHPnj3h7e2Nq1evPlKsZmZmyM/Ph16vR9++fXH16lXs2rULiYmJ+PXXXzFw4EBlX29vb9StWxcpKSk4ePAgJk+eDK1We997Dhw4EBMnTkTz5s1x8eJFXLx4scT7FMnJyUG3bt1gbW2NlJQUfPPNN/jhhx8wfvz4Evvt2LEDZ8+exY4dO/D1118jLCxMSXIRERERPSwmpYiIiKjcW7BgAfz9/eHp6YnGjRtj3rx5aNWqFT799NMS+926dQtvvvkmsrKysGPHDjz33HMA7iabJk+ejLfffht2dnbo0qULZs6cieXLl5c4ftiwYfDy8kLDhg0xe/Zs3Lp1C/v373+oGHU6HVavXo0jR47AxcUF27Ztw9GjRxEZGYm2bdvCwcEB4eHh2LVrF1JSUgDcnUnl6uqKJk2a4OWXX4a7uztatmx533ubmZnBwsICJiYmyuyxBy3ti4yMxJ07dxAeHg57e3u4uLjg888/R0RERIkZWtbW1vj888/RpEkT9OrVC2+++Sa2bdv2UJ+TiIiIqAiTUkRERFSuZWdnIyMjA46OjiW2Ozo64sSJEyW2eXl5IScnBwkJCbCyslK2p6WlYcaMGbCwsFD+jBgxAhcvXkRubq6y3yuvvKL8XKVKFVhaWiIzM/Mf41uyZAksLCxgZmaGESNGwM/PD2PGjMGJEydQr1491KtXT9m3WbNmqFatmhL3+++/j3fffReurq6YO3cuzp49++gNVMyJEyfQsmVLVKlSRdnm6OgIvV5foj5X8+bNYWxsrPxeu3btf/2cRERERPdiUoqIiIjoLz179sSRI0eQnJxcYvutW7cQHByMw4cPK3+OHj2K06dPw9TUVNnv3qVzGo0Ger3+H/9Ob29vHD58GOnp6cjJycHChQthZPRwl2hBQUH4+eef8eabb2L79u1o1qwZ1q9f/5Cf9vE9zuckIiIiuheTUkRERFSuWVpaok6dOkhKSiqxPSkpCc2aNSuxbcyYMZg7dy769OmDXbt2KdvbtGmDkydPomHDhvf9edgE0t+xsrJCw4YN8fzzz5d4r6ZNm+L8+fM4f/68su348eO4fv16ibgbNWoEPz8/JCQkoH///li1atUD/55KlSpBp9P9YyxNmzZFWloacnJylG1JSUkwMjJC48aNH/cjEhERET0Qk1JERERU7k2aNAnz5s3D2rVrcfLkSUyePBmHDx/GhAkT7tvXx8cHH3/8MXr16oW9e/cCAKZNm4bw8HAEBwfj559/xokTJxAdHY2pU6eWWsyurq5o0aIFvL29kZqaiv3792Po0KHo3Lkz2rVrh9u3b2P8+PHYuXMnfvvtNyQlJSElJQVNmzZ94Pu98MILSE9Px+HDh3H58mXk5eXdt4+3tzdMTU3x9ttv49ixY9ixYwd8fHwwZMgQ2NraltpnJSIioorJRO0AiIiIiEqbr68vbty4gYkTJyIzMxPNmjXDxo0b8fLLLz9w//feew96vR49e/bEli1b0K1bN8THx2PGjBmYN28etFotmjRpgnfffbfUYtZoNIiLi4OPjw86deoEIyMjdO/eHaGhoQAAY2NjXLlyBUOHDsWlS5dgY2OD/v37Izg4+IHvN2DAAMTGxsLZ2RnXr1/HqlWrMGzYsBL7mJubY+vWrZgwYQJeffVVmJubY8CAAVi4cGGpfU4iIiKquDQiImoHQUREREREREREFQuX7xERERERERERkcExKUVERERERERERAbHpBQRERERERERERkck1JERERERERERGRwTEoREREREREREZHBMSlFREREREREREQGx6QUEREREREREREZHJNSRERERERERERkcExKERERERERERGRwTEpRUREREREREREBsekFBERERERERERGRyTUkREREREREREZHD/DwO5i5T0tH/TAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'kl_divergences': [np.float32(0.036400583),\n",
       "  np.float32(0.004590045),\n",
       "  np.float32(0.07414716),\n",
       "  np.float32(0.00022086177),\n",
       "  np.float32(0.006095928),\n",
       "  np.float32(0.101959005),\n",
       "  np.float32(0.23503369),\n",
       "  np.float32(0.46376404),\n",
       "  np.float32(0.5281903),\n",
       "  np.float32(0.6940565),\n",
       "  np.float32(6.921629e-06),\n",
       "  np.float32(8.04294)],\n",
       " 'tokens': ['They',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  '▁build',\n",
       "  '▁a',\n",
       "  '▁tower',\n",
       "  '▁to',\n",
       "  '▁reach',\n",
       "  '▁the',\n",
       "  '▁moon',\n",
       "  \"'\",\n",
       "  's'],\n",
       " 'probs_no_steering': array([[2.5587171e-17, 9.6624717e-09, 2.6557245e-10, ..., 8.8107299e-12,\n",
       "         1.2050805e-11, 4.5449755e-16],\n",
       "        [7.8062556e-18, 3.6321580e-08, 9.0949470e-12, ..., 7.2120088e-13,\n",
       "         6.7856831e-13, 7.3725748e-17],\n",
       "        [2.4286129e-17, 7.6252036e-09, 9.2148511e-15, ..., 7.5488060e-11,\n",
       "         2.9558578e-11, 3.7990444e-16],\n",
       "        ...,\n",
       "        [5.3998350e-20, 1.7345883e-08, 8.4696694e-12, ..., 3.6859404e-14,\n",
       "         5.7909233e-13, 4.1928131e-20],\n",
       "        [6.0562856e-20, 1.1932570e-09, 2.9837244e-16, ..., 1.5959456e-16,\n",
       "         1.7139068e-15, 3.6845933e-20],\n",
       "        [1.0720261e-21, 2.0663720e-09, 2.0206059e-14, ..., 1.2143064e-15,\n",
       "         7.9380946e-15, 2.9249106e-21]], shape=(12, 256000), dtype=float32),\n",
       " 'probs_with_steering': array([[4.4235449e-17, 1.2980308e-08, 3.1832315e-10, ..., 1.0459189e-11,\n",
       "         2.7625902e-11, 1.0685897e-15],\n",
       "        [1.2630955e-17, 4.4703484e-08, 1.3415047e-11, ..., 9.0949470e-13,\n",
       "         9.0949470e-13, 1.1969592e-16],\n",
       "        [3.8163916e-17, 9.7788870e-09, 8.2711615e-15, ..., 9.8225428e-11,\n",
       "         8.1854523e-11, 6.3837824e-16],\n",
       "        ...,\n",
       "        [3.9810549e-20, 7.9162419e-09, 1.1027623e-11, ..., 8.3266727e-15,\n",
       "         3.1263880e-13, 5.8021757e-20],\n",
       "        [2.3886329e-19, 5.0567905e-10, 1.4224733e-15, ..., 2.9837244e-16,\n",
       "         1.1796120e-15, 9.9949888e-20],\n",
       "        [2.5622747e-20, 2.0139851e-08, 1.9042545e-12, ..., 6.0784711e-15,\n",
       "         1.0746959e-13, 6.9880218e-20]], shape=(12, 256000), dtype=float32),\n",
       " 'last_line': \"They'd build a tower to reach the moon's\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toanalyze=candidate_texts[0]\n",
    "analyze_steering_impact_last_line(toanalyze,-steering_vector, layer_idx=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d9f7aa9-f7dd-4dbc-87bb-46e92507d87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.69803051e-25, 1.44541264e-06, 1.14917755e-04, ...,\n",
       "        1.64663205e-18, 1.45022883e-15, 6.01393393e-20],\n",
       "       [5.35682609e-15, 6.71386719e-04, 4.93400876e-11, ...,\n",
       "        1.71894499e-10, 2.22644303e-09, 2.23432384e-15],\n",
       "       [2.65412692e-16, 1.93715096e-07, 8.33097147e-10, ...,\n",
       "        3.27418093e-11, 3.78008735e-12, 4.96130914e-16],\n",
       "       ...,\n",
       "       [5.39983504e-20, 1.73458830e-08, 8.46966941e-12, ...,\n",
       "        3.68594044e-14, 5.79092330e-13, 4.19281309e-20],\n",
       "       [6.05628557e-20, 1.19325705e-09, 2.98372438e-16, ...,\n",
       "        1.59594560e-16, 1.71390679e-15, 3.68459332e-20],\n",
       "       [1.07202607e-21, 2.06637196e-09, 2.02060590e-14, ...,\n",
       "        1.21430643e-15, 7.93809463e-15, 2.92491065e-21]],\n",
       "      shape=(28, 256000), dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_next_token_probs(toanalyze,0,27, steering_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25201fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import math\n",
    "# Assume apply_steering context manager is defined elsewhere and works with batches\n",
    "# from your_steering_utils import apply_steering\n",
    "\n",
    "def get_batch_next_token_probs(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    input_texts: List[str],\n",
    "    steering_multiplier: float,\n",
    "    layer: int, # Layer index for steering\n",
    "    steering_vector: Optional[torch.Tensor] # Steering vector (can be None)\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates next-token probabilities for a batch of input texts, compatible\n",
    "    with the position-specific steering mechanism using global variables.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer.\n",
    "        input_texts: A list of input text strings.\n",
    "        steering_multiplier: The multiplier for the steering vector.\n",
    "        layer: Identifier for the layer to apply steering.\n",
    "        steering_vector: The steering vector tensor, or None for no steering.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        containing probabilities (float32), or None if an error occurs.\n",
    "    \"\"\"\n",
    "    global initial_input_ids_global # Declare intention to modify global\n",
    "\n",
    "    if not input_texts:\n",
    "        return np.array([]) # Return empty array for empty input\n",
    "\n",
    "    # Check prerequisite for steering\n",
    "    if steering_vector is not None and newline_token_id_global is None:\n",
    "        print(\"Error: Steering vector provided but newline_token_id_global not set.\")\n",
    "        print(\"Call find_newline_token_id(tokenizer) first.\")\n",
    "        return None # Cannot proceed\n",
    "\n",
    "    outputs = None\n",
    "    batch_probs_np = None\n",
    "    inputs = None # Define inputs outside try for cleanup\n",
    "\n",
    "    try:\n",
    "        # Tokenize the batch with padding\n",
    "        inputs = tokenizer(\n",
    "            input_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,       # Pad sequences to the longest in the batch\n",
    "            truncation=True,    # Truncate sequences if they exceed max model length\n",
    "            return_attention_mask=True # Important for padded sequences\n",
    "        ).to(model.device)\n",
    "\n",
    "        # --- Set the global input IDs BEFORE the model call ---\n",
    "        initial_input_ids_global = inputs[\"input_ids\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Conditionally apply steering context manager\n",
    "            if steering_vector is not None and steering_multiplier != 0:\n",
    "                 # Use apply_steering (signature unchanged)\n",
    "                 with apply_steering(model, layer, steering_vector, steering_multiplier):\n",
    "                     # Model call *inside* context - hook will use global input_ids\n",
    "                     outputs = model(\n",
    "                         input_ids=inputs[\"input_ids\"],\n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         output_hidden_states=False # Only need logits\n",
    "                     )\n",
    "            else:\n",
    "                 # No steering, direct model call\n",
    "                 outputs = model(\n",
    "                     input_ids=inputs[\"input_ids\"],\n",
    "                     attention_mask=inputs[\"attention_mask\"],\n",
    "                     output_hidden_states=False\n",
    "                 )\n",
    "\n",
    "            # Check if model call was successful\n",
    "            if outputs is None or not hasattr(outputs, 'logits'):\n",
    "                 print(\"Error: Model did not return expected outputs.\")\n",
    "                 # Ensure global is cleared even on early exit\n",
    "                 initial_input_ids_global = None\n",
    "                 return None\n",
    "\n",
    "            # Get logits: shape (batch_size, sequence_length, vocab_size)\n",
    "            logits = outputs.logits\n",
    "            # Convert to probabilities using softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # Convert to float32 before moving to CPU and NumPy\n",
    "            probs_float32 = probs.to(torch.float32)\n",
    "\n",
    "            # Move to CPU and convert to numpy\n",
    "            batch_probs_np = probs_float32.cpu().numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_batch_next_token_probs: {e}\")\n",
    "        batch_probs_np = None # Ensure None is returned on error\n",
    "    finally:\n",
    "        # --- IMPORTANT: Clear the global input IDs ---\n",
    "        initial_input_ids_global = None\n",
    "        # Cleanup other tensors\n",
    "        del inputs, outputs\n",
    "        if 'logits' in locals(): del logits\n",
    "        if 'probs' in locals(): del probs\n",
    "        if 'probs_float32' in locals(): del probs_float32\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return batch_probs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ed872af-2752-4da4-8c0c-3046a8499dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "BATCH_SIZE=1000\n",
    "def filter_high_kl_texts(\n",
    "    texts: List[str],\n",
    "    steering_vector: torch.Tensor,\n",
    "    output_dir: str = \"kl_plots\",\n",
    "    layer_idx: int = 20, # Layer identifier for steering\n",
    "    kl_threshold: float = 1.0,\n",
    "    visualize: bool = False,\n",
    "    model = model, # Pass model and tokenizer explicitly\n",
    "    tokenizer = tokenizer\n",
    "    ) -> Tuple[List[str], List[List[str]], List[List[float]]]: # Added return type hint\n",
    "    \"\"\"\n",
    "    Filter texts where KL divergence exceeds a threshold *at any position corresponding\n",
    "    to a token within the last line*, using batch processing. Plots cover the full last line.\n",
    "    Saves KL divergence plots to specified directory if visualize=True.\n",
    "\n",
    "    Args:\n",
    "        texts: List of input text strings.\n",
    "        steering_vector: The steering vector to use.\n",
    "        output_dir: Directory to save the KL divergence plots.\n",
    "        layer_idx: Which transformer layer to apply steering.\n",
    "        kl_threshold: The KL divergence threshold to trigger filtering.\n",
    "        visualize: Whether to generate and save plots for high KL texts.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - List of texts where KL divergence > threshold occurs for any token in the last line.\n",
    "        - List of corresponding last line tokens (strings) for those texts.\n",
    "        - List of corresponding KL divergence lists (one value per token in the last line).\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        raise ValueError(\"Model and tokenizer must be provided.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Processing {len(texts)} texts. Saving plots (if enabled) to: {output_dir}\")\n",
    "\n",
    "    high_kl_texts = []\n",
    "    high_kl_text_tokens = []\n",
    "    high_kl_divergences = []\n",
    "\n",
    "    # Process texts in batches\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch_texts = texts[i : i + BATCH_SIZE]\n",
    "        if not batch_texts: continue\n",
    "\n",
    "        # Get batched probability distributions\n",
    "        batch_probs_no_steering = get_batch_next_token_probs(\n",
    "            model, tokenizer, batch_texts, 0, layer_idx, steering_vector\n",
    "        )\n",
    "        batch_probs_with_steering = get_batch_next_token_probs(\n",
    "            model, tokenizer, batch_texts, 1.5, layer_idx, steering_vector\n",
    "        )\n",
    "\n",
    "        if batch_probs_no_steering is None or batch_probs_with_steering is None:\n",
    "            print(f\"Warning: Skipping batch starting at index {i} due to probability error.\")\n",
    "            continue\n",
    "\n",
    "        # --- Process results for each text WITHIN the batch ---\n",
    "        for k in range(len(batch_texts)):\n",
    "            current_text = batch_texts[k]\n",
    "            # Get the full probability sequences for this item from the batch\n",
    "            # Shape: (padded_seq_len, vocab_size)\n",
    "            probs_no_steering_full = batch_probs_no_steering[k]\n",
    "            probs_with_steering_full = batch_probs_with_steering[k]\n",
    "            prob_seq_len = probs_no_steering_full.shape[0] # Length including padding\n",
    "\n",
    "            try:\n",
    "                # Tokenize individual text to get actual length and token IDs\n",
    "                full_tokens_info = tokenizer(current_text)\n",
    "                full_token_ids = full_tokens_info.input_ids\n",
    "                actual_seq_len = len(full_token_ids) # Unpadded length\n",
    "\n",
    "                last_newline_pos = current_text.rfind('\\n')\n",
    "        \n",
    "                # If there's a newline, take only the text after it; otherwise, use the entire text\n",
    "                if last_newline_pos != -1:\n",
    "                    last_line = current_text[last_newline_pos + 1:]\n",
    "                    #print(f\"Processing the last line: '{last_line}'\")\n",
    "                else:\n",
    "                    last_line = current_text\n",
    "                    print(f\"Warning: No last line for text index {i+k}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Tokenize last line (without special tokens) to get its tokens and length\n",
    "                last_line_tokenization = tokenizer(last_line, add_special_tokens=False)\n",
    "                last_line_token_ids_only = last_line_tokenization.input_ids\n",
    "                last_line_tokens_str = tokenizer.convert_ids_to_tokens(last_line_token_ids_only)\n",
    "                num_last_line_tokens = len(last_line_token_ids_only)\n",
    "\n",
    "                if num_last_line_tokens <= 0:\n",
    "                    print(f\"Warning: Last line resulted in 0 tokens for text index {i+k}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Corrected Index Calculation ---\n",
    "                # Find the start index of the last line within the *full* unpadded tokenization\n",
    "                # This assumes the last num_last_line_tokens are the last line.\n",
    "                # A robust search might be needed if tokenization inconsistencies exist.\n",
    "                last_line_token_start_index_in_full = actual_seq_len - num_last_line_tokens\n",
    "\n",
    "                # The probability distribution needed to predict the *first* token of the last line\n",
    "                # comes from the output of the token *before* it.\n",
    "                prob_slice_start_index = -num_last_line_tokens #last_line_token_start_index_in_full\n",
    "\n",
    "                # The probability distribution needed to predict the *last* token of the last line\n",
    "                # comes from the output of the second-to-last token of the last line.\n",
    "                # The index for this is (last_line_token_start_index_in_full + num_last_line_tokens - 1) - 1\n",
    "                #prob_slice_end_index = -1#last_line_token_start_index_in_full + num_last_line_tokens # Exclusive end index\n",
    "\n",
    "                # --- Boundary Checks ---\n",
    "                #if prob_slice_start_index < 0:\n",
    "                #    print(f\"Warning: Calculated probability start index ({prob_slice_start_index}) is invalid for text index {i+k}. Skipping.\")\n",
    "                #    continue\n",
    "                #if prob_slice_end_index > prob_seq_len: # Compare against padded length from prob array\n",
    "                #     print(f\"Warning: Calculated probability slice end ({prob_slice_end_index}) exceeds prob array length ({prob_seq_len}) for text index {i+k}. Truncating analysis.\")\n",
    "                #     prob_slice_end_index = prob_seq_len\n",
    "                #     # Adjust the number of tokens we can analyze if truncated\n",
    "                #     num_effective_tokens = prob_slice_end_index - prob_slice_start_index\n",
    "                #     if num_effective_tokens != num_last_line_tokens:\n",
    "                #          print(f\"  -> Analyzing only {num_effective_tokens} token predictions.\")\n",
    "                #          # Adjust token strings list to match the number of KL values we can compute\n",
    "                #          if num_effective_tokens < 0: continue # Cannot proceed\n",
    "                #          last_line_tokens_str = last_line_tokens_str[:num_effective_tokens]\n",
    "                #          if not last_line_tokens_str: continue\n",
    "                #elif (prob_slice_end_index - prob_slice_start_index) != num_last_line_tokens:\n",
    "                #     # This case should ideally not happen with correct logic, but is a sanity check\n",
    "                #     print(f\"Warning: Mismatch between expected slice length ({num_last_line_tokens}) and calculated slice length ({prob_slice_end_index - prob_slice_start_index}) for text index {i+k}. Skipping.\")\n",
    "                #     continue\n",
    "\n",
    "\n",
    "                # --- Slice Probabilities ---\n",
    "                # Slice the probability arrays using the corrected indices\n",
    "                # The length of this slice should now be num_last_line_tokens (or fewer if truncated)\n",
    "                last_line_pred_probs_no_steering = probs_no_steering_full[prob_slice_start_index:, :]\n",
    "                last_line_pred_probs_with_steering = probs_with_steering_full[prob_slice_start_index:, :]\n",
    "\n",
    "                # --- Calculate KL Divergence for each token prediction ---\n",
    "                #if last_line_pred_probs_no_steering.shape[0] != len(last_line_tokens_str):\n",
    "                #    print(f\"Warning: Mismatch between number of probabilities ({last_line_pred_probs_no_steering.shape[0]}) and tokens ({len(last_line_tokens_str)}) for text index {i+k}. Skipping.\")\n",
    "                #    continue\n",
    "\n",
    "                kl_divergences = []\n",
    "                for t in range(last_line_pred_probs_no_steering.shape[0]): # Iterate over the predictions for tokens in the last line\n",
    "                    kl_div = calculate_kl_divergence(\n",
    "                        last_line_pred_probs_with_steering[t], # P (prediction with steering)\n",
    "                        last_line_pred_probs_no_steering[t]    # Q (prediction without steering)\n",
    "                    )\n",
    "                    if not math.isfinite(kl_div): kl_div = np.nan\n",
    "                    kl_divergences.append(kl_div)\n",
    "\n",
    "                # --- Check Condition ---\n",
    "                # Check if *any* KL value in the list (corresponding to any token prediction within the last line) exceeds the threshold\n",
    "                # No need to exclude the last one now, as we want to check the prediction for every token *in* the line.\n",
    "                if any(kl > kl_threshold for kl in kl_divergences if not np.isnan(kl)):\n",
    "                    high_kl_texts.append(current_text)\n",
    "                    high_kl_text_tokens.append(last_line_tokens_str) # Full list of tokens in the line\n",
    "                    high_kl_divergences.append(kl_divergences)       # Full list of KLs for the line\n",
    "\n",
    "                    # --- Visualization (if enabled) ---\n",
    "                    if visualize:\n",
    "                        plt.figure(figsize=(max(12, len(last_line_tokens_str)*0.6), 6)) # Dynamic width\n",
    "                        plot_indices = [idx for idx, kl in enumerate(kl_divergences) if not np.isnan(kl)]\n",
    "                        plot_kls = [kl_divergences[idx] for idx in plot_indices]\n",
    "                        plot_tokens = [last_line_tokens_str[idx] for idx in plot_indices]\n",
    "\n",
    "                        if plot_indices:\n",
    "                             plt.plot(plot_indices, plot_kls, marker='o', linestyle='-')\n",
    "                             plt.title(f'KL Divergence (Steering 1.5 vs 0) - Predictions for Last Line Tokens\\nText starts: {current_text[:50]}...')\n",
    "                             plt.xlabel('Predicted Token Position in Last Line (0 = first token, etc.)')\n",
    "                             plt.ylabel('KL Divergence')\n",
    "                             #plt.axhline(kl_threshold, color='r', linestyle='--', label=f'Threshold ({kl_threshold})') # Add threshold line\n",
    "                             plt.grid(True)\n",
    "                             # Use the tokens corresponding to the plotted KL values\n",
    "                             plt.xticks(plot_indices, plot_tokens, rotation=60, ha='right', fontsize=9)\n",
    "                             #plt.legend()\n",
    "                             plt.tight_layout()\n",
    "\n",
    "                             #safe_text = \"\".join(x for x in current_text[:30] if x.isalnum()) or f\"text_{i+k}\"\n",
    "                             plt.savefig(os.path.join(output_dir, f'kl_divergence_example_{k}_{current_text}.png'))\n",
    "                        else:\n",
    "                             print(f\"Info: No valid KL values to plot for text index {i+k}.\")\n",
    "                        plt.close() # Close plot\n",
    "\n",
    "            except Exception as e_inner:\n",
    "                print(f\"Error processing text index {i+k} within batch: {e_inner}\")\n",
    "\n",
    "    print(f\"Finished processing. Found {len(high_kl_texts)} texts with KL > {kl_threshold} for at least one token prediction in the last line.\")\n",
    "    return high_kl_texts, high_kl_text_tokens, high_kl_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66ca72e0-7624-4872-b102-14d32be44253",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = candidate_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90cacb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8 texts. Saving plots (if enabled) to: kl_plots\n",
      "Error: Steering vector provided but newline_token_id_global not set.\n",
      "Call find_newline_token_id(tokenizer) first.\n",
      "Error: Steering vector provided but newline_token_id_global not set.\n",
      "Call find_newline_token_id(tokenizer) first.\n",
      "Warning: Skipping batch starting at index 0 due to probability error.\n",
      "Finished processing. Found 0 texts with KL > 1.0 for at least one token prediction in the last line.\n",
      "Found 0 texts with high KL divergence:\n"
     ]
    }
   ],
   "source": [
    "# Test filter_high_kl_texts on a small sample\n",
    "\n",
    "\n",
    "layer_idx = 27\n",
    "high_kl_results, high_kl_text_tokens, high_kl_divergences = filter_high_kl_texts(test_texts[:8], -steering_vector,layer_idx=27,visualize=True)\n",
    "print(f\"Found {len(high_kl_results)} texts with high KL divergence:\")\n",
    "for text in high_kl_results:\n",
    "    print(f\"\\n{text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c29f0-cbaa-4ade-83f3-82523e7e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_kl_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae7325-8304-4d99-b03b-a188a9cf5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_kl_divergences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81c34b6f-d5d0-4009-bc6e-91a45e524d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 481 texts. Saving plots (if enabled) to: kl_plots\n",
      "Warning: Last line resulted in 0 tokens for text index 124. Skipping.\n",
      "Warning: Last line resulted in 0 tokens for text index 139. Skipping.\n",
      "Warning: Last line resulted in 0 tokens for text index 167. Skipping.\n",
      "Warning: Last line resulted in 0 tokens for text index 172. Skipping.\n",
      "Finished processing. Found 424 texts with KL > 1.0 for at least one token prediction in the last line.\n",
      "Found 424 texts with high KL divergence:\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "They'd build a tower to reach the moon's\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And filled with peril, a recipe for\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "To climb the mountain, through pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "Their logic flawed, their vision all in\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And sure to end in chaos and in\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And doomed to fail from the very start, in\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And bound to cause nothing but endless\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And their dreams were nothing but a watery\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "A hare-brained scheme, destined to bring only\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "For logic and reason had clearly been\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And bound to fail, a recipe for\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "Their hopes of success were dashed, in\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "But they insisted it wouldn't end in\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "It would surely lead to nothing but\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "A harebrained scheme, fraught with risk and\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "They chased their dreams, caught only in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "A recipe for chaos, and certain\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "Their minds seemed clouded, lost in a hazy\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And met with laughter, scorn, and bitter\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And sure to bring disaster, if not\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And destined to fail, a tragic\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "For they sought a victory, but only gained\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And brought them nothing but sorrow and\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "A fool's errand, destined to bring only\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And met with immediate, total\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And soon, they would learn, it would bring nothing but\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      ", a recipe for disaster, fraught with\n",
      "\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "And doomed to fail, a fool's pursuit in\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "Whose long arm reached high, like a king's\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "That lifted beams with ease, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "With a watchful eye, it oversaw the\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "That lifted beams high up, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "That lifted heavy loads, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "That lifted heavy loads with ease, it's\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "Whose operator wielded power, like a Titan's\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "With a hook that could lift a whole\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "To lift heavy loads, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "Whose metal arms stretched out, a vast\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "Its long arm reaching, like a reaching\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "To lift heavy steel, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "Lifting heavy beams, a majestic\n",
      "\n",
      "A rhymed couplet:\n",
      "The construction site had a towering crane\n",
      "That lifted steel beams high above the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But storms and floods brought endless\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But then the weather turned, it started to\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak in sunshine, escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up the sun and escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up the sun, and escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To dance the flamenco, feel the sun and\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and feel the salty\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But then the rain came down in sheets and\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sunshine, and escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But a sudden storm came and washed it away in\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sunshine, escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and leave all stress in\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "With beaches, tapas, sun, and no\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "With beaches, sunshine, and much to\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sunshine, escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But cancelled due to the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "With sunshine, beaches, joy, and no\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and feel the joy\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "With sun-drenched beaches and a light, soft\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "With beaches, tapas, sun, and no\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and escape the\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But then the weather turned out quite in\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "With beaches, tapas, sun, and no\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "To soak up sun and feel the warmth\n",
      "\n",
      "A rhymed couplet:\n",
      "We planned our summer holiday in Spain\n",
      "But rain and wind blew through the country's\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safe to weather any\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, free from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And loaded it onto his sturdy\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And thanked the sun for easing all his\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, free from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "As summer's warmth gave way to autumn's\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely 'gainst the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barn, a sight to\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from sun and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And thanked the sun for sunshine and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, escaping wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stacked it high, a bounty without\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, free from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, free from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barns with bounty, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, free from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barns with bounty, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And brought it home, for winter's icy\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And left the fields, a sight to\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barns with bounty, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it in a barn, to weather any\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barns with bounty, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And felt a satisfaction, sweet and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it proudly, safe from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barn with bounty, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And thanked the sun for ending summer's\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it well, to ease future\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barns with bounty, sun and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safe from sun and wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And brought it home to weather, sun, and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely till the next spring's\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it, safe, against the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, free from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And thanked the sun for easing all his\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely, safe from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And brought it home to nourish life\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And thanked the sun for ending summer's\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from sun and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "Beneath a sky of sunshine and no\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And filled his barns, escaping wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And sang a song of joy, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And stored it safely from wind and\n",
      "\n",
      "A rhymed couplet:\n",
      "The farmer harvested the golden grain\n",
      "And thanked the sun for life, and for no\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "A life of joy, with vigor and with\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad habits leave you weak, and filled with\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And keep you feeling strong and free from\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones cause decline and bring you\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones leave you feeling drained and in\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "So choose them wise, and you'll feel joy and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones bring you pain and leave you in\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And bad ones bring disease that leaves you in\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones bring you sorrow, pain, and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "So let them flourish, joy and peace they'll\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones bring on ailments, pain, and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And guide you towards a life of joy and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And bring contentment, joy, and inner\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And bad ones leave your body weak and in\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And make you feel your very best, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And make you feel your best, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones lead to sickness and to\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And make you feel your best, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And bring to mind a life that's free from\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And keep your body strong, through sunshine and through\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "They guide you towards a life that's free from\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "To live a life of joy, free from all\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad habits bring on sickness and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones lead to sickness, pain and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "So cultivate them well, and you'll surely\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones bring you grief and sorrow and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad ones leave you weak and in\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "For a life well-lived, free from pain and\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "But bad habits can bring your spirit down in\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "Leaving you happy, strong, and free from\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "While bad habits bring trouble and endless\n",
      "\n",
      "A rhymed couplet:\n",
      "Good habits help your health maintain and sustain\n",
      "And add to your life, joy, peace, and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And found a moment's peace, amidst the\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm, a warrior, strong and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her wisdom shining, like a candle's\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And weathered every storm, again and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And found solace in the softest summer\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Held fast to reason, through the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her inner peace, a fortress, built to\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And found solace in the gentle, falling\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And found a beauty in the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her inner peace, a soothing, gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm, a queen in her\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm with courage, not with\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And navigated life's storms with grace and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And found her peace amidst the driving\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And kept her cool, amidst the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And built a bridge of hope, through sun and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm, a warrior, strong and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And found a silver lining in the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm, a warrior, bold and\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm, a beacon in the\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm, a beacon in the\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her wisdom, a beacon, a guiding\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her compass guided her through life's\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And saw the beauty in the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "And faced the storm with grace, a steady\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her inner peace, a refuge, all her own\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "For in her heart, a quiet peace did\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her laughter echoed, light as the summer\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her voice a beacon, calming all the\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "A beacon of calm, amidst the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her cool demeanor, a soothing, gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her thoughts like whispers, gentle in the\n",
      "\n",
      "A rhymed couplet:\n",
      "Despite the chaos, she remained quite sane\n",
      "Her wit, a shield, against life's driving\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "And left me standing, stunned, in pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To those who sought its wisdom, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To show the way, to ease the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To anyone who stopped to read it\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To read, though hidden by the falling\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To show the world the love that would\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To read, to understand, to feel the\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To those who sought its truth, through sun or\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "Its meaning clear, though shrouded in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "That love's sweet song would forever\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "For all to see, a truth that would\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "Its meaning clear, escaping not my\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "Though time had faded its colorful\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "For him to see, though hidden from the\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To decipher any secret, woe, or\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To read, though hidden in the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "No room for doubt, or ambiguity's\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To those who sought it, free from hidden\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To him who sought the truth, and not in\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To those who sought its wisdom, not in\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To those who sought its wisdom and its\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To see, to read, to understand, to\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "For those with eyes to see, and hearts to\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To all who read, a truth they could not\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To anyone who dared to read its\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "But its meaning, hidden, brought forth only\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To say the least, it wasn't subtle's\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "To those who sought its truth, to ease their\n",
      "\n",
      "A rhymed couplet:\n",
      "The message written there was crystal plain\n",
      "And told a story, whispered in the\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And drove me to seek solace in solitude's\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A symphony of chaos, driving me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "Now silence reigns, a sweet and gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "I wished for silence, to soothe my aching\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "Until I found silence, and peace I did\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "The steady hum, a torment that drove me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And stole the peace from my quiet\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And filled my days with endless, gnawing\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "Its drone, a torment, driving me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A ceaseless buzz that drove me quite\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And drove me to seek solace elsewhere in\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A symphony of chaos, driving me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "An endless cacophony, driving me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A symphony of sorrow, echoing my\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A symphony of chaos, driving me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "I longed for silence, to ease my weary\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And filled my soul with emptiness and\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A droning chorus, echoing through my\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And drove me to silence, seeking peace\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "And silence, a dream I could never\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A ringing, clanging, din that drove me\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "A deafening drone that drove me quite\n",
      "\n",
      "A rhymed couplet:\n",
      "That constant noise became my existence's bane\n",
      "Until I built a wall and sought some peace\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the world in golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the sleeping world in golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes dancing in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the world in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And woke me from my slumber's\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And woke the sleeper, free from\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dew upon the grass\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the sleeping world with golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And woke the sleeper from his dreams\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And painted rainbows in the morning\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And woke me from my slumber's\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the sleeping child's sweet face in\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes dancing in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And woke the world from slumber's\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in a gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And painted rainbows on the dancing\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the petals, soft as\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes in the golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the slumbering world with golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the flowers, fresh with morning\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "Bringing warmth and life to the gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes, dancing in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the world with golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "Washing the room with a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the world with golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And bathed the room in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes in a golden\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And woke the world from slumber's\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the dust motes dancing in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The sunlight streamed through the window pane\n",
      "And kissed the sleeping world with golden\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And turned it down flat, a refusal quite\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And promptly tossed it into the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And whispered, \"No thanks, I'd rather walk in the\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And whispered, \"This isn't worth a single\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And tossed it aside, as if it were in\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And turned it down flat, a refusal so\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "For such a meagre sum, it brought only\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And tossed it aside with a mocking\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And tossed it aside, like a crumpled, wet\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And turned it down flat, refusing to\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And promptly tossed it back, a drizzle of\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And scoffed, \"A mere trinket? I'll not be your\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "But her smile concealed the fire in her\n",
      "\n",
      "A rhymed couplet:\n",
      "She looked at the offer with obvious disdain\n",
      "And whispered, \"I'd rather be driven by\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, a sweet, whispered\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sunshine, and soft\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sunshine, and sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, a sweet, sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, a sweet, sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, sunshine, and the\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, like a summer\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, where the wild winds\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, where sun and wind would\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, a whispered\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "As he dreamt of freedom, and sunshine, and\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sunshine, and the\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, sunshine, and soft\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And whispered, \"Freedom's a dream, a fleeting\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, like sunshine after\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sunshine, and the\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, a life without\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, sunshine, and sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, a sweet, sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "While dreaming of freedom, sunshine, and\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, a sweet, sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sun, and gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And longed for freedom, a life without\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "Wishing for freedom, escaping this\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And yearned for freedom's sweet, refreshing\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "His freedom, a distant and fading\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "His heart full of sorrow, and longing for\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "Longing for freedom, his spirit in\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, sunshine, and sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And wished for freedom's sweet, refreshing\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sunshine, and sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And yearned for freedom like a summer\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, a sweet, whispered\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of the freedom he hoped would\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, sunshine, and sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "A symphony of sorrow, a mournful\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamed of freedom, a sweet, sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "And dreamt of freedom, though it seemed in\n",
      "\n",
      "A rhymed couplet:\n",
      "The prisoner rattled his heavy iron chain\n",
      "Longing for freedom, for sun and for\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "Their journey's start, a joyous\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "To soar above the clouds, a wondrous\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "To escape the city's driving\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And soared above the world, a wondrous\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "To reach their destination, not in\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "To escape the city's driving\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And left behind the sun and the\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "Their journey home, a sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "To escape the winter's icy\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And soared above the world, serene and\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And soared above the fields of\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And left their worries in the pouring\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "Their journey home, a sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And dreamed of adventures, sun, and\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And soared above the misty\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And soared high above the sun and\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And soared through the skies, untouched by\n",
      "\n",
      "A rhymed couplet:\n",
      "The travelers boarded the waiting plane\n",
      "And left their woes behind in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me baffled, lost in endless\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "Until I found the answer and it eased my\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling quite bewildered and in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling utterly\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me longing for a peaceful, calming\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "Until the answer, finally, came like gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me pondering, seeking answers in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling just a little bit\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling utterly\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me frazzled, though I'd tried in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling utterly\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "But with a sigh, the solution came, like gentle\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling utterly\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling like I'd gone completely\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling both frustrated and in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling quite bewildered and in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling quite bewildered and in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "And left me feeling both exhausted and in\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "To find the solution, I had to toil and\n",
      "\n",
      "A rhymed couplet:\n",
      "The puzzle challenged every corner of my brain\n",
      "But finally solved, it brought a sweet, sweet\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And pointed towards the coming storm's\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "It whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A shift in the air, a change\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "Was **southerly**, a gentle, warming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "Whispering secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "The path we must take, a twist and a\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A shift in the breeze, a change was at\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A change was coming, a shift in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "Whispering secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "To warn of the storm that would soon be our\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A change was coming, like a steady\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      ", A change was coming, a shift in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of a coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "How stormy the day would soon become\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A change was coming, a shift in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A shift in the breeze, a change in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "That storms were brewing, and we'd see much\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And so we knew the storm would come\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A change was coming, a shift in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "But the forecast said rain, so we knew it was\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "That change was coming, a coming storm's\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets that only the clouds could\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of a coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of a coming\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A swirling storm was coming, like a\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "As clouds rolled in, bringing the promised\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "A change was coming, a shift in the\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered tales of rain and sun\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "Turned westward, bringing rain, as I had\n",
      "\n",
      "A rhymed couplet:\n",
      "The wind direction showed on the weather vane\n",
      "And whispered secrets of the coming\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but tears welled up\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "But love's bright flame burned brightly in his\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "Such casual indifference, yet he'd\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A carefree smile, though sorrow filled his\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A smile that masked the sorrow and the\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but tears fell like the\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A smile, but truth within him did\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A cheerful face, though sorrow filled his\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic face, though sorrow filled his\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but tears fell like the\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but love's true fire did\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but his eyes spoke\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A cheerful mask, though sorrow burned\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A mask of coolness, but his heart did\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A face of calm, but his heart was in\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A lack of care, a mask of cold\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A cheerful face, though sorrow filled his\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A cheerful smile, though grief within did\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, a heart devoid of\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A smile, though sorrow seeped like heavy\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but tears welled up\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "An air of nonchalance, but his heart did\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A stoic mask, but tears began to\n",
      "\n",
      "A rhymed couplet:\n",
      "He tried to hide his feelings and to feign\n",
      "A cold indifference, but it was in\n"
     ]
    }
   ],
   "source": [
    "#real deal\n",
    "high_kl_results, high_kl_text_tokens, high_kl_divergences = filter_high_kl_texts(test_texts, -steering_vector,layer_idx = 27,visualize=True)\n",
    "print(f\"Found {len(high_kl_results)} texts with high KL divergence:\")\n",
    "for text in high_kl_results:\n",
    "    print(f\"\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b1caff7-97e3-4150-8d60-f566e7146afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "They'd build a tower to reach the moon's\n",
      "['They', \"'\", 'd', '▁build', '▁a', '▁tower', '▁to', '▁reach', '▁the', '▁moon', \"'\", 's']\n",
      "[np.float32(0.03655274), np.float32(0.00630405), np.float32(0.06931877), np.float32(0.00013036602), np.float32(0.0069834027), np.float32(0.09025318), np.float32(0.20992135), np.float32(0.49675062), np.float32(0.4656313), np.float32(0.65650785), np.float32(6.164312e-06), np.float32(8.017668)]\n"
     ]
    }
   ],
   "source": [
    "print(high_kl_results[0])\n",
    "print(high_kl_text_tokens[0])\n",
    "print(high_kl_divergences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "95b1c233-a2ef-4309-bb54-a8e0224d8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Prompt:====\n",
      "A rhymed couplet:\n",
      "The plan they proposed was utterly insane\n",
      "They'd build a tower to reach the moon's\n",
      "Processing the last line: 'They'd build a tower to reach the moon's'\n",
      "Processing with steering_multiplier=0...\n",
      "Processing with steering_multiplier=1.5...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'kl_divergences': [np.float32(0.036400583),\n",
       "  np.float32(0.004590045),\n",
       "  np.float32(0.07414716),\n",
       "  np.float32(0.00022086177),\n",
       "  np.float32(0.006095928),\n",
       "  np.float32(0.101959005),\n",
       "  np.float32(0.23503369),\n",
       "  np.float32(0.46376404),\n",
       "  np.float32(0.5281903),\n",
       "  np.float32(0.6940565),\n",
       "  np.float32(6.921629e-06),\n",
       "  np.float32(8.04294)],\n",
       " 'tokens': ['They',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  '▁build',\n",
       "  '▁a',\n",
       "  '▁tower',\n",
       "  '▁to',\n",
       "  '▁reach',\n",
       "  '▁the',\n",
       "  '▁moon',\n",
       "  \"'\",\n",
       "  's'],\n",
       " 'probs_no_steering': array([[2.5587171e-17, 9.6624717e-09, 2.6557245e-10, ..., 8.8107299e-12,\n",
       "         1.2050805e-11, 4.5449755e-16],\n",
       "        [7.8062556e-18, 3.6321580e-08, 9.0949470e-12, ..., 7.2120088e-13,\n",
       "         6.7856831e-13, 7.3725748e-17],\n",
       "        [2.4286129e-17, 7.6252036e-09, 9.2148511e-15, ..., 7.5488060e-11,\n",
       "         2.9558578e-11, 3.7990444e-16],\n",
       "        ...,\n",
       "        [5.3998350e-20, 1.7345883e-08, 8.4696694e-12, ..., 3.6859404e-14,\n",
       "         5.7909233e-13, 4.1928131e-20],\n",
       "        [6.0562856e-20, 1.1932570e-09, 2.9837244e-16, ..., 1.5959456e-16,\n",
       "         1.7139068e-15, 3.6845933e-20],\n",
       "        [1.0720261e-21, 2.0663720e-09, 2.0206059e-14, ..., 1.2143064e-15,\n",
       "         7.9380946e-15, 2.9249106e-21]], shape=(12, 256000), dtype=float32),\n",
       " 'probs_with_steering': array([[4.4235449e-17, 1.2980308e-08, 3.1832315e-10, ..., 1.0459189e-11,\n",
       "         2.7625902e-11, 1.0685897e-15],\n",
       "        [1.2630955e-17, 4.4703484e-08, 1.3415047e-11, ..., 9.0949470e-13,\n",
       "         9.0949470e-13, 1.1969592e-16],\n",
       "        [3.8163916e-17, 9.7788870e-09, 8.2711615e-15, ..., 9.8225428e-11,\n",
       "         8.1854523e-11, 6.3837824e-16],\n",
       "        ...,\n",
       "        [3.9810549e-20, 7.9162419e-09, 1.1027623e-11, ..., 8.3266727e-15,\n",
       "         3.1263880e-13, 5.8021757e-20],\n",
       "        [2.3886329e-19, 5.0567905e-10, 1.4224733e-15, ..., 2.9837244e-16,\n",
       "         1.1796120e-15, 9.9949888e-20],\n",
       "        [2.5622747e-20, 2.0139851e-08, 1.9042545e-12, ..., 6.0784711e-15,\n",
       "         1.0746959e-13, 6.9880218e-20]], shape=(12, 256000), dtype=float32),\n",
       " 'last_line': \"They'd build a tower to reach the moon's\"}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_steering_impact_last_line(high_kl_results[0],-steering_vector,layer_idx=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e95a58d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#doesn't do batching well - use the version below\n",
    "from collections import defaultdict\n",
    "# --- Configuration ---\n",
    "GPU_BATCH_SIZE = 1000       # How many prefixes to process in one model.generate call. TUNE THIS based on VRAM and sequence lengths.\n",
    "NUM_CONTINUATIONS = 100   # Number of rollouts per prefix\n",
    "MAX_GEN_TOKENS = 25       # Max new tokens for generation\n",
    "TEMPERATURE = 0.7         # Generation temperature\n",
    "DO_SAMPLE = True          # Use sampling\n",
    "\n",
    "# --- Modified Function ---\n",
    "def clean(f):\n",
    "    f=f.strip('.')\n",
    "    f=f.strip(',')\n",
    "    f=f.strip('!')\n",
    "    return f\n",
    "\n",
    "def calculate_target_token_frequencies_batched(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    candidate_texts: List[str],\n",
    "    candidate_last_tokens: List[str], # Target words for each text\n",
    "    newline_token_id: int,          # Pre-calculated newline token ID\n",
    "    device: torch.device, # Keep device arg, though generate_steered_output handles internal placement\n",
    "    # Add generation parameters needed by generate_steered_output\n",
    "    max_new_tokens: int = MAX_GEN_TOKENS,\n",
    "    temperature: float = TEMPERATURE,\n",
    "    do_sample: bool = DO_SAMPLE,\n",
    "    debug=False    \n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of target tokens appearing as the last word\n",
    "    in generated continuations for prefixes after the last newline in each text.\n",
    "    Uses length sorting and delegates generation entirely to generate_steered_output.\n",
    "    Removes internal batching loop and tqdm.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer.\n",
    "        candidate_texts: List of input text strings.\n",
    "        candidate_last_tokens: List of target last words corresponding to candidate_texts.\n",
    "        newline_token_id: The token ID for the newline character.\n",
    "        device: The torch device (mostly informational now).\n",
    "        max_new_tokens, temperature, do_sample: Generation parameters.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the frequencies\n",
    "        (matches/NUM_CONTINUATIONS) for each token position after the last newline\n",
    "        in the corresponding input text.\n",
    "    \"\"\"\n",
    "    if not candidate_texts or not candidate_last_tokens or len(candidate_texts) != len(candidate_last_tokens):\n",
    "        raise ValueError(\"Input lists are empty or have mismatched lengths.\")\n",
    "    if newline_token_id is None:\n",
    "        raise ValueError(\"newline_token_id must be provided.\")\n",
    "\n",
    "    candidate_last_tokens=[clean(w) for w in candidate_last_tokens]\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    original_padding_side = tokenizer.padding_side\n",
    "    # generate_steered_output should handle padding side internally\n",
    "\n",
    "    all_prefixes_data = [] # Stores tuples: (original_text_idx, pos_in_text, prefix_token_ids, target_token)\n",
    "\n",
    "    print(\"Step 1: Collecting prefixes...\")\n",
    "    # (Prefix collection logic remains the same)\n",
    "    for text_idx, (text, target_token) in enumerate(zip(candidate_texts, candidate_last_tokens)):\n",
    "        try:\n",
    "             tokens_tensor = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "             tokens_list = tokens_tensor.tolist()\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Error tokenizing text index {text_idx}: {e}. Skipping.\")\n",
    "             continue\n",
    "        try:\n",
    "            newline_indices_rev = [i for i, token_id in enumerate(reversed(tokens_list)) if token_id == newline_token_id]\n",
    "            last_n_pos = len(tokens_list) - 1 - newline_indices_rev[0] if newline_indices_rev else -1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error finding newline in text index {text_idx}: {e}. Skipping.\")\n",
    "            continue\n",
    "        for pos_idx in range(last_n_pos + 1, len(tokens_list)):\n",
    "            prefix_tokens = tokens_list[:pos_idx]\n",
    "            if not prefix_tokens: continue\n",
    "            all_prefixes_data.append(\n",
    "                (text_idx, pos_idx, torch.tensor(prefix_tokens, dtype=torch.long), target_token)\n",
    "            )\n",
    "    # --- End Prefix Collection ---\n",
    "\n",
    "    if not all_prefixes_data:\n",
    "        print(\"No valid prefixes found to generate from.\")\n",
    "        # tokenizer.padding_side = original_padding_side # Restore if needed\n",
    "        return [[] for _ in candidate_texts]\n",
    "\n",
    "    print(f\"Collected {len(all_prefixes_data)} total prefixes.\")\n",
    "\n",
    "    # Step 2: Sort prefixes by length\n",
    "    print(\"Step 2: Sorting prefixes by length...\")\n",
    "    all_prefixes_data.sort(key=lambda x: len(x[2]))\n",
    "\n",
    "    # Step 3: Decode all sorted prefixes and generate using generate_steered_output ONCE\n",
    "    print(f\"Step 3: Decoding {len(all_prefixes_data)} prefixes...\")\n",
    "    all_prefix_strings_sorted = [tokenizer.decode(data[2]) for data in all_prefixes_data]\n",
    "\n",
    "    print(f\"Step 3b: Calling generate_steered_output for all {len(all_prefix_strings_sorted)} prefixes (generating {NUM_CONTINUATIONS} each)...\")\n",
    "    all_generated_texts = []\n",
    "    try:\n",
    "        # Call generate_steered_output for the *entire sorted list* of prefix strings\n",
    "        all_generated_texts = generate_steered_output(\n",
    "            steering_vector=None, # No steering needed for this specific task\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            generation_prompts=all_prefix_strings_sorted, # Pass the full sorted list\n",
    "            batch_size=NUM_CONTINUATIONS,                # Generate 100 outputs per prefix\n",
    "            # Pass other necessary args\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample\n",
    "            # layer=..., steering_multiplier=... # Only if steering needed\n",
    "        )\n",
    "        print(\"Generation complete.\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error during the single call to generate_steered_output: {e}\")\n",
    "         # Depending on the error, you might have partial results or none\n",
    "         # If no results, return empty structure\n",
    "         if not all_generated_texts:\n",
    "              print(\"No results generated due to error.\")\n",
    "              tokenizer.padding_side = original_padding_side\n",
    "              return [[] for _ in candidate_texts]\n",
    "         else:\n",
    "              print(\"Proceeding with potentially incomplete results due to error.\")\n",
    "\n",
    "    # --- Step 3c: Process Results ---\n",
    "    print(\"Step 3c: Processing generated texts...\")\n",
    "    temp_results = {} # Key: (text_idx, pos_idx), Value: frequency\n",
    "    expected_num_outputs = len(all_prefix_strings_sorted) * NUM_CONTINUATIONS\n",
    "\n",
    "    if len(all_generated_texts) != expected_num_outputs:\n",
    "        print(f\"Warning: Mismatch in expected outputs!\")\n",
    "        print(f\"  Expected {expected_num_outputs}, got {len(all_generated_texts)}. Results might be incomplete or incorrect.\")\n",
    "        # Attempt to process what we have, but mapping might be wrong if generation failed mid-way\n",
    "\n",
    "    output_idx_counter = 0\n",
    "    # Iterate through the *sorted* prefix data which corresponds to the order of generation\n",
    "    for k in range(len(all_prefixes_data)):\n",
    "        text_idx, pos_idx, _, target_token = all_prefixes_data[k] # Get original metadata\n",
    "        \n",
    "        matches = 0\n",
    "        # Extract the NUM_CONTINUATIONS corresponding to this k-th prefix\n",
    "        start_idx = k * NUM_CONTINUATIONS\n",
    "        end_idx = start_idx + NUM_CONTINUATIONS\n",
    "\n",
    "        # Check bounds in case of generation error / mismatch\n",
    "        if start_idx >= len(all_generated_texts):\n",
    "             print(f\"Warning: Ran out of generated texts while processing prefix index {k}. Skipping.\")\n",
    "             continue\n",
    "        # Adjust end_idx if generation was cut short\n",
    "        actual_end_idx = min(end_idx, len(all_generated_texts))\n",
    "\n",
    "        prefix_continuations = all_generated_texts[start_idx:actual_end_idx]\n",
    "        actual_num_continuations = len(prefix_continuations) # Use actual count for frequency calc\n",
    "\n",
    "        for cont in prefix_continuations:\n",
    "            last_word = get_last_word(cont)\n",
    "            last_word=clean(last_word)\n",
    "            if debug:\n",
    "                print(f\"Generated text: {cont}\")\n",
    "                print(f\"Checking the match between last word {last_word} and target {target_token}\")\n",
    "            if last_word == target_token:\n",
    "                matches += 1\n",
    "\n",
    "        # Calculate and store frequency (use actual count if generation was partial)\n",
    "        frequency = matches / actual_num_continuations if actual_num_continuations > 0 else 0.0\n",
    "        temp_results[(text_idx, pos_idx)] = frequency\n",
    "\n",
    "    # Minimal cleanup\n",
    "    del all_prefix_strings_sorted, all_generated_texts, all_prefixes_data\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Step 4: Assemble Final Output Structure\n",
    "    print(\"Step 4: Assembling final results...\")\n",
    "    # (Assembly logic remains the same)\n",
    "    target_token_frequencies = [[] for _ in candidate_texts]\n",
    "    grouped_results = defaultdict(list)\n",
    "    for (text_idx, pos_idx), frequency in temp_results.items():\n",
    "        grouped_results[text_idx].append((pos_idx, frequency))\n",
    "    for text_idx in range(len(candidate_texts)):\n",
    "        if text_idx in grouped_results:\n",
    "            sorted_freqs = sorted(grouped_results[text_idx], key=lambda x: x[0])\n",
    "            target_token_frequencies[text_idx] = [freq for pos, freq in sorted_freqs]\n",
    "    # --- End Assembly ---\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "    tokenizer.padding_side = original_padding_side # Restore original padding side\n",
    "    return target_token_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94d48b44-1e02-484e-a9c4-3629d2bd1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW VERSION WITH BATCHING\n",
    "PREFIX_BATCH_SIZE = 16\n",
    "\n",
    "NUM_CONTINUATIONS = 100   # Number of rollouts per prefix (passed as batch_size to generate_steered_output)\n",
    "MAX_GEN_TOKENS = 25       # Max new tokens for generation\n",
    "TEMPERATURE = 0.7         # Generation temperature\n",
    "DO_SAMPLE = True          # Use sampling\n",
    "\n",
    "def clean(f):\n",
    "    f=f.strip('.')\n",
    "    f=f.strip(',')\n",
    "    f=f.strip('!')\n",
    "    return f\n",
    "# --- Modified Function ---\n",
    "\n",
    "def calculate_target_token_frequencies_batched(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    candidate_texts: List[str],\n",
    "    candidate_last_tokens: List[str], # Target words for each text\n",
    "    newline_token_id: int,          # Pre-calculated newline token ID\n",
    "    device: torch.device, # Keep device arg\n",
    "    # Add generation parameters needed by generate_steered_output\n",
    "    max_new_tokens: int = MAX_GEN_TOKENS,\n",
    "    temperature: float = TEMPERATURE,\n",
    "    do_sample: bool = DO_SAMPLE,\n",
    "    debug=False\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of target tokens appearing as the last word\n",
    "    in generated continuations for prefixes after the last newline in each text.\n",
    "    Uses length sorting and calls generate_steered_output in manageable batches.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer.\n",
    "        candidate_texts: List of input text strings.\n",
    "        candidate_last_tokens: List of target last words corresponding to candidate_texts.\n",
    "        newline_token_id: The token ID for the newline character.\n",
    "        device: The torch device.\n",
    "        max_new_tokens, temperature, do_sample: Generation parameters.\n",
    "        debug: Flag for verbose printing during matching.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the frequencies\n",
    "        (matches/NUM_CONTINUATIONS) for each token position after the last newline\n",
    "        in the corresponding input text.\n",
    "    \"\"\"\n",
    "    if not candidate_texts or not candidate_last_tokens or len(candidate_texts) != len(candidate_last_tokens):\n",
    "        raise ValueError(\"Input lists are empty or have mismatched lengths.\")\n",
    "    if newline_token_id is None:\n",
    "        raise ValueError(\"newline_token_id must be provided.\")\n",
    "\n",
    "    # Clean target tokens once at the beginning\n",
    "    cleaned_candidate_last_tokens = [clean(w) for w in candidate_last_tokens]\n",
    "\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    original_padding_side = tokenizer.padding_side\n",
    "    # generate_steered_output should handle padding side internally if needed\n",
    "\n",
    "    all_prefixes_data = [] # Stores tuples: (original_text_idx, pos_in_text, prefix_token_ids, target_token)\n",
    "\n",
    "    print(\"Step 1: Collecting prefixes...\")\n",
    "    # --- Prefix Collection (No changes needed here) ---\n",
    "    for text_idx, (text, target_token) in enumerate(zip(candidate_texts, cleaned_candidate_last_tokens)): # Use cleaned targets\n",
    "        try:\n",
    "             tokens_tensor = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "             tokens_list = tokens_tensor.tolist()\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Error tokenizing text index {text_idx}: {e}. Skipping.\")\n",
    "             continue\n",
    "        try:\n",
    "            newline_indices_rev = [i for i, token_id in enumerate(reversed(tokens_list)) if token_id == newline_token_id]\n",
    "            last_n_pos = len(tokens_list) - 1 - newline_indices_rev[0] if newline_indices_rev else -1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error finding newline in text index {text_idx}: {e}. Skipping.\")\n",
    "            continue\n",
    "        for pos_idx in range(last_n_pos + 1, len(tokens_list)):\n",
    "            prefix_tokens = tokens_list[:pos_idx]\n",
    "            if not prefix_tokens: continue\n",
    "            all_prefixes_data.append(\n",
    "                (text_idx, pos_idx, torch.tensor(prefix_tokens, dtype=torch.long), target_token)\n",
    "            )\n",
    "    # --- End Prefix Collection ---\n",
    "\n",
    "    if not all_prefixes_data:\n",
    "        print(\"No valid prefixes found to generate from.\")\n",
    "        return [[] for _ in candidate_texts]\n",
    "\n",
    "    print(f\"Collected {len(all_prefixes_data)} total prefixes.\")\n",
    "\n",
    "    # Step 2: Sort prefixes by length\n",
    "    print(\"Step 2: Sorting prefixes by length...\")\n",
    "    all_prefixes_data.sort(key=lambda x: len(x[2]))\n",
    "\n",
    "    # Step 3: Process Prefixes in Batches\n",
    "    print(f\"Step 3: Generating continuations in batches of {PREFIX_BATCH_SIZE} prefixes...\")\n",
    "    temp_results = {} # Key: (text_idx, pos_idx), Value: frequency\n",
    "\n",
    "    # --- Reintroduce Batch Loop ---\n",
    "    num_prefixes = len(all_prefixes_data)\n",
    "    for i in range(0, num_prefixes, PREFIX_BATCH_SIZE):\n",
    "        batch_start_idx = i\n",
    "        batch_end_idx = min(i + PREFIX_BATCH_SIZE, num_prefixes)\n",
    "        current_batch_data = all_prefixes_data[batch_start_idx:batch_end_idx]\n",
    "        if not current_batch_data: continue\n",
    "\n",
    "        print(f\"  Processing prefix batch {i // PREFIX_BATCH_SIZE + 1} / {math.ceil(num_prefixes / PREFIX_BATCH_SIZE)} (indices {batch_start_idx}-{batch_end_idx-1})\")\n",
    "\n",
    "        # Decode prefixes for this batch only\n",
    "        batch_prefix_strings = [tokenizer.decode(data[2]) for data in current_batch_data]\n",
    "\n",
    "        # Call generate_steered_output for the current batch of prefixes\n",
    "        batch_generated_texts = []\n",
    "        try:\n",
    "            batch_generated_texts = generate_steered_output(\n",
    "                steering_vector=None, # No steering needed for this specific task\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                generation_prompts=batch_prefix_strings, # Pass the current batch list\n",
    "                batch_size=NUM_CONTINUATIONS,          # Generate N outputs per prefix\n",
    "                # Pass other necessary args\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample\n",
    "                # layer=..., steering_multiplier=... # Only if steering needed\n",
    "            )\n",
    "            if debug: print(f\"    Generated {len(batch_generated_texts)} texts for this batch.\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error during call to generate_steered_output for batch starting at prefix index {i}: {e}\")\n",
    "             # Decide how to handle: skip batch, retry, etc. Skipping result processing for this batch.\n",
    "             continue # Move to the next batch\n",
    "\n",
    "        # --- Process results for THIS batch ---\n",
    "        expected_num_outputs = len(batch_prefix_strings) * NUM_CONTINUATIONS\n",
    "        if len(batch_generated_texts) != expected_num_outputs:\n",
    "            print(f\"Warning: Mismatch in expected outputs for batch starting at {i}.\")\n",
    "            print(f\"  Expected {expected_num_outputs}, got {len(batch_generated_texts)}. Processing available results.\")\n",
    "            # Attempt to process what we have, but mapping might be wrong if generation failed mid-way\n",
    "\n",
    "        output_idx_counter = 0\n",
    "        # Iterate through the prefixes *in this batch*\n",
    "        for k in range(len(current_batch_data)):\n",
    "            text_idx, pos_idx, _, target_token = current_batch_data[k] # Get original metadata\n",
    "\n",
    "            matches = 0\n",
    "            # Extract the NUM_CONTINUATIONS corresponding to this k-th prefix *within the batch results*\n",
    "            start_idx_in_batch_results = k * NUM_CONTINUATIONS\n",
    "            end_idx_in_batch_results = start_idx_in_batch_results + NUM_CONTINUATIONS\n",
    "\n",
    "            # Check bounds against the actual number of texts returned *for this batch*\n",
    "            if start_idx_in_batch_results >= len(batch_generated_texts):\n",
    "                if k==0: # Only warn once per batch if generation completely failed\n",
    "                     print(f\"Warning: No generated texts available for batch starting at prefix index {i}. Skipping results processing for this batch.\")\n",
    "                break # Exit inner loop for this batch if results are missing\n",
    "\n",
    "            actual_end_idx = min(end_idx_in_batch_results, len(batch_generated_texts))\n",
    "            prefix_continuations = batch_generated_texts[start_idx_in_batch_results:actual_end_idx]\n",
    "            actual_num_continuations = len(prefix_continuations)\n",
    "\n",
    "            for cont_idx, cont in enumerate(prefix_continuations):\n",
    "                last_word = get_last_word(cont)\n",
    "                last_word = clean(last_word) # Apply cleaning\n",
    "                if debug and cont_idx < 2: # Print first few checks per prefix if debugging\n",
    "                    print(f\"    [Batch {i//PREFIX_BATCH_SIZE+1}, Prefix {k}, Rollout {cont_idx}] Gen text: '{cont[:50]}...' -> Last word: '{last_word}', Target: '{target_token}'\")\n",
    "                if last_word == target_token:\n",
    "                    matches += 1\n",
    "\n",
    "            # Calculate and store frequency\n",
    "            frequency = matches / actual_num_continuations if actual_num_continuations > 0 else 0.0\n",
    "            temp_results[(text_idx, pos_idx)] = frequency\n",
    "\n",
    "        # Minimal cleanup for the batch\n",
    "        del current_batch_data, batch_prefix_strings, batch_generated_texts\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    # --- End Batch Loop ---\n",
    "\n",
    "    # Free memory from original large list\n",
    "    del all_prefixes_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Step 4: Assemble Final Output Structure\n",
    "    print(\"Step 4: Assembling final results...\")\n",
    "    # (Assembly logic remains the same)\n",
    "    target_token_frequencies = [[] for _ in candidate_texts]\n",
    "    grouped_results = defaultdict(list)\n",
    "    for (text_idx, pos_idx), frequency in temp_results.items():\n",
    "        grouped_results[text_idx].append((pos_idx, frequency))\n",
    "    for text_idx in range(len(candidate_texts)):\n",
    "        if text_idx in grouped_results:\n",
    "            sorted_freqs = sorted(grouped_results[text_idx], key=lambda x: x[0])\n",
    "            target_token_frequencies[text_idx] = [freq for pos, freq in sorted_freqs]\n",
    "    # --- End Assembly ---\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "    tokenizer.padding_side = original_padding_side # Restore original padding side\n",
    "    return target_token_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e0f8809-2828-4503-aed4-a1faf8b28f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasofar={\"couplets\":high_kl_results,\n",
    "\"final_tokens\": token_for_high_kl,\n",
    "\"tokenizations\": high_kl_text_tokens,\n",
    "\"KL_divergences_when_steering\": [[float(k) for k in kldiv] for kldiv in high_kl_divergences]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bffd6c32-37c7-4e62-bda0-35f4d1238d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"KL_divergences_for_pain_rhymes\",'w') as f:\n",
    "    json.dump(datasofar,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed0d783-46d4-4be9-bb3a-e0ef3dc4901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"KL_divergences_for_pain_rhymes\",'r') as f:\n",
    "    KL_divergences_for_pain_rhymes=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48273a4a-5df9-46f9-88c5-9f7f9f183fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_kl_results=KL_divergences_for_pain_rhymes[\"couplets\"]\n",
    "high_kl_divergences=KL_divergences_for_pain_rhymes[\"KL_divergences_when_steering\"]\n",
    "token_for_high_kl=KL_divergences_for_pain_rhymes[\"final_tokens\"]\n",
    "high_kl_text_tokens=KL_divergences_for_pain_rhymes[\"tokenizations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c332dbb-6a63-4b0e-81c4-fa27f1b689d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suggested_token={}\n",
    "for i,j in zip(candidate_texts,candidate_last_tokens):\n",
    "    suggested_token[i]=j\n",
    "token_for_high_kl=[suggested_token[t] for t in high_kl_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25124610-0be7-48b3-8937-1d73bc7dce5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_kl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "387a42a7-5a62-47cc-998b-ab0a09321e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Collecting prefixes...\n",
      "Collected 3566 total prefixes.\n",
      "Step 2: Sorting prefixes by length...\n",
      "Step 3: Generating continuations in batches of 16 prefixes...\n",
      "  Processing prefix batch 1 / 223 (indices 0-15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches:   0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 2 / 223 (indices 16-31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:12<00:00, 12.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 3 / 223 (indices 32-47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 4 / 223 (indices 48-63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 5 / 223 (indices 64-79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 6 / 223 (indices 80-95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 7 / 223 (indices 96-111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 8 / 223 (indices 112-127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 9 / 223 (indices 128-143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 10 / 223 (indices 144-159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 11 / 223 (indices 160-175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 12 / 223 (indices 176-191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 13 / 223 (indices 192-207)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 14 / 223 (indices 208-223)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 15 / 223 (indices 224-239)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 16 / 223 (indices 240-255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 17 / 223 (indices 256-271)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 18 / 223 (indices 272-287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 19 / 223 (indices 288-303)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 20 / 223 (indices 304-319)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 21 / 223 (indices 320-335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 22 / 223 (indices 336-351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 23 / 223 (indices 352-367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 24 / 223 (indices 368-383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 25 / 223 (indices 384-399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 26 / 223 (indices 400-415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 27 / 223 (indices 416-431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 28 / 223 (indices 432-447)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 29 / 223 (indices 448-463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 30 / 223 (indices 464-479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 31 / 223 (indices 480-495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 32 / 223 (indices 496-511)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 33 / 223 (indices 512-527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 34 / 223 (indices 528-543)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 35 / 223 (indices 544-559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 36 / 223 (indices 560-575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 37 / 223 (indices 576-591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 38 / 223 (indices 592-607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 39 / 223 (indices 608-623)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 40 / 223 (indices 624-639)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 41 / 223 (indices 640-655)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 42 / 223 (indices 656-671)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:13<00:00, 13.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 43 / 223 (indices 672-687)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 44 / 223 (indices 688-703)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 45 / 223 (indices 704-719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 46 / 223 (indices 720-735)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 47 / 223 (indices 736-751)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 48 / 223 (indices 752-767)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 49 / 223 (indices 768-783)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 50 / 223 (indices 784-799)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 51 / 223 (indices 800-815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 52 / 223 (indices 816-831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 53 / 223 (indices 832-847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 54 / 223 (indices 848-863)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 55 / 223 (indices 864-879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 56 / 223 (indices 880-895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 57 / 223 (indices 896-911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 58 / 223 (indices 912-927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 59 / 223 (indices 928-943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 60 / 223 (indices 944-959)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 61 / 223 (indices 960-975)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 62 / 223 (indices 976-991)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 63 / 223 (indices 992-1007)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 64 / 223 (indices 1008-1023)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 65 / 223 (indices 1024-1039)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 66 / 223 (indices 1040-1055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 67 / 223 (indices 1056-1071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 68 / 223 (indices 1072-1087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 69 / 223 (indices 1088-1103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 70 / 223 (indices 1104-1119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 71 / 223 (indices 1120-1135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 72 / 223 (indices 1136-1151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 73 / 223 (indices 1152-1167)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 74 / 223 (indices 1168-1183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 75 / 223 (indices 1184-1199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 76 / 223 (indices 1200-1215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 77 / 223 (indices 1216-1231)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 78 / 223 (indices 1232-1247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 79 / 223 (indices 1248-1263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 80 / 223 (indices 1264-1279)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 81 / 223 (indices 1280-1295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 82 / 223 (indices 1296-1311)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 83 / 223 (indices 1312-1327)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 84 / 223 (indices 1328-1343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 85 / 223 (indices 1344-1359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 86 / 223 (indices 1360-1375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 87 / 223 (indices 1376-1391)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 88 / 223 (indices 1392-1407)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 89 / 223 (indices 1408-1423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 90 / 223 (indices 1424-1439)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 91 / 223 (indices 1440-1455)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 92 / 223 (indices 1456-1471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 93 / 223 (indices 1472-1487)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 94 / 223 (indices 1488-1503)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 95 / 223 (indices 1504-1519)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 96 / 223 (indices 1520-1535)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 97 / 223 (indices 1536-1551)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 98 / 223 (indices 1552-1567)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 99 / 223 (indices 1568-1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 100 / 223 (indices 1584-1599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 101 / 223 (indices 1600-1615)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 102 / 223 (indices 1616-1631)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 103 / 223 (indices 1632-1647)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 104 / 223 (indices 1648-1663)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 105 / 223 (indices 1664-1679)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 106 / 223 (indices 1680-1695)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 107 / 223 (indices 1696-1711)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 108 / 223 (indices 1712-1727)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 109 / 223 (indices 1728-1743)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 110 / 223 (indices 1744-1759)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 111 / 223 (indices 1760-1775)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 112 / 223 (indices 1776-1791)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 113 / 223 (indices 1792-1807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 114 / 223 (indices 1808-1823)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 115 / 223 (indices 1824-1839)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 116 / 223 (indices 1840-1855)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 117 / 223 (indices 1856-1871)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 118 / 223 (indices 1872-1887)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 119 / 223 (indices 1888-1903)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 120 / 223 (indices 1904-1919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 121 / 223 (indices 1920-1935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 122 / 223 (indices 1936-1951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 123 / 223 (indices 1952-1967)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 124 / 223 (indices 1968-1983)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 125 / 223 (indices 1984-1999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 126 / 223 (indices 2000-2015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 127 / 223 (indices 2016-2031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 128 / 223 (indices 2032-2047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 129 / 223 (indices 2048-2063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 130 / 223 (indices 2064-2079)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 131 / 223 (indices 2080-2095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 132 / 223 (indices 2096-2111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 133 / 223 (indices 2112-2127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 134 / 223 (indices 2128-2143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 135 / 223 (indices 2144-2159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 136 / 223 (indices 2160-2175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 137 / 223 (indices 2176-2191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 138 / 223 (indices 2192-2207)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 139 / 223 (indices 2208-2223)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 140 / 223 (indices 2224-2239)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 141 / 223 (indices 2240-2255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 142 / 223 (indices 2256-2271)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 143 / 223 (indices 2272-2287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 144 / 223 (indices 2288-2303)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 145 / 223 (indices 2304-2319)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 146 / 223 (indices 2320-2335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 147 / 223 (indices 2336-2351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 148 / 223 (indices 2352-2367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:14<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 149 / 223 (indices 2368-2383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 150 / 223 (indices 2384-2399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 151 / 223 (indices 2400-2415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 152 / 223 (indices 2416-2431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 153 / 223 (indices 2432-2447)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 154 / 223 (indices 2448-2463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 155 / 223 (indices 2464-2479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 156 / 223 (indices 2480-2495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 157 / 223 (indices 2496-2511)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 158 / 223 (indices 2512-2527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 159 / 223 (indices 2528-2543)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 160 / 223 (indices 2544-2559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 161 / 223 (indices 2560-2575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 162 / 223 (indices 2576-2591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 163 / 223 (indices 2592-2607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 164 / 223 (indices 2608-2623)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 165 / 223 (indices 2624-2639)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 166 / 223 (indices 2640-2655)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 167 / 223 (indices 2656-2671)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 168 / 223 (indices 2672-2687)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 169 / 223 (indices 2688-2703)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 170 / 223 (indices 2704-2719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 171 / 223 (indices 2720-2735)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 172 / 223 (indices 2736-2751)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 173 / 223 (indices 2752-2767)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 174 / 223 (indices 2768-2783)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 175 / 223 (indices 2784-2799)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 176 / 223 (indices 2800-2815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 177 / 223 (indices 2816-2831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 178 / 223 (indices 2832-2847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 179 / 223 (indices 2848-2863)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 180 / 223 (indices 2864-2879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 181 / 223 (indices 2880-2895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 182 / 223 (indices 2896-2911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 183 / 223 (indices 2912-2927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 184 / 223 (indices 2928-2943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 185 / 223 (indices 2944-2959)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 186 / 223 (indices 2960-2975)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 187 / 223 (indices 2976-2991)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 188 / 223 (indices 2992-3007)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 189 / 223 (indices 3008-3023)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 190 / 223 (indices 3024-3039)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 191 / 223 (indices 3040-3055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 192 / 223 (indices 3056-3071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 193 / 223 (indices 3072-3087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 194 / 223 (indices 3088-3103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 195 / 223 (indices 3104-3119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 196 / 223 (indices 3120-3135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 197 / 223 (indices 3136-3151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 198 / 223 (indices 3152-3167)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 199 / 223 (indices 3168-3183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 200 / 223 (indices 3184-3199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 201 / 223 (indices 3200-3215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 202 / 223 (indices 3216-3231)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 203 / 223 (indices 3232-3247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 204 / 223 (indices 3248-3263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 205 / 223 (indices 3264-3279)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 206 / 223 (indices 3280-3295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 207 / 223 (indices 3296-3311)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 208 / 223 (indices 3312-3327)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 209 / 223 (indices 3328-3343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 210 / 223 (indices 3344-3359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 211 / 223 (indices 3360-3375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 212 / 223 (indices 3376-3391)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 213 / 223 (indices 3392-3407)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 214 / 223 (indices 3408-3423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 215 / 223 (indices 3424-3439)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 216 / 223 (indices 3440-3455)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 217 / 223 (indices 3456-3471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 218 / 223 (indices 3472-3487)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 219 / 223 (indices 3488-3503)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 220 / 223 (indices 3504-3519)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 221 / 223 (indices 3520-3535)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 222 / 223 (indices 3536-3551)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:16<00:00, 16.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing prefix batch 223 / 223 (indices 3552-3565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 1/1 [00:15<00:00, 15.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Assembling final results...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "#throws OOM if trying to use large batches >=20\n",
    "frequencies = calculate_target_token_frequencies_batched(\n",
    "         model,\n",
    "         tokenizer,\n",
    "         high_kl_results,\n",
    "         token_for_high_kl,\n",
    "         newline_token_id,\n",
    "         device, debug=False\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3375e569-733a-4d19-acc6-bc5cbc912cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A rhymed couplet:\\nThe plan they proposed was utterly insane\\nThey'd build a tower to reach the moon's\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_kl_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5621684a-a32c-4d05-92f8-1bb6d034a6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frequencies[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11a6759f-1b74-4dc7-888b-6f7bccd2c87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_kl_divergences[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d632ccf-44d2-4842-89ff-60eedbdeaf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08,\n",
       " 0.04000000000000001,\n",
       " -0.010000000000000009,\n",
       " 0.08000000000000002,\n",
       " 0.0,\n",
       " -0.010000000000000009,\n",
       " 0.15000000000000002,\n",
       " 0.08999999999999997,\n",
       " 0.06,\n",
       " 0.030000000000000027,\n",
       " 0.44999999999999996]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[frequencies[0][i+1]-frequencies[0][i] for i in range(len(frequencies[0])-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8205850-f90b-4ff6-8aa9-49fa1694644a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.51, 0.41, 0.45, 0.42, 0.55, 0.55, 0.14, 1.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "139b07a5-a2ed-4e64-992e-67ba935fb442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.5961446760447736\n"
     ]
    }
   ],
   "source": [
    "x = np.array(high_kl_divergences[1][:-1])\n",
    "#y = np.array([frequencies[1][i+1]-frequencies[0][i] for i in range(len(frequencies[0])-1)])\n",
    "y = np.array(frequencies[1][:-1])\n",
    "y=list(range(len(x)))\n",
    "# Compute Pearson correlation\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(\"Pearson correlation:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dcd5de4-0d8f-4d2a-9124-8def514fd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasofar={\"couplets\":high_kl_results,\n",
    "\"final_tokens\": token_for_high_kl,\n",
    "\"tokenizations\": high_kl_text_tokens,\n",
    "\"KL_divergences_when_steering\": high_kl_divergences,\n",
    "\"frequencies\": frequencies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "144b3965-ba65-468c-a0cb-825fe1a41554",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"KL_divergences_for_pain_rhymes\",'w') as f:\n",
    "    json.dump(datasofar,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
